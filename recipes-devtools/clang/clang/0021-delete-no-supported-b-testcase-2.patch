From caf9849edd0e2336b74d712e8774a49cdf9cae30 Mon Sep 17 00:00:00 2001
From: "yilun.xie" <yilun.xie@starfivetech.com>
Date: Fri, 11 Feb 2022 14:51:40 +0800
Subject: [PATCH 21/24] delete no supported b testcase(2)

---
 llvm/test/CodeGen/RISCV/attributes.ll        |   56 +-
 llvm/test/CodeGen/RISCV/rv32zbb-zbp.ll       |  828 ----
 llvm/test/CodeGen/RISCV/rv32zbe-intrinsic.ll |   37 -
 llvm/test/CodeGen/RISCV/rv32zbp-intrinsic.ll |  233 --
 llvm/test/CodeGen/RISCV/rv32zbp.ll           | 3413 ----------------
 llvm/test/CodeGen/RISCV/rv32zbr.ll           |   69 -
 llvm/test/CodeGen/RISCV/rv32zbt.ll           |  824 ----
 llvm/test/CodeGen/RISCV/rv64zbb-zbp.ll       |  779 ----
 llvm/test/CodeGen/RISCV/rv64zbe-intrinsic.ll |  109 -
 llvm/test/CodeGen/RISCV/rv64zbp-intrinsic.ll |  445 --
 llvm/test/CodeGen/RISCV/rv64zbp.ll           | 3858 ------------------
 llvm/test/CodeGen/RISCV/rv64zbr.ll           |   91 -
 llvm/test/CodeGen/RISCV/rv64zbt.ll           |  618 ---
 13 files changed, 16 insertions(+), 11344 deletions(-)
 delete mode 100644 llvm/test/CodeGen/RISCV/rv32zbb-zbp.ll
 delete mode 100644 llvm/test/CodeGen/RISCV/rv32zbe-intrinsic.ll
 delete mode 100644 llvm/test/CodeGen/RISCV/rv32zbp-intrinsic.ll
 delete mode 100644 llvm/test/CodeGen/RISCV/rv32zbp.ll
 delete mode 100644 llvm/test/CodeGen/RISCV/rv32zbr.ll
 delete mode 100644 llvm/test/CodeGen/RISCV/rv32zbt.ll
 delete mode 100644 llvm/test/CodeGen/RISCV/rv64zbb-zbp.ll
 delete mode 100644 llvm/test/CodeGen/RISCV/rv64zbe-intrinsic.ll
 delete mode 100644 llvm/test/CodeGen/RISCV/rv64zbp-intrinsic.ll
 delete mode 100644 llvm/test/CodeGen/RISCV/rv64zbp.ll
 delete mode 100644 llvm/test/CodeGen/RISCV/rv64zbr.ll
 delete mode 100644 llvm/test/CodeGen/RISCV/rv64zbt.ll

diff --git a/llvm/test/CodeGen/RISCV/attributes.ll b/llvm/test/CodeGen/RISCV/attributes.ll
index c26a6d5b4a69..aa25a11d1bb9 100644
--- a/llvm/test/CodeGen/RISCV/attributes.ll
+++ b/llvm/test/CodeGen/RISCV/attributes.ll
@@ -11,14 +11,8 @@
 ; RUN: llc -mtriple=riscv32 -mattr=+experimental-zba %s -o - | FileCheck --check-prefix=RV32ZBA %s
 ; RUN: llc -mtriple=riscv32 -mattr=+experimental-zbb %s -o - | FileCheck --check-prefix=RV32ZBB %s
 ; RUN: llc -mtriple=riscv32 -mattr=+experimental-zbc %s -o - | FileCheck --check-prefix=RV32ZBC %s
-; RUN: llc -mtriple=riscv32 -mattr=+experimental-zbe %s -o - | FileCheck --check-prefix=RV32ZBE %s
-; RUN: llc -mtriple=riscv32 -mattr=+experimental-zbf %s -o - | FileCheck --check-prefix=RV32ZBF %s
-; RUN: llc -mtriple=riscv32 -mattr=+experimental-zbm %s -o - | FileCheck --check-prefix=RV32ZBM %s
-; RUN: llc -mtriple=riscv32 -mattr=+experimental-zbp %s -o - | FileCheck --check-prefix=RV32ZBP %s
 ; RUN: llc -mtriple=riscv32 -mattr=+experimental-zbproposedc %s -o - | FileCheck --check-prefix=RV32ZBPROPOSEDC %s
-; RUN: llc -mtriple=riscv32 -mattr=+experimental-zbr %s -o - | FileCheck --check-prefix=RV32ZBR %s
 ; RUN: llc -mtriple=riscv32 -mattr=+experimental-zbs %s -o - | FileCheck --check-prefix=RV32ZBS %s
-; RUN: llc -mtriple=riscv32 -mattr=+experimental-zbt %s -o - | FileCheck --check-prefix=RV32ZBT %s
 ; RUN: llc -mtriple=riscv32 -mattr=+experimental-zbb,+experimental-zfh,+experimental-zvamo,+experimental-v,+f,+experimental-zvlsseg %s -o - | FileCheck --check-prefix=RV32COMBINED %s
 ; RUN: llc -mtriple=riscv64 -mattr=+m %s -o - | FileCheck --check-prefix=RV64M %s
 ; RUN: llc -mtriple=riscv64 -mattr=+a %s -o - | FileCheck --check-prefix=RV64A %s
@@ -31,14 +25,8 @@
 ; RUN: llc -mtriple=riscv64 -mattr=+experimental-zba %s -o - | FileCheck --check-prefix=RV64ZBA %s
 ; RUN: llc -mtriple=riscv64 -mattr=+experimental-zbb %s -o - | FileCheck --check-prefix=RV64ZBB %s
 ; RUN: llc -mtriple=riscv64 -mattr=+experimental-zbc %s -o - | FileCheck --check-prefix=RV64ZBC %s
-; RUN: llc -mtriple=riscv64 -mattr=+experimental-zbe %s -o - | FileCheck --check-prefix=RV64ZBE %s
-; RUN: llc -mtriple=riscv64 -mattr=+experimental-zbf %s -o - | FileCheck --check-prefix=RV64ZBF %s
-; RUN: llc -mtriple=riscv64 -mattr=+experimental-zbm %s -o - | FileCheck --check-prefix=RV64ZBM %s
-; RUN: llc -mtriple=riscv64 -mattr=+experimental-zbp %s -o - | FileCheck --check-prefix=RV64ZBP %s
 ; RUN: llc -mtriple=riscv64 -mattr=+experimental-zbproposedc %s -o - | FileCheck --check-prefix=RV64ZBPROPOSEDC %s
-; RUN: llc -mtriple=riscv64 -mattr=+experimental-zbr %s -o - | FileCheck --check-prefix=RV64ZBR %s
 ; RUN: llc -mtriple=riscv64 -mattr=+experimental-zbs %s -o - | FileCheck --check-prefix=RV64ZBS %s
-; RUN: llc -mtriple=riscv64 -mattr=+experimental-zbt %s -o - | FileCheck --check-prefix=RV64ZBT %s
 ; RUN: llc -mtriple=riscv64 -mattr=+experimental-zbb,+experimental-zfh,+experimental-zvamo,+experimental-v,+f,+experimental-zvlsseg %s -o - | FileCheck --check-prefix=RV64COMBINED %s
 
 ; RV32M: .attribute 5, "rv32i2p0_m2p0"
@@ -46,42 +34,30 @@
 ; RV32F: .attribute 5, "rv32i2p0_f2p0"
 ; RV32D: .attribute 5, "rv32i2p0_f2p0_d2p0"
 ; RV32C: .attribute 5, "rv32i2p0_c2p0"
-; RV32B: .attribute 5, "rv32i2p0_b0p93_zba0p93_zbb0p93_zbc0p93_zbe0p93_zbf0p93_zbm0p93_zbp0p93_zbr0p93_zbs0p93_zbt0p93"
-; RV32V: .attribute 5, "rv32i2p0_v0p10_zvamo0p10_zvlsseg0p10"
+; RV32B: .attribute 5, "rv32i2p0_b1p0_zba1p0_zbb1p0_zbc1p0_zbs1p0"
+; RV32V: .attribute 5, "rv32i2p0_v1p0_zvamo1p0_zvlsseg1p0"
 ; RV32ZFH: .attribute 5, "rv32i2p0_f2p0_zfh0p1"
-; RV32ZBA: .attribute 5, "rv32i2p0_zba0p93"
-; RV32ZBB: .attribute 5, "rv32i2p0_zbb0p93"
-; RV32ZBC: .attribute 5, "rv32i2p0_zbc0p93"
-; RV32ZBE: .attribute 5, "rv32i2p0_zbe0p93"
-; RV32ZBF: .attribute 5, "rv32i2p0_zbf0p93"
-; RV32ZBM: .attribute 5, "rv32i2p0_zbm0p93"
-; RV32ZBP: .attribute 5, "rv32i2p0_zbp0p93"
-; RV32ZBPROPOSEDC: .attribute 5, "rv32i2p0_zbproposedc0p93"
-; RV32ZBR: .attribute 5, "rv32i2p0_zbr0p93"
-; RV32ZBS: .attribute 5, "rv32i2p0_zbs0p93"
-; RV32ZBT: .attribute 5, "rv32i2p0_zbt0p93"
-; RV32COMBINED: .attribute 5, "rv32i2p0_f2p0_v0p10_zfh0p1_zbb0p93_zvamo0p10_zvlsseg0p10"
+; RV32ZBA: .attribute 5, "rv32i2p0_zba1p0"
+; RV32ZBB: .attribute 5, "rv32i2p0_zbb1p0"
+; RV32ZBC: .attribute 5, "rv32i2p0_zbc1p0"
+; RV32ZBPROPOSEDC: .attribute 5, "rv32i2p0_zbproposedc0p94"
+; RV32ZBS: .attribute 5, "rv32i2p0_zbs1p0"
+; RV32COMBINED: .attribute 5, "rv32i2p0_f2p0_v1p0_zfh0p1_zbb1p0_zvamo1p0_zvlsseg1p0"
 
 ; RV64M: .attribute 5, "rv64i2p0_m2p0"
 ; RV64A: .attribute 5, "rv64i2p0_a2p0"
 ; RV64F: .attribute 5, "rv64i2p0_f2p0"
 ; RV64D: .attribute 5, "rv64i2p0_f2p0_d2p0"
 ; RV64C: .attribute 5, "rv64i2p0_c2p0"
-; RV64B: .attribute 5, "rv64i2p0_b0p93_zba0p93_zbb0p93_zbc0p93_zbe0p93_zbf0p93_zbm0p93_zbp0p93_zbr0p93_zbs0p93_zbt0p93"
+; RV64B: .attribute 5, "rv64i2p0_b1p0_zba1p0_zbb1p0_zbc1p0_zbs1p0_zbt1p0"
 ; RV64ZFH: .attribute 5, "rv64i2p0_f2p0_zfh0p1"
-; RV64ZBA: .attribute 5, "rv64i2p0_zba0p93"
-; RV64ZBB: .attribute 5, "rv64i2p0_zbb0p93"
-; RV64ZBC: .attribute 5, "rv64i2p0_zbc0p93"
-; RV64ZBE: .attribute 5, "rv64i2p0_zbe0p93"
-; RV64ZBF: .attribute 5, "rv64i2p0_zbf0p93"
-; RV64ZBM: .attribute 5, "rv64i2p0_zbm0p93"
-; RV64ZBP: .attribute 5, "rv64i2p0_zbp0p93"
-; RV64ZBPROPOSEDC: .attribute 5, "rv64i2p0_zbproposedc0p93"
-; RV64ZBR: .attribute 5, "rv64i2p0_zbr0p93"
-; RV64ZBS: .attribute 5, "rv64i2p0_zbs0p93"
-; RV64ZBT: .attribute 5, "rv64i2p0_zbt0p93"
-; RV64V: .attribute 5, "rv64i2p0_v0p10_zvamo0p10_zvlsseg0p10"
-; RV64COMBINED: .attribute 5, "rv64i2p0_f2p0_v0p10_zfh0p1_zbb0p93_zvamo0p10_zvlsseg0p10"
+; RV64ZBA: .attribute 5, "rv64i2p0_zba1p0"
+; RV64ZBB: .attribute 5, "rv64i2p0_zbb1p0"
+; RV64ZBC: .attribute 5, "rv64i2p0_zbc1p0"
+; RV64ZBPROPOSEDC: .attribute 5, "rv64i2p0_zbproposedc0p94"
+; RV64ZBS: .attribute 5, "rv64i2p0_zbs1p0"
+; RV64V: .attribute 5, "rv64i2p0_v1p0_zvamo1p0_zvlsseg1p0"
+; RV64COMBINED: .attribute 5, "rv64i2p0_f2p0_v1p0_zfh0p1_zbb1p0_zvamo1p0_zvlsseg1p0"
 
 
 define i32 @addi(i32 %a) {
diff --git a/llvm/test/CodeGen/RISCV/rv32zbb-zbp.ll b/llvm/test/CodeGen/RISCV/rv32zbb-zbp.ll
deleted file mode 100644
index 9f87f106cbb3..000000000000
--- a/llvm/test/CodeGen/RISCV/rv32zbb-zbp.ll
+++ /dev/null
@@ -1,828 +0,0 @@
-; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
-; RUN: llc -mtriple=riscv32 -verify-machineinstrs < %s \
-; RUN:   | FileCheck %s -check-prefix=RV32I
-; RUN: llc -mtriple=riscv32 -mattr=+experimental-b -verify-machineinstrs < %s \
-; RUN:   | FileCheck %s -check-prefix=RV32B
-; RUN: llc -mtriple=riscv32 -mattr=+experimental-zbb -verify-machineinstrs < %s \
-; RUN:   | FileCheck %s -check-prefix=RV32ZBB
-; RUN: llc -mtriple=riscv32 -mattr=+experimental-zbp -verify-machineinstrs < %s \
-; RUN:   | FileCheck %s -check-prefix=RV32ZBP
-
-define i32 @andn_i32(i32 %a, i32 %b) nounwind {
-; RV32I-LABEL: andn_i32:
-; RV32I:       # %bb.0:
-; RV32I-NEXT:    not a1, a1
-; RV32I-NEXT:    and a0, a1, a0
-; RV32I-NEXT:    ret
-;
-; RV32B-LABEL: andn_i32:
-; RV32B:       # %bb.0:
-; RV32B-NEXT:    andn a0, a0, a1
-; RV32B-NEXT:    ret
-;
-; RV32ZBB-LABEL: andn_i32:
-; RV32ZBB:       # %bb.0:
-; RV32ZBB-NEXT:    andn a0, a0, a1
-; RV32ZBB-NEXT:    ret
-;
-; RV32ZBP-LABEL: andn_i32:
-; RV32ZBP:       # %bb.0:
-; RV32ZBP-NEXT:    andn a0, a0, a1
-; RV32ZBP-NEXT:    ret
-  %neg = xor i32 %b, -1
-  %and = and i32 %neg, %a
-  ret i32 %and
-}
-
-define i64 @andn_i64(i64 %a, i64 %b) nounwind {
-; RV32I-LABEL: andn_i64:
-; RV32I:       # %bb.0:
-; RV32I-NEXT:    not a3, a3
-; RV32I-NEXT:    not a2, a2
-; RV32I-NEXT:    and a0, a2, a0
-; RV32I-NEXT:    and a1, a3, a1
-; RV32I-NEXT:    ret
-;
-; RV32B-LABEL: andn_i64:
-; RV32B:       # %bb.0:
-; RV32B-NEXT:    andn a0, a0, a2
-; RV32B-NEXT:    andn a1, a1, a3
-; RV32B-NEXT:    ret
-;
-; RV32ZBB-LABEL: andn_i64:
-; RV32ZBB:       # %bb.0:
-; RV32ZBB-NEXT:    andn a0, a0, a2
-; RV32ZBB-NEXT:    andn a1, a1, a3
-; RV32ZBB-NEXT:    ret
-;
-; RV32ZBP-LABEL: andn_i64:
-; RV32ZBP:       # %bb.0:
-; RV32ZBP-NEXT:    andn a0, a0, a2
-; RV32ZBP-NEXT:    andn a1, a1, a3
-; RV32ZBP-NEXT:    ret
-  %neg = xor i64 %b, -1
-  %and = and i64 %neg, %a
-  ret i64 %and
-}
-
-define i32 @orn_i32(i32 %a, i32 %b) nounwind {
-; RV32I-LABEL: orn_i32:
-; RV32I:       # %bb.0:
-; RV32I-NEXT:    not a1, a1
-; RV32I-NEXT:    or a0, a1, a0
-; RV32I-NEXT:    ret
-;
-; RV32B-LABEL: orn_i32:
-; RV32B:       # %bb.0:
-; RV32B-NEXT:    orn a0, a0, a1
-; RV32B-NEXT:    ret
-;
-; RV32ZBB-LABEL: orn_i32:
-; RV32ZBB:       # %bb.0:
-; RV32ZBB-NEXT:    orn a0, a0, a1
-; RV32ZBB-NEXT:    ret
-;
-; RV32ZBP-LABEL: orn_i32:
-; RV32ZBP:       # %bb.0:
-; RV32ZBP-NEXT:    orn a0, a0, a1
-; RV32ZBP-NEXT:    ret
-  %neg = xor i32 %b, -1
-  %or = or i32 %neg, %a
-  ret i32 %or
-}
-
-define i64 @orn_i64(i64 %a, i64 %b) nounwind {
-; RV32I-LABEL: orn_i64:
-; RV32I:       # %bb.0:
-; RV32I-NEXT:    not a3, a3
-; RV32I-NEXT:    not a2, a2
-; RV32I-NEXT:    or a0, a2, a0
-; RV32I-NEXT:    or a1, a3, a1
-; RV32I-NEXT:    ret
-;
-; RV32B-LABEL: orn_i64:
-; RV32B:       # %bb.0:
-; RV32B-NEXT:    orn a0, a0, a2
-; RV32B-NEXT:    orn a1, a1, a3
-; RV32B-NEXT:    ret
-;
-; RV32ZBB-LABEL: orn_i64:
-; RV32ZBB:       # %bb.0:
-; RV32ZBB-NEXT:    orn a0, a0, a2
-; RV32ZBB-NEXT:    orn a1, a1, a3
-; RV32ZBB-NEXT:    ret
-;
-; RV32ZBP-LABEL: orn_i64:
-; RV32ZBP:       # %bb.0:
-; RV32ZBP-NEXT:    orn a0, a0, a2
-; RV32ZBP-NEXT:    orn a1, a1, a3
-; RV32ZBP-NEXT:    ret
-  %neg = xor i64 %b, -1
-  %or = or i64 %neg, %a
-  ret i64 %or
-}
-
-define i32 @xnor_i32(i32 %a, i32 %b) nounwind {
-; RV32I-LABEL: xnor_i32:
-; RV32I:       # %bb.0:
-; RV32I-NEXT:    xor a0, a0, a1
-; RV32I-NEXT:    not a0, a0
-; RV32I-NEXT:    ret
-;
-; RV32B-LABEL: xnor_i32:
-; RV32B:       # %bb.0:
-; RV32B-NEXT:    xnor a0, a0, a1
-; RV32B-NEXT:    ret
-;
-; RV32ZBB-LABEL: xnor_i32:
-; RV32ZBB:       # %bb.0:
-; RV32ZBB-NEXT:    xnor a0, a0, a1
-; RV32ZBB-NEXT:    ret
-;
-; RV32ZBP-LABEL: xnor_i32:
-; RV32ZBP:       # %bb.0:
-; RV32ZBP-NEXT:    xnor a0, a0, a1
-; RV32ZBP-NEXT:    ret
-  %neg = xor i32 %a, -1
-  %xor = xor i32 %neg, %b
-  ret i32 %xor
-}
-
-define i64 @xnor_i64(i64 %a, i64 %b) nounwind {
-; RV32I-LABEL: xnor_i64:
-; RV32I:       # %bb.0:
-; RV32I-NEXT:    xor a1, a1, a3
-; RV32I-NEXT:    xor a0, a0, a2
-; RV32I-NEXT:    not a0, a0
-; RV32I-NEXT:    not a1, a1
-; RV32I-NEXT:    ret
-;
-; RV32B-LABEL: xnor_i64:
-; RV32B:       # %bb.0:
-; RV32B-NEXT:    xnor a0, a0, a2
-; RV32B-NEXT:    xnor a1, a1, a3
-; RV32B-NEXT:    ret
-;
-; RV32ZBB-LABEL: xnor_i64:
-; RV32ZBB:       # %bb.0:
-; RV32ZBB-NEXT:    xnor a0, a0, a2
-; RV32ZBB-NEXT:    xnor a1, a1, a3
-; RV32ZBB-NEXT:    ret
-;
-; RV32ZBP-LABEL: xnor_i64:
-; RV32ZBP:       # %bb.0:
-; RV32ZBP-NEXT:    xnor a0, a0, a2
-; RV32ZBP-NEXT:    xnor a1, a1, a3
-; RV32ZBP-NEXT:    ret
-  %neg = xor i64 %a, -1
-  %xor = xor i64 %neg, %b
-  ret i64 %xor
-}
-
-declare i32 @llvm.fshl.i32(i32, i32, i32)
-
-define i32 @rol_i32(i32 %a, i32 %b) nounwind {
-; RV32I-LABEL: rol_i32:
-; RV32I:       # %bb.0:
-; RV32I-NEXT:    sll a2, a0, a1
-; RV32I-NEXT:    neg a1, a1
-; RV32I-NEXT:    srl a0, a0, a1
-; RV32I-NEXT:    or a0, a2, a0
-; RV32I-NEXT:    ret
-;
-; RV32B-LABEL: rol_i32:
-; RV32B:       # %bb.0:
-; RV32B-NEXT:    rol a0, a0, a1
-; RV32B-NEXT:    ret
-;
-; RV32ZBB-LABEL: rol_i32:
-; RV32ZBB:       # %bb.0:
-; RV32ZBB-NEXT:    rol a0, a0, a1
-; RV32ZBB-NEXT:    ret
-;
-; RV32ZBP-LABEL: rol_i32:
-; RV32ZBP:       # %bb.0:
-; RV32ZBP-NEXT:    rol a0, a0, a1
-; RV32ZBP-NEXT:    ret
-  %or = tail call i32 @llvm.fshl.i32(i32 %a, i32 %a, i32 %b)
-  ret i32 %or
-}
-
-; As we are not matching directly i64 code patterns on RV32 some i64 patterns
-; don't have yet any matching bit manipulation instructions on RV32.
-; This test is presented here in case future expansions of the experimental-b
-; extension introduce instructions suitable for this pattern.
-
-declare i64 @llvm.fshl.i64(i64, i64, i64)
-
-define i64 @rol_i64(i64 %a, i64 %b) nounwind {
-; RV32I-LABEL: rol_i64:
-; RV32I:       # %bb.0:
-; RV32I-NEXT:    mv t1, a1
-; RV32I-NEXT:    andi a1, a2, 63
-; RV32I-NEXT:    addi a7, a1, -32
-; RV32I-NEXT:    addi a6, zero, 31
-; RV32I-NEXT:    bltz a7, .LBB7_2
-; RV32I-NEXT:  # %bb.1:
-; RV32I-NEXT:    sll a1, a0, a7
-; RV32I-NEXT:    j .LBB7_3
-; RV32I-NEXT:  .LBB7_2:
-; RV32I-NEXT:    sll a4, t1, a2
-; RV32I-NEXT:    sub a1, a6, a1
-; RV32I-NEXT:    srli a5, a0, 1
-; RV32I-NEXT:    srl a1, a5, a1
-; RV32I-NEXT:    or a1, a4, a1
-; RV32I-NEXT:  .LBB7_3:
-; RV32I-NEXT:    neg a5, a2
-; RV32I-NEXT:    andi a4, a5, 63
-; RV32I-NEXT:    addi t0, a4, -32
-; RV32I-NEXT:    bltz t0, .LBB7_5
-; RV32I-NEXT:  # %bb.4:
-; RV32I-NEXT:    srl a3, t1, t0
-; RV32I-NEXT:    bltz a7, .LBB7_6
-; RV32I-NEXT:    j .LBB7_7
-; RV32I-NEXT:  .LBB7_5:
-; RV32I-NEXT:    srl a3, t1, a5
-; RV32I-NEXT:    or a1, a1, a3
-; RV32I-NEXT:    srl a3, a0, a5
-; RV32I-NEXT:    sub a4, a6, a4
-; RV32I-NEXT:    slli a5, t1, 1
-; RV32I-NEXT:    sll a4, a5, a4
-; RV32I-NEXT:    or a3, a3, a4
-; RV32I-NEXT:    bgez a7, .LBB7_7
-; RV32I-NEXT:  .LBB7_6:
-; RV32I-NEXT:    sll a0, a0, a2
-; RV32I-NEXT:    or a3, a3, a0
-; RV32I-NEXT:  .LBB7_7:
-; RV32I-NEXT:    mv a0, a3
-; RV32I-NEXT:    ret
-;
-; RV32B-LABEL: rol_i64:
-; RV32B:       # %bb.0:
-; RV32B-NEXT:    sll a7, a1, a2
-; RV32B-NEXT:    andi a4, a2, 63
-; RV32B-NEXT:    addi a6, zero, 31
-; RV32B-NEXT:    sub a5, a6, a4
-; RV32B-NEXT:    srli a3, a0, 1
-; RV32B-NEXT:    srl a3, a3, a5
-; RV32B-NEXT:    or a3, a7, a3
-; RV32B-NEXT:    addi a7, a4, -32
-; RV32B-NEXT:    sll a5, a0, a7
-; RV32B-NEXT:    slti a4, a7, 0
-; RV32B-NEXT:    cmov t0, a4, a3, a5
-; RV32B-NEXT:    neg a4, a2
-; RV32B-NEXT:    srl t2, a1, a4
-; RV32B-NEXT:    andi a3, a4, 63
-; RV32B-NEXT:    addi t1, a3, -32
-; RV32B-NEXT:    srai a5, t1, 31
-; RV32B-NEXT:    and a5, a5, t2
-; RV32B-NEXT:    or t0, t0, a5
-; RV32B-NEXT:    srl a4, a0, a4
-; RV32B-NEXT:    sub a3, a6, a3
-; RV32B-NEXT:    slli a5, a1, 1
-; RV32B-NEXT:    sll a3, a5, a3
-; RV32B-NEXT:    or a3, a4, a3
-; RV32B-NEXT:    srl a1, a1, t1
-; RV32B-NEXT:    slti a4, t1, 0
-; RV32B-NEXT:    cmov a1, a4, a3, a1
-; RV32B-NEXT:    sll a0, a0, a2
-; RV32B-NEXT:    srai a2, a7, 31
-; RV32B-NEXT:    and a0, a2, a0
-; RV32B-NEXT:    or a0, a0, a1
-; RV32B-NEXT:    mv a1, t0
-; RV32B-NEXT:    ret
-;
-; RV32ZBB-LABEL: rol_i64:
-; RV32ZBB:       # %bb.0:
-; RV32ZBB-NEXT:    mv t1, a1
-; RV32ZBB-NEXT:    andi a1, a2, 63
-; RV32ZBB-NEXT:    addi a7, a1, -32
-; RV32ZBB-NEXT:    addi a6, zero, 31
-; RV32ZBB-NEXT:    bltz a7, .LBB7_2
-; RV32ZBB-NEXT:  # %bb.1:
-; RV32ZBB-NEXT:    sll a1, a0, a7
-; RV32ZBB-NEXT:    j .LBB7_3
-; RV32ZBB-NEXT:  .LBB7_2:
-; RV32ZBB-NEXT:    sll a4, t1, a2
-; RV32ZBB-NEXT:    sub a1, a6, a1
-; RV32ZBB-NEXT:    srli a5, a0, 1
-; RV32ZBB-NEXT:    srl a1, a5, a1
-; RV32ZBB-NEXT:    or a1, a4, a1
-; RV32ZBB-NEXT:  .LBB7_3:
-; RV32ZBB-NEXT:    neg a5, a2
-; RV32ZBB-NEXT:    andi a4, a5, 63
-; RV32ZBB-NEXT:    addi t0, a4, -32
-; RV32ZBB-NEXT:    bltz t0, .LBB7_5
-; RV32ZBB-NEXT:  # %bb.4:
-; RV32ZBB-NEXT:    srl a3, t1, t0
-; RV32ZBB-NEXT:    bltz a7, .LBB7_6
-; RV32ZBB-NEXT:    j .LBB7_7
-; RV32ZBB-NEXT:  .LBB7_5:
-; RV32ZBB-NEXT:    srl a3, t1, a5
-; RV32ZBB-NEXT:    or a1, a1, a3
-; RV32ZBB-NEXT:    srl a3, a0, a5
-; RV32ZBB-NEXT:    sub a4, a6, a4
-; RV32ZBB-NEXT:    slli a5, t1, 1
-; RV32ZBB-NEXT:    sll a4, a5, a4
-; RV32ZBB-NEXT:    or a3, a3, a4
-; RV32ZBB-NEXT:    bgez a7, .LBB7_7
-; RV32ZBB-NEXT:  .LBB7_6:
-; RV32ZBB-NEXT:    sll a0, a0, a2
-; RV32ZBB-NEXT:    or a3, a3, a0
-; RV32ZBB-NEXT:  .LBB7_7:
-; RV32ZBB-NEXT:    mv a0, a3
-; RV32ZBB-NEXT:    ret
-;
-; RV32ZBP-LABEL: rol_i64:
-; RV32ZBP:       # %bb.0:
-; RV32ZBP-NEXT:    mv t1, a1
-; RV32ZBP-NEXT:    andi a1, a2, 63
-; RV32ZBP-NEXT:    addi a7, a1, -32
-; RV32ZBP-NEXT:    addi a6, zero, 31
-; RV32ZBP-NEXT:    bltz a7, .LBB7_2
-; RV32ZBP-NEXT:  # %bb.1:
-; RV32ZBP-NEXT:    sll a1, a0, a7
-; RV32ZBP-NEXT:    j .LBB7_3
-; RV32ZBP-NEXT:  .LBB7_2:
-; RV32ZBP-NEXT:    sll a4, t1, a2
-; RV32ZBP-NEXT:    sub a1, a6, a1
-; RV32ZBP-NEXT:    srli a5, a0, 1
-; RV32ZBP-NEXT:    srl a1, a5, a1
-; RV32ZBP-NEXT:    or a1, a4, a1
-; RV32ZBP-NEXT:  .LBB7_3:
-; RV32ZBP-NEXT:    neg a5, a2
-; RV32ZBP-NEXT:    andi a4, a5, 63
-; RV32ZBP-NEXT:    addi t0, a4, -32
-; RV32ZBP-NEXT:    bltz t0, .LBB7_5
-; RV32ZBP-NEXT:  # %bb.4:
-; RV32ZBP-NEXT:    srl a3, t1, t0
-; RV32ZBP-NEXT:    bltz a7, .LBB7_6
-; RV32ZBP-NEXT:    j .LBB7_7
-; RV32ZBP-NEXT:  .LBB7_5:
-; RV32ZBP-NEXT:    srl a3, t1, a5
-; RV32ZBP-NEXT:    or a1, a1, a3
-; RV32ZBP-NEXT:    srl a3, a0, a5
-; RV32ZBP-NEXT:    sub a4, a6, a4
-; RV32ZBP-NEXT:    slli a5, t1, 1
-; RV32ZBP-NEXT:    sll a4, a5, a4
-; RV32ZBP-NEXT:    or a3, a3, a4
-; RV32ZBP-NEXT:    bgez a7, .LBB7_7
-; RV32ZBP-NEXT:  .LBB7_6:
-; RV32ZBP-NEXT:    sll a0, a0, a2
-; RV32ZBP-NEXT:    or a3, a3, a0
-; RV32ZBP-NEXT:  .LBB7_7:
-; RV32ZBP-NEXT:    mv a0, a3
-; RV32ZBP-NEXT:    ret
-  %or = tail call i64 @llvm.fshl.i64(i64 %a, i64 %a, i64 %b)
-  ret i64 %or
-}
-
-declare i32 @llvm.fshr.i32(i32, i32, i32)
-
-define i32 @ror_i32(i32 %a, i32 %b) nounwind {
-; RV32I-LABEL: ror_i32:
-; RV32I:       # %bb.0:
-; RV32I-NEXT:    srl a2, a0, a1
-; RV32I-NEXT:    neg a1, a1
-; RV32I-NEXT:    sll a0, a0, a1
-; RV32I-NEXT:    or a0, a2, a0
-; RV32I-NEXT:    ret
-;
-; RV32B-LABEL: ror_i32:
-; RV32B:       # %bb.0:
-; RV32B-NEXT:    ror a0, a0, a1
-; RV32B-NEXT:    ret
-;
-; RV32ZBB-LABEL: ror_i32:
-; RV32ZBB:       # %bb.0:
-; RV32ZBB-NEXT:    ror a0, a0, a1
-; RV32ZBB-NEXT:    ret
-;
-; RV32ZBP-LABEL: ror_i32:
-; RV32ZBP:       # %bb.0:
-; RV32ZBP-NEXT:    ror a0, a0, a1
-; RV32ZBP-NEXT:    ret
-  %or = tail call i32 @llvm.fshr.i32(i32 %a, i32 %a, i32 %b)
-  ret i32 %or
-}
-
-; As we are not matching directly i64 code patterns on RV32 some i64 patterns
-; don't have yet any matching bit manipulation instructions on RV32.
-; This test is presented here in case future expansions of the experimental-b
-; extension introduce instructions suitable for this pattern.
-
-declare i64 @llvm.fshr.i64(i64, i64, i64)
-
-define i64 @ror_i64(i64 %a, i64 %b) nounwind {
-; RV32I-LABEL: ror_i64:
-; RV32I:       # %bb.0:
-; RV32I-NEXT:    mv t1, a0
-; RV32I-NEXT:    andi a0, a2, 63
-; RV32I-NEXT:    addi a7, a0, -32
-; RV32I-NEXT:    addi a6, zero, 31
-; RV32I-NEXT:    bltz a7, .LBB9_2
-; RV32I-NEXT:  # %bb.1:
-; RV32I-NEXT:    srl a0, a1, a7
-; RV32I-NEXT:    j .LBB9_3
-; RV32I-NEXT:  .LBB9_2:
-; RV32I-NEXT:    srl a4, t1, a2
-; RV32I-NEXT:    sub a0, a6, a0
-; RV32I-NEXT:    slli a5, a1, 1
-; RV32I-NEXT:    sll a0, a5, a0
-; RV32I-NEXT:    or a0, a4, a0
-; RV32I-NEXT:  .LBB9_3:
-; RV32I-NEXT:    neg a5, a2
-; RV32I-NEXT:    andi a4, a5, 63
-; RV32I-NEXT:    addi t0, a4, -32
-; RV32I-NEXT:    bltz t0, .LBB9_5
-; RV32I-NEXT:  # %bb.4:
-; RV32I-NEXT:    sll a3, t1, t0
-; RV32I-NEXT:    bltz a7, .LBB9_6
-; RV32I-NEXT:    j .LBB9_7
-; RV32I-NEXT:  .LBB9_5:
-; RV32I-NEXT:    sll a3, t1, a5
-; RV32I-NEXT:    or a0, a0, a3
-; RV32I-NEXT:    sll a3, a1, a5
-; RV32I-NEXT:    sub a4, a6, a4
-; RV32I-NEXT:    srli a5, t1, 1
-; RV32I-NEXT:    srl a4, a5, a4
-; RV32I-NEXT:    or a3, a3, a4
-; RV32I-NEXT:    bgez a7, .LBB9_7
-; RV32I-NEXT:  .LBB9_6:
-; RV32I-NEXT:    srl a1, a1, a2
-; RV32I-NEXT:    or a3, a3, a1
-; RV32I-NEXT:  .LBB9_7:
-; RV32I-NEXT:    mv a1, a3
-; RV32I-NEXT:    ret
-;
-; RV32B-LABEL: ror_i64:
-; RV32B:       # %bb.0:
-; RV32B-NEXT:    srl a7, a0, a2
-; RV32B-NEXT:    andi a4, a2, 63
-; RV32B-NEXT:    addi a6, zero, 31
-; RV32B-NEXT:    sub a5, a6, a4
-; RV32B-NEXT:    slli a3, a1, 1
-; RV32B-NEXT:    sll a3, a3, a5
-; RV32B-NEXT:    or a3, a7, a3
-; RV32B-NEXT:    addi a7, a4, -32
-; RV32B-NEXT:    srl a5, a1, a7
-; RV32B-NEXT:    slti a4, a7, 0
-; RV32B-NEXT:    cmov t0, a4, a3, a5
-; RV32B-NEXT:    neg a4, a2
-; RV32B-NEXT:    sll t2, a0, a4
-; RV32B-NEXT:    andi a3, a4, 63
-; RV32B-NEXT:    addi t1, a3, -32
-; RV32B-NEXT:    srai a5, t1, 31
-; RV32B-NEXT:    and a5, a5, t2
-; RV32B-NEXT:    or t0, t0, a5
-; RV32B-NEXT:    sll a4, a1, a4
-; RV32B-NEXT:    sub a3, a6, a3
-; RV32B-NEXT:    srli a5, a0, 1
-; RV32B-NEXT:    srl a3, a5, a3
-; RV32B-NEXT:    or a3, a4, a3
-; RV32B-NEXT:    sll a0, a0, t1
-; RV32B-NEXT:    slti a4, t1, 0
-; RV32B-NEXT:    cmov a0, a4, a3, a0
-; RV32B-NEXT:    srl a1, a1, a2
-; RV32B-NEXT:    srai a2, a7, 31
-; RV32B-NEXT:    and a1, a2, a1
-; RV32B-NEXT:    or a1, a1, a0
-; RV32B-NEXT:    mv a0, t0
-; RV32B-NEXT:    ret
-;
-; RV32ZBB-LABEL: ror_i64:
-; RV32ZBB:       # %bb.0:
-; RV32ZBB-NEXT:    mv t1, a0
-; RV32ZBB-NEXT:    andi a0, a2, 63
-; RV32ZBB-NEXT:    addi a7, a0, -32
-; RV32ZBB-NEXT:    addi a6, zero, 31
-; RV32ZBB-NEXT:    bltz a7, .LBB9_2
-; RV32ZBB-NEXT:  # %bb.1:
-; RV32ZBB-NEXT:    srl a0, a1, a7
-; RV32ZBB-NEXT:    j .LBB9_3
-; RV32ZBB-NEXT:  .LBB9_2:
-; RV32ZBB-NEXT:    srl a4, t1, a2
-; RV32ZBB-NEXT:    sub a0, a6, a0
-; RV32ZBB-NEXT:    slli a5, a1, 1
-; RV32ZBB-NEXT:    sll a0, a5, a0
-; RV32ZBB-NEXT:    or a0, a4, a0
-; RV32ZBB-NEXT:  .LBB9_3:
-; RV32ZBB-NEXT:    neg a5, a2
-; RV32ZBB-NEXT:    andi a4, a5, 63
-; RV32ZBB-NEXT:    addi t0, a4, -32
-; RV32ZBB-NEXT:    bltz t0, .LBB9_5
-; RV32ZBB-NEXT:  # %bb.4:
-; RV32ZBB-NEXT:    sll a3, t1, t0
-; RV32ZBB-NEXT:    bltz a7, .LBB9_6
-; RV32ZBB-NEXT:    j .LBB9_7
-; RV32ZBB-NEXT:  .LBB9_5:
-; RV32ZBB-NEXT:    sll a3, t1, a5
-; RV32ZBB-NEXT:    or a0, a0, a3
-; RV32ZBB-NEXT:    sll a3, a1, a5
-; RV32ZBB-NEXT:    sub a4, a6, a4
-; RV32ZBB-NEXT:    srli a5, t1, 1
-; RV32ZBB-NEXT:    srl a4, a5, a4
-; RV32ZBB-NEXT:    or a3, a3, a4
-; RV32ZBB-NEXT:    bgez a7, .LBB9_7
-; RV32ZBB-NEXT:  .LBB9_6:
-; RV32ZBB-NEXT:    srl a1, a1, a2
-; RV32ZBB-NEXT:    or a3, a3, a1
-; RV32ZBB-NEXT:  .LBB9_7:
-; RV32ZBB-NEXT:    mv a1, a3
-; RV32ZBB-NEXT:    ret
-;
-; RV32ZBP-LABEL: ror_i64:
-; RV32ZBP:       # %bb.0:
-; RV32ZBP-NEXT:    mv t1, a0
-; RV32ZBP-NEXT:    andi a0, a2, 63
-; RV32ZBP-NEXT:    addi a7, a0, -32
-; RV32ZBP-NEXT:    addi a6, zero, 31
-; RV32ZBP-NEXT:    bltz a7, .LBB9_2
-; RV32ZBP-NEXT:  # %bb.1:
-; RV32ZBP-NEXT:    srl a0, a1, a7
-; RV32ZBP-NEXT:    j .LBB9_3
-; RV32ZBP-NEXT:  .LBB9_2:
-; RV32ZBP-NEXT:    srl a4, t1, a2
-; RV32ZBP-NEXT:    sub a0, a6, a0
-; RV32ZBP-NEXT:    slli a5, a1, 1
-; RV32ZBP-NEXT:    sll a0, a5, a0
-; RV32ZBP-NEXT:    or a0, a4, a0
-; RV32ZBP-NEXT:  .LBB9_3:
-; RV32ZBP-NEXT:    neg a5, a2
-; RV32ZBP-NEXT:    andi a4, a5, 63
-; RV32ZBP-NEXT:    addi t0, a4, -32
-; RV32ZBP-NEXT:    bltz t0, .LBB9_5
-; RV32ZBP-NEXT:  # %bb.4:
-; RV32ZBP-NEXT:    sll a3, t1, t0
-; RV32ZBP-NEXT:    bltz a7, .LBB9_6
-; RV32ZBP-NEXT:    j .LBB9_7
-; RV32ZBP-NEXT:  .LBB9_5:
-; RV32ZBP-NEXT:    sll a3, t1, a5
-; RV32ZBP-NEXT:    or a0, a0, a3
-; RV32ZBP-NEXT:    sll a3, a1, a5
-; RV32ZBP-NEXT:    sub a4, a6, a4
-; RV32ZBP-NEXT:    srli a5, t1, 1
-; RV32ZBP-NEXT:    srl a4, a5, a4
-; RV32ZBP-NEXT:    or a3, a3, a4
-; RV32ZBP-NEXT:    bgez a7, .LBB9_7
-; RV32ZBP-NEXT:  .LBB9_6:
-; RV32ZBP-NEXT:    srl a1, a1, a2
-; RV32ZBP-NEXT:    or a3, a3, a1
-; RV32ZBP-NEXT:  .LBB9_7:
-; RV32ZBP-NEXT:    mv a1, a3
-; RV32ZBP-NEXT:    ret
-  %or = tail call i64 @llvm.fshr.i64(i64 %a, i64 %a, i64 %b)
-  ret i64 %or
-}
-
-define i32 @rori_i32_fshl(i32 %a) nounwind {
-; RV32I-LABEL: rori_i32_fshl:
-; RV32I:       # %bb.0:
-; RV32I-NEXT:    srli a1, a0, 1
-; RV32I-NEXT:    slli a0, a0, 31
-; RV32I-NEXT:    or a0, a0, a1
-; RV32I-NEXT:    ret
-;
-; RV32B-LABEL: rori_i32_fshl:
-; RV32B:       # %bb.0:
-; RV32B-NEXT:    rori a0, a0, 1
-; RV32B-NEXT:    ret
-;
-; RV32ZBB-LABEL: rori_i32_fshl:
-; RV32ZBB:       # %bb.0:
-; RV32ZBB-NEXT:    rori a0, a0, 1
-; RV32ZBB-NEXT:    ret
-;
-; RV32ZBP-LABEL: rori_i32_fshl:
-; RV32ZBP:       # %bb.0:
-; RV32ZBP-NEXT:    rori a0, a0, 1
-; RV32ZBP-NEXT:    ret
-  %1 = tail call i32 @llvm.fshl.i32(i32 %a, i32 %a, i32 31)
-  ret i32 %1
-}
-
-define i32 @rori_i32_fshr(i32 %a) nounwind {
-; RV32I-LABEL: rori_i32_fshr:
-; RV32I:       # %bb.0:
-; RV32I-NEXT:    slli a1, a0, 1
-; RV32I-NEXT:    srli a0, a0, 31
-; RV32I-NEXT:    or a0, a0, a1
-; RV32I-NEXT:    ret
-;
-; RV32B-LABEL: rori_i32_fshr:
-; RV32B:       # %bb.0:
-; RV32B-NEXT:    rori a0, a0, 31
-; RV32B-NEXT:    ret
-;
-; RV32ZBB-LABEL: rori_i32_fshr:
-; RV32ZBB:       # %bb.0:
-; RV32ZBB-NEXT:    rori a0, a0, 31
-; RV32ZBB-NEXT:    ret
-;
-; RV32ZBP-LABEL: rori_i32_fshr:
-; RV32ZBP:       # %bb.0:
-; RV32ZBP-NEXT:    rori a0, a0, 31
-; RV32ZBP-NEXT:    ret
-  %1 = tail call i32 @llvm.fshr.i32(i32 %a, i32 %a, i32 31)
-  ret i32 %1
-}
-
-define i64 @rori_i64(i64 %a) nounwind {
-; RV32I-LABEL: rori_i64:
-; RV32I:       # %bb.0:
-; RV32I-NEXT:    slli a2, a1, 31
-; RV32I-NEXT:    srli a3, a0, 1
-; RV32I-NEXT:    or a2, a3, a2
-; RV32I-NEXT:    srli a1, a1, 1
-; RV32I-NEXT:    slli a0, a0, 31
-; RV32I-NEXT:    or a1, a0, a1
-; RV32I-NEXT:    mv a0, a2
-; RV32I-NEXT:    ret
-;
-; RV32B-LABEL: rori_i64:
-; RV32B:       # %bb.0:
-; RV32B-NEXT:    fsri a2, a0, a1, 1
-; RV32B-NEXT:    fsri a1, a1, a0, 1
-; RV32B-NEXT:    mv a0, a2
-; RV32B-NEXT:    ret
-;
-; RV32ZBB-LABEL: rori_i64:
-; RV32ZBB:       # %bb.0:
-; RV32ZBB-NEXT:    slli a2, a1, 31
-; RV32ZBB-NEXT:    srli a3, a0, 1
-; RV32ZBB-NEXT:    or a2, a3, a2
-; RV32ZBB-NEXT:    srli a1, a1, 1
-; RV32ZBB-NEXT:    slli a0, a0, 31
-; RV32ZBB-NEXT:    or a1, a0, a1
-; RV32ZBB-NEXT:    mv a0, a2
-; RV32ZBB-NEXT:    ret
-;
-; RV32ZBP-LABEL: rori_i64:
-; RV32ZBP:       # %bb.0:
-; RV32ZBP-NEXT:    slli a2, a1, 31
-; RV32ZBP-NEXT:    srli a3, a0, 1
-; RV32ZBP-NEXT:    or a2, a3, a2
-; RV32ZBP-NEXT:    srli a1, a1, 1
-; RV32ZBP-NEXT:    slli a0, a0, 31
-; RV32ZBP-NEXT:    or a1, a0, a1
-; RV32ZBP-NEXT:    mv a0, a2
-; RV32ZBP-NEXT:    ret
-  %1 = tail call i64 @llvm.fshl.i64(i64 %a, i64 %a, i64 63)
-  ret i64 %1
-}
-
-define i64 @rori_i64_fshr(i64 %a) nounwind {
-; RV32I-LABEL: rori_i64_fshr:
-; RV32I:       # %bb.0:
-; RV32I-NEXT:    slli a2, a0, 1
-; RV32I-NEXT:    srli a3, a1, 31
-; RV32I-NEXT:    or a2, a3, a2
-; RV32I-NEXT:    srli a0, a0, 31
-; RV32I-NEXT:    slli a1, a1, 1
-; RV32I-NEXT:    or a1, a1, a0
-; RV32I-NEXT:    mv a0, a2
-; RV32I-NEXT:    ret
-;
-; RV32B-LABEL: rori_i64_fshr:
-; RV32B:       # %bb.0:
-; RV32B-NEXT:    fsri a2, a1, a0, 31
-; RV32B-NEXT:    fsri a1, a0, a1, 31
-; RV32B-NEXT:    mv a0, a2
-; RV32B-NEXT:    ret
-;
-; RV32ZBB-LABEL: rori_i64_fshr:
-; RV32ZBB:       # %bb.0:
-; RV32ZBB-NEXT:    slli a2, a0, 1
-; RV32ZBB-NEXT:    srli a3, a1, 31
-; RV32ZBB-NEXT:    or a2, a3, a2
-; RV32ZBB-NEXT:    srli a0, a0, 31
-; RV32ZBB-NEXT:    slli a1, a1, 1
-; RV32ZBB-NEXT:    or a1, a1, a0
-; RV32ZBB-NEXT:    mv a0, a2
-; RV32ZBB-NEXT:    ret
-;
-; RV32ZBP-LABEL: rori_i64_fshr:
-; RV32ZBP:       # %bb.0:
-; RV32ZBP-NEXT:    slli a2, a0, 1
-; RV32ZBP-NEXT:    srli a3, a1, 31
-; RV32ZBP-NEXT:    or a2, a3, a2
-; RV32ZBP-NEXT:    srli a0, a0, 31
-; RV32ZBP-NEXT:    slli a1, a1, 1
-; RV32ZBP-NEXT:    or a1, a1, a0
-; RV32ZBP-NEXT:    mv a0, a2
-; RV32ZBP-NEXT:    ret
-  %1 = tail call i64 @llvm.fshr.i64(i64 %a, i64 %a, i64 63)
-  ret i64 %1
-}
-
-define i8 @srli_i8(i8 %a) nounwind {
-; RV32I-LABEL: srli_i8:
-; RV32I:       # %bb.0:
-; RV32I-NEXT:    andi a0, a0, 192
-; RV32I-NEXT:    srli a0, a0, 6
-; RV32I-NEXT:    ret
-;
-; RV32B-LABEL: srli_i8:
-; RV32B:       # %bb.0:
-; RV32B-NEXT:    andi a0, a0, 192
-; RV32B-NEXT:    srli a0, a0, 6
-; RV32B-NEXT:    ret
-;
-; RV32ZBB-LABEL: srli_i8:
-; RV32ZBB:       # %bb.0:
-; RV32ZBB-NEXT:    andi a0, a0, 192
-; RV32ZBB-NEXT:    srli a0, a0, 6
-; RV32ZBB-NEXT:    ret
-;
-; RV32ZBP-LABEL: srli_i8:
-; RV32ZBP:       # %bb.0:
-; RV32ZBP-NEXT:    andi a0, a0, 192
-; RV32ZBP-NEXT:    srli a0, a0, 6
-; RV32ZBP-NEXT:    ret
-  %1 = lshr i8 %a, 6
-  ret i8 %1
-}
-
-define i8 @srai_i8(i8 %a) nounwind {
-; RV32I-LABEL: srai_i8:
-; RV32I:       # %bb.0:
-; RV32I-NEXT:    slli a0, a0, 24
-; RV32I-NEXT:    srai a0, a0, 29
-; RV32I-NEXT:    ret
-;
-; RV32B-LABEL: srai_i8:
-; RV32B:       # %bb.0:
-; RV32B-NEXT:    sext.b a0, a0
-; RV32B-NEXT:    srai a0, a0, 5
-; RV32B-NEXT:    ret
-;
-; RV32ZBB-LABEL: srai_i8:
-; RV32ZBB:       # %bb.0:
-; RV32ZBB-NEXT:    sext.b a0, a0
-; RV32ZBB-NEXT:    srai a0, a0, 5
-; RV32ZBB-NEXT:    ret
-;
-; RV32ZBP-LABEL: srai_i8:
-; RV32ZBP:       # %bb.0:
-; RV32ZBP-NEXT:    slli a0, a0, 24
-; RV32ZBP-NEXT:    srai a0, a0, 29
-; RV32ZBP-NEXT:    ret
-  %1 = ashr i8 %a, 5
-  ret i8 %1
-}
-
-define i16 @srli_i16(i16 %a) nounwind {
-; RV32I-LABEL: srli_i16:
-; RV32I:       # %bb.0:
-; RV32I-NEXT:    slli a0, a0, 16
-; RV32I-NEXT:    srli a0, a0, 22
-; RV32I-NEXT:    ret
-;
-; RV32B-LABEL: srli_i16:
-; RV32B:       # %bb.0:
-; RV32B-NEXT:    zext.h a0, a0
-; RV32B-NEXT:    srli a0, a0, 6
-; RV32B-NEXT:    ret
-;
-; RV32ZBB-LABEL: srli_i16:
-; RV32ZBB:       # %bb.0:
-; RV32ZBB-NEXT:    zext.h a0, a0
-; RV32ZBB-NEXT:    srli a0, a0, 6
-; RV32ZBB-NEXT:    ret
-;
-; RV32ZBP-LABEL: srli_i16:
-; RV32ZBP:       # %bb.0:
-; RV32ZBP-NEXT:    zext.h a0, a0
-; RV32ZBP-NEXT:    srli a0, a0, 6
-; RV32ZBP-NEXT:    ret
-  %1 = lshr i16 %a, 6
-  ret i16 %1
-}
-
-define i16 @srai_i16(i16 %a) nounwind {
-; RV32I-LABEL: srai_i16:
-; RV32I:       # %bb.0:
-; RV32I-NEXT:    slli a0, a0, 16
-; RV32I-NEXT:    srai a0, a0, 25
-; RV32I-NEXT:    ret
-;
-; RV32B-LABEL: srai_i16:
-; RV32B:       # %bb.0:
-; RV32B-NEXT:    sext.h a0, a0
-; RV32B-NEXT:    srai a0, a0, 9
-; RV32B-NEXT:    ret
-;
-; RV32ZBB-LABEL: srai_i16:
-; RV32ZBB:       # %bb.0:
-; RV32ZBB-NEXT:    sext.h a0, a0
-; RV32ZBB-NEXT:    srai a0, a0, 9
-; RV32ZBB-NEXT:    ret
-;
-; RV32ZBP-LABEL: srai_i16:
-; RV32ZBP:       # %bb.0:
-; RV32ZBP-NEXT:    slli a0, a0, 16
-; RV32ZBP-NEXT:    srai a0, a0, 25
-; RV32ZBP-NEXT:    ret
-  %1 = ashr i16 %a, 9
-  ret i16 %1
-}
diff --git a/llvm/test/CodeGen/RISCV/rv32zbe-intrinsic.ll b/llvm/test/CodeGen/RISCV/rv32zbe-intrinsic.ll
deleted file mode 100644
index fc97f4e3c825..000000000000
--- a/llvm/test/CodeGen/RISCV/rv32zbe-intrinsic.ll
+++ /dev/null
@@ -1,37 +0,0 @@
-; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
-; RUN: llc -mtriple=riscv32 -mattr=+experimental-b -verify-machineinstrs < %s \
-; RUN:   | FileCheck %s -check-prefix=RV32B
-; RUN: llc -mtriple=riscv32 -mattr=+experimental-zbe -verify-machineinstrs < %s \
-; RUN:   | FileCheck %s -check-prefix=RV32ZBE
-
-declare i32 @llvm.riscv.bcompress.i32(i32 %a, i32 %b)
-
-define i32 @bcompress32(i32 %a, i32 %b) nounwind {
-; RV32B-LABEL: bcompress32:
-; RV32B:       # %bb.0:
-; RV32B-NEXT:    bcompress a0, a0, a1
-; RV32B-NEXT:    ret
-;
-; RV32ZBE-LABEL: bcompress32:
-; RV32ZBE:       # %bb.0:
-; RV32ZBE-NEXT:    bcompress a0, a0, a1
-; RV32ZBE-NEXT:    ret
-  %tmp = call i32 @llvm.riscv.bcompress.i32(i32 %a, i32 %b)
- ret i32 %tmp
-}
-
-declare i32 @llvm.riscv.bdecompress.i32(i32 %a, i32 %b)
-
-define i32 @bdecompress32(i32 %a, i32 %b) nounwind {
-; RV32B-LABEL: bdecompress32:
-; RV32B:       # %bb.0:
-; RV32B-NEXT:    bdecompress a0, a0, a1
-; RV32B-NEXT:    ret
-;
-; RV32ZBE-LABEL: bdecompress32:
-; RV32ZBE:       # %bb.0:
-; RV32ZBE-NEXT:    bdecompress a0, a0, a1
-; RV32ZBE-NEXT:    ret
-  %tmp = call i32 @llvm.riscv.bdecompress.i32(i32 %a, i32 %b)
- ret i32 %tmp
-}
diff --git a/llvm/test/CodeGen/RISCV/rv32zbp-intrinsic.ll b/llvm/test/CodeGen/RISCV/rv32zbp-intrinsic.ll
deleted file mode 100644
index e5c3c40f7034..000000000000
--- a/llvm/test/CodeGen/RISCV/rv32zbp-intrinsic.ll
+++ /dev/null
@@ -1,233 +0,0 @@
-; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
-; RUN: llc -mtriple=riscv32 -mattr=+experimental-b -verify-machineinstrs < %s \
-; RUN:   | FileCheck %s -check-prefix=RV32B
-; RUN: llc -mtriple=riscv32 -mattr=+experimental-zbp -verify-machineinstrs < %s \
-; RUN:   | FileCheck %s -check-prefix=RV32ZBP
-
-declare i32 @llvm.riscv.grev.i32(i32 %a, i32 %b)
-
-define i32 @grev32(i32 %a, i32 %b) nounwind {
-; RV32B-LABEL: grev32:
-; RV32B:       # %bb.0:
-; RV32B-NEXT:    grev a0, a0, a1
-; RV32B-NEXT:    ret
-;
-; RV32ZBP-LABEL: grev32:
-; RV32ZBP:       # %bb.0:
-; RV32ZBP-NEXT:    grev a0, a0, a1
-; RV32ZBP-NEXT:    ret
-  %tmp = call i32 @llvm.riscv.grev.i32(i32 %a, i32 %b)
- ret i32 %tmp
-}
-
-define i32 @grev32_demandedbits(i32 %a, i32 %b) nounwind {
-; RV32B-LABEL: grev32_demandedbits:
-; RV32B:       # %bb.0:
-; RV32B-NEXT:    grev a0, a0, a1
-; RV32B-NEXT:    ret
-;
-; RV32ZBP-LABEL: grev32_demandedbits:
-; RV32ZBP:       # %bb.0:
-; RV32ZBP-NEXT:    grev a0, a0, a1
-; RV32ZBP-NEXT:    ret
-  %c = and i32 %b, 31
-  %tmp = call i32 @llvm.riscv.grev.i32(i32 %a, i32 %b)
-  ret i32 %tmp
-}
-
-define i32 @grevi32(i32 %a) nounwind {
-; RV32B-LABEL: grevi32:
-; RV32B:       # %bb.0:
-; RV32B-NEXT:    grevi a0, a0, 13
-; RV32B-NEXT:    ret
-;
-; RV32ZBP-LABEL: grevi32:
-; RV32ZBP:       # %bb.0:
-; RV32ZBP-NEXT:    grevi a0, a0, 13
-; RV32ZBP-NEXT:    ret
-  %tmp = call i32 @llvm.riscv.grev.i32(i32 %a, i32 13)
- ret i32 %tmp
-}
-
-declare i32 @llvm.riscv.gorc.i32(i32 %a, i32 %b)
-
-define i32 @gorc32(i32 %a, i32 %b) nounwind {
-; RV32B-LABEL: gorc32:
-; RV32B:       # %bb.0:
-; RV32B-NEXT:    gorc a0, a0, a1
-; RV32B-NEXT:    ret
-;
-; RV32ZBP-LABEL: gorc32:
-; RV32ZBP:       # %bb.0:
-; RV32ZBP-NEXT:    gorc a0, a0, a1
-; RV32ZBP-NEXT:    ret
-  %tmp = call i32 @llvm.riscv.gorc.i32(i32 %a, i32 %b)
- ret i32 %tmp
-}
-
-define i32 @gorc32_demandedbits(i32 %a, i32 %b) nounwind {
-; RV32B-LABEL: gorc32_demandedbits:
-; RV32B:       # %bb.0:
-; RV32B-NEXT:    gorc a0, a0, a1
-; RV32B-NEXT:    ret
-;
-; RV32ZBP-LABEL: gorc32_demandedbits:
-; RV32ZBP:       # %bb.0:
-; RV32ZBP-NEXT:    gorc a0, a0, a1
-; RV32ZBP-NEXT:    ret
-  %c = and i32 %b, 31
-  %tmp = call i32 @llvm.riscv.gorc.i32(i32 %a, i32 %b)
-  ret i32 %tmp
-}
-
-define i32 @gorci32(i32 %a) nounwind {
-; RV32B-LABEL: gorci32:
-; RV32B:       # %bb.0:
-; RV32B-NEXT:    gorci a0, a0, 13
-; RV32B-NEXT:    ret
-;
-; RV32ZBP-LABEL: gorci32:
-; RV32ZBP:       # %bb.0:
-; RV32ZBP-NEXT:    gorci a0, a0, 13
-; RV32ZBP-NEXT:    ret
-  %tmp = call i32 @llvm.riscv.gorc.i32(i32 %a, i32 13)
- ret i32 %tmp
-}
-
-declare i32 @llvm.riscv.shfl.i32(i32 %a, i32 %b)
-
-define i32 @shfl32(i32 %a, i32 %b) nounwind {
-; RV32B-LABEL: shfl32:
-; RV32B:       # %bb.0:
-; RV32B-NEXT:    shfl a0, a0, a1
-; RV32B-NEXT:    ret
-;
-; RV32ZBP-LABEL: shfl32:
-; RV32ZBP:       # %bb.0:
-; RV32ZBP-NEXT:    shfl a0, a0, a1
-; RV32ZBP-NEXT:    ret
-  %tmp = call i32 @llvm.riscv.shfl.i32(i32 %a, i32 %b)
- ret i32 %tmp
-}
-
-define i32 @shfl32_demandedbits(i32 %a, i32 %b) nounwind {
-; RV32B-LABEL: shfl32_demandedbits:
-; RV32B:       # %bb.0:
-; RV32B-NEXT:    shfl a0, a0, a1
-; RV32B-NEXT:    ret
-;
-; RV32ZBP-LABEL: shfl32_demandedbits:
-; RV32ZBP:       # %bb.0:
-; RV32ZBP-NEXT:    shfl a0, a0, a1
-; RV32ZBP-NEXT:    ret
-  %c = and i32 %b, 15
-  %tmp = call i32 @llvm.riscv.shfl.i32(i32 %a, i32 %c)
-  ret i32 %tmp
-}
-
-define i32 @shfli32(i32 %a) nounwind {
-; RV32B-LABEL: shfli32:
-; RV32B:       # %bb.0:
-; RV32B-NEXT:    shfli a0, a0, 13
-; RV32B-NEXT:    ret
-;
-; RV32ZBP-LABEL: shfli32:
-; RV32ZBP:       # %bb.0:
-; RV32ZBP-NEXT:    shfli a0, a0, 13
-; RV32ZBP-NEXT:    ret
-  %tmp = call i32 @llvm.riscv.shfl.i32(i32 %a, i32 13)
- ret i32 %tmp
-}
-
-declare i32 @llvm.riscv.unshfl.i32(i32 %a, i32 %b)
-
-define i32 @unshfl32(i32 %a, i32 %b) nounwind {
-; RV32B-LABEL: unshfl32:
-; RV32B:       # %bb.0:
-; RV32B-NEXT:    unshfl a0, a0, a1
-; RV32B-NEXT:    ret
-;
-; RV32ZBP-LABEL: unshfl32:
-; RV32ZBP:       # %bb.0:
-; RV32ZBP-NEXT:    unshfl a0, a0, a1
-; RV32ZBP-NEXT:    ret
-  %tmp = call i32 @llvm.riscv.unshfl.i32(i32 %a, i32 %b)
- ret i32 %tmp
-}
-
-define i32 @unshfl32_demandedbits(i32 %a, i32 %b) nounwind {
-; RV32B-LABEL: unshfl32_demandedbits:
-; RV32B:       # %bb.0:
-; RV32B-NEXT:    unshfl a0, a0, a1
-; RV32B-NEXT:    ret
-;
-; RV32ZBP-LABEL: unshfl32_demandedbits:
-; RV32ZBP:       # %bb.0:
-; RV32ZBP-NEXT:    unshfl a0, a0, a1
-; RV32ZBP-NEXT:    ret
-  %c = and i32 %b, 15
-  %tmp = call i32 @llvm.riscv.unshfl.i32(i32 %a, i32 %c)
-  ret i32 %tmp
-}
-
-define i32 @unshfli32(i32 %a) nounwind {
-; RV32B-LABEL: unshfli32:
-; RV32B:       # %bb.0:
-; RV32B-NEXT:    unshfli a0, a0, 13
-; RV32B-NEXT:    ret
-;
-; RV32ZBP-LABEL: unshfli32:
-; RV32ZBP:       # %bb.0:
-; RV32ZBP-NEXT:    unshfli a0, a0, 13
-; RV32ZBP-NEXT:    ret
-  %tmp = call i32 @llvm.riscv.unshfl.i32(i32 %a, i32 13)
- ret i32 %tmp
-}
-
-declare i32 @llvm.riscv.xperm.n.i32(i32 %a, i32 %b)
-
-define i32 @xpermn32(i32 %a, i32 %b) nounwind {
-; RV32B-LABEL: xpermn32:
-; RV32B:       # %bb.0:
-; RV32B-NEXT:    xperm.n a0, a0, a1
-; RV32B-NEXT:    ret
-;
-; RV32ZBP-LABEL: xpermn32:
-; RV32ZBP:       # %bb.0:
-; RV32ZBP-NEXT:    xperm.n a0, a0, a1
-; RV32ZBP-NEXT:    ret
-  %tmp = call i32 @llvm.riscv.xperm.n.i32(i32 %a, i32 %b)
- ret i32 %tmp
-}
-
-declare i32 @llvm.riscv.xperm.b.i32(i32 %a, i32 %b)
-
-define i32 @xpermb32(i32 %a, i32 %b) nounwind {
-; RV32B-LABEL: xpermb32:
-; RV32B:       # %bb.0:
-; RV32B-NEXT:    xperm.b a0, a0, a1
-; RV32B-NEXT:    ret
-;
-; RV32ZBP-LABEL: xpermb32:
-; RV32ZBP:       # %bb.0:
-; RV32ZBP-NEXT:    xperm.b a0, a0, a1
-; RV32ZBP-NEXT:    ret
-  %tmp = call i32 @llvm.riscv.xperm.b.i32(i32 %a, i32 %b)
- ret i32 %tmp
-}
-
-declare i32 @llvm.riscv.xperm.h.i32(i32 %a, i32 %b)
-
-define i32 @xpermh32(i32 %a, i32 %b) nounwind {
-; RV32B-LABEL: xpermh32:
-; RV32B:       # %bb.0:
-; RV32B-NEXT:    xperm.h a0, a0, a1
-; RV32B-NEXT:    ret
-;
-; RV32ZBP-LABEL: xpermh32:
-; RV32ZBP:       # %bb.0:
-; RV32ZBP-NEXT:    xperm.h a0, a0, a1
-; RV32ZBP-NEXT:    ret
-  %tmp = call i32 @llvm.riscv.xperm.h.i32(i32 %a, i32 %b)
- ret i32 %tmp
-}
diff --git a/llvm/test/CodeGen/RISCV/rv32zbp.ll b/llvm/test/CodeGen/RISCV/rv32zbp.ll
deleted file mode 100644
index 32ef963abab8..000000000000
--- a/llvm/test/CodeGen/RISCV/rv32zbp.ll
+++ /dev/null
@@ -1,3413 +0,0 @@
-; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
-; RUN: llc -mtriple=riscv32 -verify-machineinstrs < %s \
-; RUN:   | FileCheck %s -check-prefix=RV32I
-; RUN: llc -mtriple=riscv32 -mattr=+experimental-b -verify-machineinstrs < %s \
-; RUN:   | FileCheck %s -check-prefix=RV32B
-; RUN: llc -mtriple=riscv32 -mattr=+experimental-zbp -verify-machineinstrs < %s \
-; RUN:   | FileCheck %s -check-prefix=RV32ZBP
-
-define i32 @gorc1_i32(i32 %a) nounwind {
-; RV32I-LABEL: gorc1_i32:
-; RV32I:       # %bb.0:
-; RV32I-NEXT:    slli a1, a0, 1
-; RV32I-NEXT:    lui a2, 699051
-; RV32I-NEXT:    addi a2, a2, -1366
-; RV32I-NEXT:    and a1, a1, a2
-; RV32I-NEXT:    srli a2, a0, 1
-; RV32I-NEXT:    lui a3, 349525
-; RV32I-NEXT:    addi a3, a3, 1365
-; RV32I-NEXT:    and a2, a2, a3
-; RV32I-NEXT:    or a0, a2, a0
-; RV32I-NEXT:    or a0, a0, a1
-; RV32I-NEXT:    ret
-;
-; RV32B-LABEL: gorc1_i32:
-; RV32B:       # %bb.0:
-; RV32B-NEXT:    orc.p a0, a0
-; RV32B-NEXT:    ret
-;
-; RV32ZBP-LABEL: gorc1_i32:
-; RV32ZBP:       # %bb.0:
-; RV32ZBP-NEXT:    orc.p a0, a0
-; RV32ZBP-NEXT:    ret
-  %and = shl i32 %a, 1
-  %shl = and i32 %and, -1431655766
-  %and1 = lshr i32 %a, 1
-  %shr = and i32 %and1, 1431655765
-  %or = or i32 %shr, %a
-  %or2 = or i32 %or, %shl
-  ret i32 %or2
-}
-
-define i64 @gorc1_i64(i64 %a) nounwind {
-; RV32I-LABEL: gorc1_i64:
-; RV32I:       # %bb.0:
-; RV32I-NEXT:    slli a2, a0, 1
-; RV32I-NEXT:    slli a3, a1, 1
-; RV32I-NEXT:    lui a4, 699051
-; RV32I-NEXT:    addi a4, a4, -1366
-; RV32I-NEXT:    and a6, a3, a4
-; RV32I-NEXT:    and a2, a2, a4
-; RV32I-NEXT:    srli a4, a1, 1
-; RV32I-NEXT:    srli a5, a0, 1
-; RV32I-NEXT:    lui a3, 349525
-; RV32I-NEXT:    addi a3, a3, 1365
-; RV32I-NEXT:    and a5, a5, a3
-; RV32I-NEXT:    and a3, a4, a3
-; RV32I-NEXT:    or a1, a3, a1
-; RV32I-NEXT:    or a0, a5, a0
-; RV32I-NEXT:    or a0, a0, a2
-; RV32I-NEXT:    or a1, a1, a6
-; RV32I-NEXT:    ret
-;
-; RV32B-LABEL: gorc1_i64:
-; RV32B:       # %bb.0:
-; RV32B-NEXT:    orc.p a0, a0
-; RV32B-NEXT:    orc.p a1, a1
-; RV32B-NEXT:    ret
-;
-; RV32ZBP-LABEL: gorc1_i64:
-; RV32ZBP:       # %bb.0:
-; RV32ZBP-NEXT:    orc.p a0, a0
-; RV32ZBP-NEXT:    orc.p a1, a1
-; RV32ZBP-NEXT:    ret
-  %and = shl i64 %a, 1
-  %shl = and i64 %and, -6148914691236517206
-  %and1 = lshr i64 %a, 1
-  %shr = and i64 %and1, 6148914691236517205
-  %or = or i64 %shr, %a
-  %or2 = or i64 %or, %shl
-  ret i64 %or2
-}
-
-define i32 @gorc2_i32(i32 %a) nounwind {
-; RV32I-LABEL: gorc2_i32:
-; RV32I:       # %bb.0:
-; RV32I-NEXT:    slli a1, a0, 2
-; RV32I-NEXT:    lui a2, 838861
-; RV32I-NEXT:    addi a2, a2, -820
-; RV32I-NEXT:    and a1, a1, a2
-; RV32I-NEXT:    srli a2, a0, 2
-; RV32I-NEXT:    lui a3, 209715
-; RV32I-NEXT:    addi a3, a3, 819
-; RV32I-NEXT:    and a2, a2, a3
-; RV32I-NEXT:    or a0, a2, a0
-; RV32I-NEXT:    or a0, a0, a1
-; RV32I-NEXT:    ret
-;
-; RV32B-LABEL: gorc2_i32:
-; RV32B:       # %bb.0:
-; RV32B-NEXT:    orc2.n a0, a0
-; RV32B-NEXT:    ret
-;
-; RV32ZBP-LABEL: gorc2_i32:
-; RV32ZBP:       # %bb.0:
-; RV32ZBP-NEXT:    orc2.n a0, a0
-; RV32ZBP-NEXT:    ret
-  %and = shl i32 %a, 2
-  %shl = and i32 %and, -858993460
-  %and1 = lshr i32 %a, 2
-  %shr = and i32 %and1, 858993459
-  %or = or i32 %shr, %a
-  %or2 = or i32 %or, %shl
-  ret i32 %or2
-}
-
-define i64 @gorc2_i64(i64 %a) nounwind {
-; RV32I-LABEL: gorc2_i64:
-; RV32I:       # %bb.0:
-; RV32I-NEXT:    slli a2, a0, 2
-; RV32I-NEXT:    slli a3, a1, 2
-; RV32I-NEXT:    lui a4, 838861
-; RV32I-NEXT:    addi a4, a4, -820
-; RV32I-NEXT:    and a6, a3, a4
-; RV32I-NEXT:    and a2, a2, a4
-; RV32I-NEXT:    srli a4, a1, 2
-; RV32I-NEXT:    srli a5, a0, 2
-; RV32I-NEXT:    lui a3, 209715
-; RV32I-NEXT:    addi a3, a3, 819
-; RV32I-NEXT:    and a5, a5, a3
-; RV32I-NEXT:    and a3, a4, a3
-; RV32I-NEXT:    or a1, a3, a1
-; RV32I-NEXT:    or a0, a5, a0
-; RV32I-NEXT:    or a0, a0, a2
-; RV32I-NEXT:    or a1, a1, a6
-; RV32I-NEXT:    ret
-;
-; RV32B-LABEL: gorc2_i64:
-; RV32B:       # %bb.0:
-; RV32B-NEXT:    orc2.n a0, a0
-; RV32B-NEXT:    orc2.n a1, a1
-; RV32B-NEXT:    ret
-;
-; RV32ZBP-LABEL: gorc2_i64:
-; RV32ZBP:       # %bb.0:
-; RV32ZBP-NEXT:    orc2.n a0, a0
-; RV32ZBP-NEXT:    orc2.n a1, a1
-; RV32ZBP-NEXT:    ret
-  %and = shl i64 %a, 2
-  %shl = and i64 %and, -3689348814741910324
-  %and1 = lshr i64 %a, 2
-  %shr = and i64 %and1, 3689348814741910323
-  %or = or i64 %shr, %a
-  %or2 = or i64 %or, %shl
-  ret i64 %or2
-}
-
-define i32 @gorc3_i32(i32 %a) nounwind {
-; RV32I-LABEL: gorc3_i32:
-; RV32I:       # %bb.0:
-; RV32I-NEXT:    slli a1, a0, 1
-; RV32I-NEXT:    lui a2, 699051
-; RV32I-NEXT:    addi a2, a2, -1366
-; RV32I-NEXT:    and a1, a1, a2
-; RV32I-NEXT:    srli a2, a0, 1
-; RV32I-NEXT:    lui a3, 349525
-; RV32I-NEXT:    addi a3, a3, 1365
-; RV32I-NEXT:    and a2, a2, a3
-; RV32I-NEXT:    or a0, a2, a0
-; RV32I-NEXT:    or a0, a0, a1
-; RV32I-NEXT:    slli a1, a0, 2
-; RV32I-NEXT:    lui a2, 838861
-; RV32I-NEXT:    addi a2, a2, -820
-; RV32I-NEXT:    and a1, a1, a2
-; RV32I-NEXT:    srli a2, a0, 2
-; RV32I-NEXT:    lui a3, 209715
-; RV32I-NEXT:    addi a3, a3, 819
-; RV32I-NEXT:    and a2, a2, a3
-; RV32I-NEXT:    or a0, a2, a0
-; RV32I-NEXT:    or a0, a0, a1
-; RV32I-NEXT:    ret
-;
-; RV32B-LABEL: gorc3_i32:
-; RV32B:       # %bb.0:
-; RV32B-NEXT:    orc.n a0, a0
-; RV32B-NEXT:    ret
-;
-; RV32ZBP-LABEL: gorc3_i32:
-; RV32ZBP:       # %bb.0:
-; RV32ZBP-NEXT:    orc.n a0, a0
-; RV32ZBP-NEXT:    ret
-  %and1 = shl i32 %a, 1
-  %shl1 = and i32 %and1, -1431655766
-  %and1b = lshr i32 %a, 1
-  %shr1 = and i32 %and1b, 1431655765
-  %or1 = or i32 %shr1, %a
-  %or1b = or i32 %or1, %shl1
-  %and2 = shl i32 %or1b, 2
-  %shl2 = and i32 %and2, -858993460
-  %and2b = lshr i32 %or1b, 2
-  %shr2 = and i32 %and2b, 858993459
-  %or2 = or i32 %shr2, %or1b
-  %or2b = or i32 %or2, %shl2
-  ret i32 %or2b
-}
-
-define i64 @gorc3_i64(i64 %a) nounwind {
-; RV32I-LABEL: gorc3_i64:
-; RV32I:       # %bb.0:
-; RV32I-NEXT:    slli a2, a1, 1
-; RV32I-NEXT:    slli a3, a0, 1
-; RV32I-NEXT:    lui a4, 699051
-; RV32I-NEXT:    addi a4, a4, -1366
-; RV32I-NEXT:    and a6, a3, a4
-; RV32I-NEXT:    and a2, a2, a4
-; RV32I-NEXT:    srli a4, a0, 1
-; RV32I-NEXT:    srli a5, a1, 1
-; RV32I-NEXT:    lui a3, 349525
-; RV32I-NEXT:    addi a3, a3, 1365
-; RV32I-NEXT:    and a5, a5, a3
-; RV32I-NEXT:    and a3, a4, a3
-; RV32I-NEXT:    or a0, a3, a0
-; RV32I-NEXT:    or a1, a5, a1
-; RV32I-NEXT:    or a1, a1, a2
-; RV32I-NEXT:    or a0, a0, a6
-; RV32I-NEXT:    slli a2, a0, 2
-; RV32I-NEXT:    slli a3, a1, 2
-; RV32I-NEXT:    lui a4, 838861
-; RV32I-NEXT:    addi a4, a4, -820
-; RV32I-NEXT:    and a6, a3, a4
-; RV32I-NEXT:    and a2, a2, a4
-; RV32I-NEXT:    srli a4, a1, 2
-; RV32I-NEXT:    srli a5, a0, 2
-; RV32I-NEXT:    lui a3, 209715
-; RV32I-NEXT:    addi a3, a3, 819
-; RV32I-NEXT:    and a5, a5, a3
-; RV32I-NEXT:    and a3, a4, a3
-; RV32I-NEXT:    or a1, a3, a1
-; RV32I-NEXT:    or a0, a5, a0
-; RV32I-NEXT:    or a0, a0, a2
-; RV32I-NEXT:    or a1, a1, a6
-; RV32I-NEXT:    ret
-;
-; RV32B-LABEL: gorc3_i64:
-; RV32B:       # %bb.0:
-; RV32B-NEXT:    orc.n a0, a0
-; RV32B-NEXT:    orc.n a1, a1
-; RV32B-NEXT:    ret
-;
-; RV32ZBP-LABEL: gorc3_i64:
-; RV32ZBP:       # %bb.0:
-; RV32ZBP-NEXT:    orc.n a0, a0
-; RV32ZBP-NEXT:    orc.n a1, a1
-; RV32ZBP-NEXT:    ret
-  %and1 = shl i64 %a, 1
-  %shl1 = and i64 %and1, -6148914691236517206
-  %and1b = lshr i64 %a, 1
-  %shr1 = and i64 %and1b, 6148914691236517205
-  %or1 = or i64 %shr1, %a
-  %or1b = or i64 %or1, %shl1
-  %and2 = shl i64 %or1b, 2
-  %shl2 = and i64 %and2, -3689348814741910324
-  %and2b = lshr i64 %or1b, 2
-  %shr2 = and i64 %and2b, 3689348814741910323
-  %or2 = or i64 %shr2, %or1b
-  %or2b = or i64 %or2, %shl2
-  ret i64 %or2b
-}
-
-define i32 @gorc4_i32(i32 %a) nounwind {
-; RV32I-LABEL: gorc4_i32:
-; RV32I:       # %bb.0:
-; RV32I-NEXT:    slli a1, a0, 4
-; RV32I-NEXT:    lui a2, 986895
-; RV32I-NEXT:    addi a2, a2, 240
-; RV32I-NEXT:    and a1, a1, a2
-; RV32I-NEXT:    srli a2, a0, 4
-; RV32I-NEXT:    lui a3, 61681
-; RV32I-NEXT:    addi a3, a3, -241
-; RV32I-NEXT:    and a2, a2, a3
-; RV32I-NEXT:    or a0, a2, a0
-; RV32I-NEXT:    or a0, a0, a1
-; RV32I-NEXT:    ret
-;
-; RV32B-LABEL: gorc4_i32:
-; RV32B:       # %bb.0:
-; RV32B-NEXT:    orc4.b a0, a0
-; RV32B-NEXT:    ret
-;
-; RV32ZBP-LABEL: gorc4_i32:
-; RV32ZBP:       # %bb.0:
-; RV32ZBP-NEXT:    orc4.b a0, a0
-; RV32ZBP-NEXT:    ret
-  %and = shl i32 %a, 4
-  %shl = and i32 %and, -252645136
-  %and1 = lshr i32 %a, 4
-  %shr = and i32 %and1, 252645135
-  %or = or i32 %shr, %a
-  %or2 = or i32 %or, %shl
-  ret i32 %or2
-}
-
-define i64 @gorc4_i64(i64 %a) nounwind {
-; RV32I-LABEL: gorc4_i64:
-; RV32I:       # %bb.0:
-; RV32I-NEXT:    slli a2, a0, 4
-; RV32I-NEXT:    slli a3, a1, 4
-; RV32I-NEXT:    lui a4, 986895
-; RV32I-NEXT:    addi a4, a4, 240
-; RV32I-NEXT:    and a6, a3, a4
-; RV32I-NEXT:    and a2, a2, a4
-; RV32I-NEXT:    srli a4, a1, 4
-; RV32I-NEXT:    srli a5, a0, 4
-; RV32I-NEXT:    lui a3, 61681
-; RV32I-NEXT:    addi a3, a3, -241
-; RV32I-NEXT:    and a5, a5, a3
-; RV32I-NEXT:    and a3, a4, a3
-; RV32I-NEXT:    or a1, a3, a1
-; RV32I-NEXT:    or a0, a5, a0
-; RV32I-NEXT:    or a0, a0, a2
-; RV32I-NEXT:    or a1, a1, a6
-; RV32I-NEXT:    ret
-;
-; RV32B-LABEL: gorc4_i64:
-; RV32B:       # %bb.0:
-; RV32B-NEXT:    orc4.b a0, a0
-; RV32B-NEXT:    orc4.b a1, a1
-; RV32B-NEXT:    ret
-;
-; RV32ZBP-LABEL: gorc4_i64:
-; RV32ZBP:       # %bb.0:
-; RV32ZBP-NEXT:    orc4.b a0, a0
-; RV32ZBP-NEXT:    orc4.b a1, a1
-; RV32ZBP-NEXT:    ret
-  %and = shl i64 %a, 4
-  %shl = and i64 %and, -1085102592571150096
-  %and1 = lshr i64 %a, 4
-  %shr = and i64 %and1, 1085102592571150095
-  %or = or i64 %shr, %a
-  %or2 = or i64 %or, %shl
-  ret i64 %or2
-}
-
-define i32 @gorc5_i32(i32 %a) nounwind {
-; RV32I-LABEL: gorc5_i32:
-; RV32I:       # %bb.0:
-; RV32I-NEXT:    slli a1, a0, 1
-; RV32I-NEXT:    lui a2, 699051
-; RV32I-NEXT:    addi a2, a2, -1366
-; RV32I-NEXT:    and a1, a1, a2
-; RV32I-NEXT:    srli a2, a0, 1
-; RV32I-NEXT:    lui a3, 349525
-; RV32I-NEXT:    addi a3, a3, 1365
-; RV32I-NEXT:    and a2, a2, a3
-; RV32I-NEXT:    or a0, a2, a0
-; RV32I-NEXT:    or a0, a0, a1
-; RV32I-NEXT:    slli a1, a0, 4
-; RV32I-NEXT:    lui a2, 986895
-; RV32I-NEXT:    addi a2, a2, 240
-; RV32I-NEXT:    and a1, a1, a2
-; RV32I-NEXT:    srli a2, a0, 4
-; RV32I-NEXT:    lui a3, 61681
-; RV32I-NEXT:    addi a3, a3, -241
-; RV32I-NEXT:    and a2, a2, a3
-; RV32I-NEXT:    or a0, a2, a0
-; RV32I-NEXT:    or a0, a0, a1
-; RV32I-NEXT:    ret
-;
-; RV32B-LABEL: gorc5_i32:
-; RV32B:       # %bb.0:
-; RV32B-NEXT:    gorci a0, a0, 5
-; RV32B-NEXT:    ret
-;
-; RV32ZBP-LABEL: gorc5_i32:
-; RV32ZBP:       # %bb.0:
-; RV32ZBP-NEXT:    gorci a0, a0, 5
-; RV32ZBP-NEXT:    ret
-  %and1 = shl i32 %a, 1
-  %shl1 = and i32 %and1, -1431655766
-  %and1b = lshr i32 %a, 1
-  %shr1 = and i32 %and1b, 1431655765
-  %or1 = or i32 %shr1, %a
-  %or1b = or i32 %or1, %shl1
-  %and2 = shl i32 %or1b, 4
-  %shl2 = and i32 %and2, -252645136
-  %and2b = lshr i32 %or1b, 4
-  %shr2 = and i32 %and2b, 252645135
-  %or2 = or i32 %shr2, %or1b
-  %or2b = or i32 %or2, %shl2
-  ret i32 %or2b
-}
-
-define i64 @gorc5_i64(i64 %a) nounwind {
-; RV32I-LABEL: gorc5_i64:
-; RV32I:       # %bb.0:
-; RV32I-NEXT:    slli a2, a1, 1
-; RV32I-NEXT:    slli a3, a0, 1
-; RV32I-NEXT:    lui a4, 699051
-; RV32I-NEXT:    addi a4, a4, -1366
-; RV32I-NEXT:    and a6, a3, a4
-; RV32I-NEXT:    and a2, a2, a4
-; RV32I-NEXT:    srli a4, a0, 1
-; RV32I-NEXT:    srli a5, a1, 1
-; RV32I-NEXT:    lui a3, 349525
-; RV32I-NEXT:    addi a3, a3, 1365
-; RV32I-NEXT:    and a5, a5, a3
-; RV32I-NEXT:    and a3, a4, a3
-; RV32I-NEXT:    or a0, a3, a0
-; RV32I-NEXT:    or a1, a5, a1
-; RV32I-NEXT:    or a1, a1, a2
-; RV32I-NEXT:    or a0, a0, a6
-; RV32I-NEXT:    slli a2, a0, 4
-; RV32I-NEXT:    slli a3, a1, 4
-; RV32I-NEXT:    lui a4, 986895
-; RV32I-NEXT:    addi a4, a4, 240
-; RV32I-NEXT:    and a6, a3, a4
-; RV32I-NEXT:    and a2, a2, a4
-; RV32I-NEXT:    srli a4, a1, 4
-; RV32I-NEXT:    srli a5, a0, 4
-; RV32I-NEXT:    lui a3, 61681
-; RV32I-NEXT:    addi a3, a3, -241
-; RV32I-NEXT:    and a5, a5, a3
-; RV32I-NEXT:    and a3, a4, a3
-; RV32I-NEXT:    or a1, a3, a1
-; RV32I-NEXT:    or a0, a5, a0
-; RV32I-NEXT:    or a0, a0, a2
-; RV32I-NEXT:    or a1, a1, a6
-; RV32I-NEXT:    ret
-;
-; RV32B-LABEL: gorc5_i64:
-; RV32B:       # %bb.0:
-; RV32B-NEXT:    gorci a0, a0, 5
-; RV32B-NEXT:    gorci a1, a1, 5
-; RV32B-NEXT:    ret
-;
-; RV32ZBP-LABEL: gorc5_i64:
-; RV32ZBP:       # %bb.0:
-; RV32ZBP-NEXT:    gorci a0, a0, 5
-; RV32ZBP-NEXT:    gorci a1, a1, 5
-; RV32ZBP-NEXT:    ret
-  %and1 = shl i64 %a, 1
-  %shl1 = and i64 %and1, -6148914691236517206
-  %and1b = lshr i64 %a, 1
-  %shr1 = and i64 %and1b, 6148914691236517205
-  %or1 = or i64 %shr1, %a
-  %or1b = or i64 %or1, %shl1
-  %and2 = shl i64 %or1b, 4
-  %shl2 = and i64 %and2, -1085102592571150096
-  %and2b = lshr i64 %or1b, 4
-  %shr2 = and i64 %and2b, 1085102592571150095
-  %or2 = or i64 %shr2, %or1b
-  %or2b = or i64 %or2, %shl2
-  ret i64 %or2b
-}
-
-define i32 @gorc6_i32(i32 %a) nounwind {
-; RV32I-LABEL: gorc6_i32:
-; RV32I:       # %bb.0:
-; RV32I-NEXT:    slli a1, a0, 2
-; RV32I-NEXT:    lui a2, 838861
-; RV32I-NEXT:    addi a2, a2, -820
-; RV32I-NEXT:    and a1, a1, a2
-; RV32I-NEXT:    srli a2, a0, 2
-; RV32I-NEXT:    lui a3, 209715
-; RV32I-NEXT:    addi a3, a3, 819
-; RV32I-NEXT:    and a2, a2, a3
-; RV32I-NEXT:    or a0, a2, a0
-; RV32I-NEXT:    or a0, a0, a1
-; RV32I-NEXT:    slli a1, a0, 4
-; RV32I-NEXT:    lui a2, 986895
-; RV32I-NEXT:    addi a2, a2, 240
-; RV32I-NEXT:    and a1, a1, a2
-; RV32I-NEXT:    srli a2, a0, 4
-; RV32I-NEXT:    lui a3, 61681
-; RV32I-NEXT:    addi a3, a3, -241
-; RV32I-NEXT:    and a2, a2, a3
-; RV32I-NEXT:    or a0, a2, a0
-; RV32I-NEXT:    or a0, a0, a1
-; RV32I-NEXT:    ret
-;
-; RV32B-LABEL: gorc6_i32:
-; RV32B:       # %bb.0:
-; RV32B-NEXT:    orc2.b a0, a0
-; RV32B-NEXT:    ret
-;
-; RV32ZBP-LABEL: gorc6_i32:
-; RV32ZBP:       # %bb.0:
-; RV32ZBP-NEXT:    orc2.b a0, a0
-; RV32ZBP-NEXT:    ret
-  %and1 = shl i32 %a, 2
-  %shl1 = and i32 %and1, -858993460
-  %and1b = lshr i32 %a, 2
-  %shr1 = and i32 %and1b, 858993459
-  %or1 = or i32 %shr1, %a
-  %or1b = or i32 %or1, %shl1
-  %and2 = shl i32 %or1b, 4
-  %shl2 = and i32 %and2, -252645136
-  %and2b = lshr i32 %or1b, 4
-  %shr2 = and i32 %and2b, 252645135
-  %or2 = or i32 %shr2, %or1b
-  %or2b = or i32 %or2, %shl2
-  ret i32 %or2b
-}
-
-define i64 @gorc6_i64(i64 %a) nounwind {
-; RV32I-LABEL: gorc6_i64:
-; RV32I:       # %bb.0:
-; RV32I-NEXT:    slli a2, a1, 2
-; RV32I-NEXT:    slli a3, a0, 2
-; RV32I-NEXT:    lui a4, 838861
-; RV32I-NEXT:    addi a4, a4, -820
-; RV32I-NEXT:    and a6, a3, a4
-; RV32I-NEXT:    and a2, a2, a4
-; RV32I-NEXT:    srli a4, a0, 2
-; RV32I-NEXT:    srli a5, a1, 2
-; RV32I-NEXT:    lui a3, 209715
-; RV32I-NEXT:    addi a3, a3, 819
-; RV32I-NEXT:    and a5, a5, a3
-; RV32I-NEXT:    and a3, a4, a3
-; RV32I-NEXT:    or a0, a3, a0
-; RV32I-NEXT:    or a1, a5, a1
-; RV32I-NEXT:    or a1, a1, a2
-; RV32I-NEXT:    or a0, a0, a6
-; RV32I-NEXT:    slli a2, a0, 4
-; RV32I-NEXT:    slli a3, a1, 4
-; RV32I-NEXT:    lui a4, 986895
-; RV32I-NEXT:    addi a4, a4, 240
-; RV32I-NEXT:    and a6, a3, a4
-; RV32I-NEXT:    and a2, a2, a4
-; RV32I-NEXT:    srli a4, a1, 4
-; RV32I-NEXT:    srli a5, a0, 4
-; RV32I-NEXT:    lui a3, 61681
-; RV32I-NEXT:    addi a3, a3, -241
-; RV32I-NEXT:    and a5, a5, a3
-; RV32I-NEXT:    and a3, a4, a3
-; RV32I-NEXT:    or a1, a3, a1
-; RV32I-NEXT:    or a0, a5, a0
-; RV32I-NEXT:    or a0, a0, a2
-; RV32I-NEXT:    or a1, a1, a6
-; RV32I-NEXT:    ret
-;
-; RV32B-LABEL: gorc6_i64:
-; RV32B:       # %bb.0:
-; RV32B-NEXT:    orc2.b a0, a0
-; RV32B-NEXT:    orc2.b a1, a1
-; RV32B-NEXT:    ret
-;
-; RV32ZBP-LABEL: gorc6_i64:
-; RV32ZBP:       # %bb.0:
-; RV32ZBP-NEXT:    orc2.b a0, a0
-; RV32ZBP-NEXT:    orc2.b a1, a1
-; RV32ZBP-NEXT:    ret
-  %and1 = shl i64 %a, 2
-  %shl1 = and i64 %and1, -3689348814741910324
-  %and1b = lshr i64 %a, 2
-  %shr1 = and i64 %and1b, 3689348814741910323
-  %or1 = or i64 %shr1, %a
-  %or1b = or i64 %or1, %shl1
-  %and2 = shl i64 %or1b, 4
-  %shl2 = and i64 %and2, -1085102592571150096
-  %and2b = lshr i64 %or1b, 4
-  %shr2 = and i64 %and2b, 1085102592571150095
-  %or2 = or i64 %shr2, %or1b
-  %or2b = or i64 %or2, %shl2
-  ret i64 %or2b
-}
-
-define i32 @gorc7_i32(i32 %a) nounwind {
-; RV32I-LABEL: gorc7_i32:
-; RV32I:       # %bb.0:
-; RV32I-NEXT:    slli a1, a0, 1
-; RV32I-NEXT:    lui a2, 699051
-; RV32I-NEXT:    addi a2, a2, -1366
-; RV32I-NEXT:    and a1, a1, a2
-; RV32I-NEXT:    srli a2, a0, 1
-; RV32I-NEXT:    lui a3, 349525
-; RV32I-NEXT:    addi a3, a3, 1365
-; RV32I-NEXT:    and a2, a2, a3
-; RV32I-NEXT:    or a0, a2, a0
-; RV32I-NEXT:    or a0, a0, a1
-; RV32I-NEXT:    slli a1, a0, 2
-; RV32I-NEXT:    lui a2, 838861
-; RV32I-NEXT:    addi a2, a2, -820
-; RV32I-NEXT:    and a1, a1, a2
-; RV32I-NEXT:    srli a2, a0, 2
-; RV32I-NEXT:    lui a3, 209715
-; RV32I-NEXT:    addi a3, a3, 819
-; RV32I-NEXT:    and a2, a2, a3
-; RV32I-NEXT:    or a0, a2, a0
-; RV32I-NEXT:    or a0, a0, a1
-; RV32I-NEXT:    slli a1, a0, 4
-; RV32I-NEXT:    lui a2, 986895
-; RV32I-NEXT:    addi a2, a2, 240
-; RV32I-NEXT:    and a1, a1, a2
-; RV32I-NEXT:    srli a2, a0, 4
-; RV32I-NEXT:    lui a3, 61681
-; RV32I-NEXT:    addi a3, a3, -241
-; RV32I-NEXT:    and a2, a2, a3
-; RV32I-NEXT:    or a0, a2, a0
-; RV32I-NEXT:    or a0, a0, a1
-; RV32I-NEXT:    ret
-;
-; RV32B-LABEL: gorc7_i32:
-; RV32B:       # %bb.0:
-; RV32B-NEXT:    orc.b a0, a0
-; RV32B-NEXT:    ret
-;
-; RV32ZBP-LABEL: gorc7_i32:
-; RV32ZBP:       # %bb.0:
-; RV32ZBP-NEXT:    orc.b a0, a0
-; RV32ZBP-NEXT:    ret
-  %and1 = shl i32 %a, 1
-  %shl1 = and i32 %and1, -1431655766
-  %and1b = lshr i32 %a, 1
-  %shr1 = and i32 %and1b, 1431655765
-  %or1 = or i32 %shr1, %a
-  %or1b = or i32 %or1, %shl1
-  %and2 = shl i32 %or1b, 2
-  %shl2 = and i32 %and2, -858993460
-  %and2b = lshr i32 %or1b, 2
-  %shr2 = and i32 %and2b, 858993459
-  %or2 = or i32 %shr2, %or1b
-  %or2b = or i32 %or2, %shl2
-  %and3 = shl i32 %or2b, 4
-  %shl3 = and i32 %and3, -252645136
-  %and3b = lshr i32 %or2b, 4
-  %shr3 = and i32 %and3b, 252645135
-  %or3 = or i32 %shr3, %or2b
-  %or3b = or i32 %or3, %shl3
-  ret i32 %or3b
-}
-
-define i64 @gorc7_i64(i64 %a) nounwind {
-; RV32I-LABEL: gorc7_i64:
-; RV32I:       # %bb.0:
-; RV32I-NEXT:    slli a2, a0, 1
-; RV32I-NEXT:    slli a3, a1, 1
-; RV32I-NEXT:    lui a4, 699051
-; RV32I-NEXT:    addi a4, a4, -1366
-; RV32I-NEXT:    and a6, a3, a4
-; RV32I-NEXT:    and a2, a2, a4
-; RV32I-NEXT:    srli a4, a1, 1
-; RV32I-NEXT:    srli a5, a0, 1
-; RV32I-NEXT:    lui a3, 349525
-; RV32I-NEXT:    addi a3, a3, 1365
-; RV32I-NEXT:    and a5, a5, a3
-; RV32I-NEXT:    and a3, a4, a3
-; RV32I-NEXT:    or a1, a3, a1
-; RV32I-NEXT:    or a0, a5, a0
-; RV32I-NEXT:    or a0, a0, a2
-; RV32I-NEXT:    or a1, a1, a6
-; RV32I-NEXT:    slli a2, a1, 2
-; RV32I-NEXT:    slli a3, a0, 2
-; RV32I-NEXT:    lui a4, 838861
-; RV32I-NEXT:    addi a4, a4, -820
-; RV32I-NEXT:    and a6, a3, a4
-; RV32I-NEXT:    and a2, a2, a4
-; RV32I-NEXT:    srli a4, a0, 2
-; RV32I-NEXT:    srli a5, a1, 2
-; RV32I-NEXT:    lui a3, 209715
-; RV32I-NEXT:    addi a3, a3, 819
-; RV32I-NEXT:    and a5, a5, a3
-; RV32I-NEXT:    and a3, a4, a3
-; RV32I-NEXT:    or a0, a3, a0
-; RV32I-NEXT:    or a1, a5, a1
-; RV32I-NEXT:    or a1, a1, a2
-; RV32I-NEXT:    or a0, a0, a6
-; RV32I-NEXT:    slli a2, a0, 4
-; RV32I-NEXT:    slli a3, a1, 4
-; RV32I-NEXT:    lui a4, 986895
-; RV32I-NEXT:    addi a4, a4, 240
-; RV32I-NEXT:    and a6, a3, a4
-; RV32I-NEXT:    and a2, a2, a4
-; RV32I-NEXT:    srli a4, a1, 4
-; RV32I-NEXT:    srli a5, a0, 4
-; RV32I-NEXT:    lui a3, 61681
-; RV32I-NEXT:    addi a3, a3, -241
-; RV32I-NEXT:    and a5, a5, a3
-; RV32I-NEXT:    and a3, a4, a3
-; RV32I-NEXT:    or a1, a3, a1
-; RV32I-NEXT:    or a0, a5, a0
-; RV32I-NEXT:    or a0, a0, a2
-; RV32I-NEXT:    or a1, a1, a6
-; RV32I-NEXT:    ret
-;
-; RV32B-LABEL: gorc7_i64:
-; RV32B:       # %bb.0:
-; RV32B-NEXT:    orc.b a0, a0
-; RV32B-NEXT:    orc.b a1, a1
-; RV32B-NEXT:    ret
-;
-; RV32ZBP-LABEL: gorc7_i64:
-; RV32ZBP:       # %bb.0:
-; RV32ZBP-NEXT:    orc.b a0, a0
-; RV32ZBP-NEXT:    orc.b a1, a1
-; RV32ZBP-NEXT:    ret
-  %and1 = shl i64 %a, 1
-  %shl1 = and i64 %and1, -6148914691236517206
-  %and1b = lshr i64 %a, 1
-  %shr1 = and i64 %and1b, 6148914691236517205
-  %or1 = or i64 %shr1, %a
-  %or1b = or i64 %or1, %shl1
-  %and2 = shl i64 %or1b, 2
-  %shl2 = and i64 %and2, -3689348814741910324
-  %and2b = lshr i64 %or1b, 2
-  %shr2 = and i64 %and2b, 3689348814741910323
-  %or2 = or i64 %shr2, %or1b
-  %or2b = or i64 %or2, %shl2
-  %and3 = shl i64 %or2b, 4
-  %shl3 = and i64 %and3, -1085102592571150096
-  %and3b = lshr i64 %or2b, 4
-  %shr3 = and i64 %and3b, 1085102592571150095
-  %or3 = or i64 %shr3, %or2b
-  %or3b = or i64 %or3, %shl3
-  ret i64 %or3b
-}
-
-define i32 @gorc8_i32(i32 %a) nounwind {
-; RV32I-LABEL: gorc8_i32:
-; RV32I:       # %bb.0:
-; RV32I-NEXT:    slli a1, a0, 8
-; RV32I-NEXT:    lui a2, 1044496
-; RV32I-NEXT:    addi a2, a2, -256
-; RV32I-NEXT:    and a1, a1, a2
-; RV32I-NEXT:    srli a2, a0, 8
-; RV32I-NEXT:    lui a3, 4080
-; RV32I-NEXT:    addi a3, a3, 255
-; RV32I-NEXT:    and a2, a2, a3
-; RV32I-NEXT:    or a0, a2, a0
-; RV32I-NEXT:    or a0, a0, a1
-; RV32I-NEXT:    ret
-;
-; RV32B-LABEL: gorc8_i32:
-; RV32B:       # %bb.0:
-; RV32B-NEXT:    orc8.h a0, a0
-; RV32B-NEXT:    ret
-;
-; RV32ZBP-LABEL: gorc8_i32:
-; RV32ZBP:       # %bb.0:
-; RV32ZBP-NEXT:    orc8.h a0, a0
-; RV32ZBP-NEXT:    ret
-  %and = shl i32 %a, 8
-  %shl = and i32 %and, -16711936
-  %and1 = lshr i32 %a, 8
-  %shr = and i32 %and1, 16711935
-  %or = or i32 %shr, %a
-  %or2 = or i32 %or, %shl
-  ret i32 %or2
-}
-
-define i64 @gorc8_i64(i64 %a) nounwind {
-; RV32I-LABEL: gorc8_i64:
-; RV32I:       # %bb.0:
-; RV32I-NEXT:    slli a2, a0, 8
-; RV32I-NEXT:    slli a3, a1, 8
-; RV32I-NEXT:    lui a4, 1044496
-; RV32I-NEXT:    addi a4, a4, -256
-; RV32I-NEXT:    and a6, a3, a4
-; RV32I-NEXT:    and a2, a2, a4
-; RV32I-NEXT:    srli a4, a1, 8
-; RV32I-NEXT:    srli a5, a0, 8
-; RV32I-NEXT:    lui a3, 4080
-; RV32I-NEXT:    addi a3, a3, 255
-; RV32I-NEXT:    and a5, a5, a3
-; RV32I-NEXT:    and a3, a4, a3
-; RV32I-NEXT:    or a1, a3, a1
-; RV32I-NEXT:    or a0, a5, a0
-; RV32I-NEXT:    or a0, a0, a2
-; RV32I-NEXT:    or a1, a1, a6
-; RV32I-NEXT:    ret
-;
-; RV32B-LABEL: gorc8_i64:
-; RV32B:       # %bb.0:
-; RV32B-NEXT:    orc8.h a0, a0
-; RV32B-NEXT:    orc8.h a1, a1
-; RV32B-NEXT:    ret
-;
-; RV32ZBP-LABEL: gorc8_i64:
-; RV32ZBP:       # %bb.0:
-; RV32ZBP-NEXT:    orc8.h a0, a0
-; RV32ZBP-NEXT:    orc8.h a1, a1
-; RV32ZBP-NEXT:    ret
-  %and = shl i64 %a, 8
-  %shl = and i64 %and, -71777214294589696
-  %and1 = lshr i64 %a, 8
-  %shr = and i64 %and1, 71777214294589695
-  %or = or i64 %shr, %a
-  %or2 = or i64 %or, %shl
-  ret i64 %or2
-}
-
-define i32 @gorc16_i32(i32 %a) nounwind {
-; RV32I-LABEL: gorc16_i32:
-; RV32I:       # %bb.0:
-; RV32I-NEXT:    slli a1, a0, 16
-; RV32I-NEXT:    srli a2, a0, 16
-; RV32I-NEXT:    or a0, a2, a0
-; RV32I-NEXT:    or a0, a0, a1
-; RV32I-NEXT:    ret
-;
-; RV32B-LABEL: gorc16_i32:
-; RV32B:       # %bb.0:
-; RV32B-NEXT:    orc16 a0, a0
-; RV32B-NEXT:    ret
-;
-; RV32ZBP-LABEL: gorc16_i32:
-; RV32ZBP:       # %bb.0:
-; RV32ZBP-NEXT:    orc16 a0, a0
-; RV32ZBP-NEXT:    ret
-  %shl = shl i32 %a, 16
-  %shr = lshr i32 %a, 16
-  %or = or i32 %shr, %a
-  %or2 = or i32 %or, %shl
-  ret i32 %or2
-}
-
-define i32 @gorc16_rotl_i32(i32 %a) nounwind {
-; RV32I-LABEL: gorc16_rotl_i32:
-; RV32I:       # %bb.0:
-; RV32I-NEXT:    srli a1, a0, 16
-; RV32I-NEXT:    slli a2, a0, 16
-; RV32I-NEXT:    or a1, a2, a1
-; RV32I-NEXT:    or a0, a1, a0
-; RV32I-NEXT:    ret
-;
-; RV32B-LABEL: gorc16_rotl_i32:
-; RV32B:       # %bb.0:
-; RV32B-NEXT:    orc16 a0, a0
-; RV32B-NEXT:    ret
-;
-; RV32ZBP-LABEL: gorc16_rotl_i32:
-; RV32ZBP:       # %bb.0:
-; RV32ZBP-NEXT:    orc16 a0, a0
-; RV32ZBP-NEXT:    ret
-  %rot = tail call i32 @llvm.fshl.i32(i32 %a, i32 %a, i32 16)
-  %or = or i32 %rot, %a
-  ret i32 %or
-}
-
-define i32 @gorc16_rotr_i32(i32 %a) nounwind {
-; RV32I-LABEL: gorc16_rotr_i32:
-; RV32I:       # %bb.0:
-; RV32I-NEXT:    slli a1, a0, 16
-; RV32I-NEXT:    srli a2, a0, 16
-; RV32I-NEXT:    or a1, a2, a1
-; RV32I-NEXT:    or a0, a1, a0
-; RV32I-NEXT:    ret
-;
-; RV32B-LABEL: gorc16_rotr_i32:
-; RV32B:       # %bb.0:
-; RV32B-NEXT:    orc16 a0, a0
-; RV32B-NEXT:    ret
-;
-; RV32ZBP-LABEL: gorc16_rotr_i32:
-; RV32ZBP:       # %bb.0:
-; RV32ZBP-NEXT:    orc16 a0, a0
-; RV32ZBP-NEXT:    ret
-  %rot = tail call i32 @llvm.fshr.i32(i32 %a, i32 %a, i32 16)
-  %or = or i32 %rot, %a
-  ret i32 %or
-}
-
-define i64 @gorc16_i64(i64 %a) nounwind {
-; RV32I-LABEL: gorc16_i64:
-; RV32I:       # %bb.0:
-; RV32I-NEXT:    slli a2, a1, 16
-; RV32I-NEXT:    slli a3, a0, 16
-; RV32I-NEXT:    srli a4, a0, 16
-; RV32I-NEXT:    srli a5, a1, 16
-; RV32I-NEXT:    or a1, a5, a1
-; RV32I-NEXT:    or a0, a4, a0
-; RV32I-NEXT:    or a0, a0, a3
-; RV32I-NEXT:    or a1, a1, a2
-; RV32I-NEXT:    ret
-;
-; RV32B-LABEL: gorc16_i64:
-; RV32B:       # %bb.0:
-; RV32B-NEXT:    orc16 a0, a0
-; RV32B-NEXT:    orc16 a1, a1
-; RV32B-NEXT:    ret
-;
-; RV32ZBP-LABEL: gorc16_i64:
-; RV32ZBP:       # %bb.0:
-; RV32ZBP-NEXT:    orc16 a0, a0
-; RV32ZBP-NEXT:    orc16 a1, a1
-; RV32ZBP-NEXT:    ret
-  %and = shl i64 %a, 16
-  %shl = and i64 %and, -281470681808896
-  %and1 = lshr i64 %a, 16
-  %shr = and i64 %and1, 281470681808895
-  %or = or i64 %shr, %a
-  %or2 = or i64 %or, %shl
-  ret i64 %or2
-}
-
-; gorc2, gorc2 -> gorc2
-define i32 @gorc2b_i32(i32 %a) nounwind {
-; RV32I-LABEL: gorc2b_i32:
-; RV32I:       # %bb.0:
-; RV32I-NEXT:    slli a1, a0, 2
-; RV32I-NEXT:    lui a2, 838861
-; RV32I-NEXT:    addi a2, a2, -820
-; RV32I-NEXT:    and a1, a1, a2
-; RV32I-NEXT:    srli a3, a0, 2
-; RV32I-NEXT:    lui a4, 209715
-; RV32I-NEXT:    addi a4, a4, 819
-; RV32I-NEXT:    and a3, a3, a4
-; RV32I-NEXT:    or a0, a3, a0
-; RV32I-NEXT:    or a0, a0, a1
-; RV32I-NEXT:    slli a1, a0, 2
-; RV32I-NEXT:    and a1, a1, a2
-; RV32I-NEXT:    srli a2, a0, 2
-; RV32I-NEXT:    and a2, a2, a4
-; RV32I-NEXT:    or a0, a2, a0
-; RV32I-NEXT:    or a0, a0, a1
-; RV32I-NEXT:    ret
-;
-; RV32B-LABEL: gorc2b_i32:
-; RV32B:       # %bb.0:
-; RV32B-NEXT:    orc2.n a0, a0
-; RV32B-NEXT:    ret
-;
-; RV32ZBP-LABEL: gorc2b_i32:
-; RV32ZBP:       # %bb.0:
-; RV32ZBP-NEXT:    orc2.n a0, a0
-; RV32ZBP-NEXT:    ret
-  %and1 = shl i32 %a, 2
-  %shl1 = and i32 %and1, -858993460
-  %and1b = lshr i32 %a, 2
-  %shr1 = and i32 %and1b, 858993459
-  %or1 = or i32 %shr1, %a
-  %or1b = or i32 %or1, %shl1
-  %and2 = shl i32 %or1b, 2
-  %shl2 = and i32 %and2, -858993460
-  %and2b = lshr i32 %or1b, 2
-  %shr2 = and i32 %and2b, 858993459
-  %or2 = or i32 %shr2, %or1b
-  %or2b = or i32 %or2, %shl2
-  ret i32 %or2b
-}
-
-; gorc2, gorc2 -> gorc2
-define i64 @gorc2b_i64(i64 %a) nounwind {
-; RV32I-LABEL: gorc2b_i64:
-; RV32I:       # %bb.0:
-; RV32I-NEXT:    slli a2, a1, 2
-; RV32I-NEXT:    slli a3, a0, 2
-; RV32I-NEXT:    lui a4, 838861
-; RV32I-NEXT:    addi a4, a4, -820
-; RV32I-NEXT:    and a6, a3, a4
-; RV32I-NEXT:    and a7, a2, a4
-; RV32I-NEXT:    srli a5, a0, 2
-; RV32I-NEXT:    srli a3, a1, 2
-; RV32I-NEXT:    lui a2, 209715
-; RV32I-NEXT:    addi a2, a2, 819
-; RV32I-NEXT:    and a3, a3, a2
-; RV32I-NEXT:    and a5, a5, a2
-; RV32I-NEXT:    or a0, a5, a0
-; RV32I-NEXT:    or a1, a3, a1
-; RV32I-NEXT:    or a1, a1, a7
-; RV32I-NEXT:    or a0, a0, a6
-; RV32I-NEXT:    slli a3, a0, 2
-; RV32I-NEXT:    slli a5, a1, 2
-; RV32I-NEXT:    and a6, a5, a4
-; RV32I-NEXT:    and a3, a3, a4
-; RV32I-NEXT:    srli a4, a1, 2
-; RV32I-NEXT:    srli a5, a0, 2
-; RV32I-NEXT:    and a5, a5, a2
-; RV32I-NEXT:    and a2, a4, a2
-; RV32I-NEXT:    or a1, a2, a1
-; RV32I-NEXT:    or a0, a5, a0
-; RV32I-NEXT:    or a0, a0, a3
-; RV32I-NEXT:    or a1, a1, a6
-; RV32I-NEXT:    ret
-;
-; RV32B-LABEL: gorc2b_i64:
-; RV32B:       # %bb.0:
-; RV32B-NEXT:    orc2.n a0, a0
-; RV32B-NEXT:    orc2.n a1, a1
-; RV32B-NEXT:    ret
-;
-; RV32ZBP-LABEL: gorc2b_i64:
-; RV32ZBP:       # %bb.0:
-; RV32ZBP-NEXT:    orc2.n a0, a0
-; RV32ZBP-NEXT:    orc2.n a1, a1
-; RV32ZBP-NEXT:    ret
-  %and1 = shl i64 %a, 2
-  %shl1 = and i64 %and1, -3689348814741910324
-  %and1b = lshr i64 %a, 2
-  %shr1 = and i64 %and1b, 3689348814741910323
-  %or1 = or i64 %shr1, %a
-  %or1b = or i64 %or1, %shl1
-  %and2 = shl i64 %or1b, 2
-  %shl2 = and i64 %and2, -3689348814741910324
-  %and2b = lshr i64 %or1b, 2
-  %shr2 = and i64 %and2b, 3689348814741910323
-  %or2 = or i64 %shr2, %or1b
-  %or2b = or i64 %or2, %shl2
-  ret i64 %or2b
-}
-
-; gorc1, gorc2, gorc1 -> gorc2
-define i32 @gorc3b_i32(i32 %a) nounwind {
-; RV32I-LABEL: gorc3b_i32:
-; RV32I:       # %bb.0:
-; RV32I-NEXT:    slli a1, a0, 1
-; RV32I-NEXT:    lui a2, 699051
-; RV32I-NEXT:    addi a2, a2, -1366
-; RV32I-NEXT:    and a1, a1, a2
-; RV32I-NEXT:    srli a3, a0, 1
-; RV32I-NEXT:    lui a4, 349525
-; RV32I-NEXT:    addi a4, a4, 1365
-; RV32I-NEXT:    and a3, a3, a4
-; RV32I-NEXT:    or a0, a3, a0
-; RV32I-NEXT:    or a0, a0, a1
-; RV32I-NEXT:    slli a1, a0, 2
-; RV32I-NEXT:    lui a3, 838861
-; RV32I-NEXT:    addi a3, a3, -820
-; RV32I-NEXT:    and a1, a1, a3
-; RV32I-NEXT:    srli a3, a0, 2
-; RV32I-NEXT:    lui a5, 209715
-; RV32I-NEXT:    addi a5, a5, 819
-; RV32I-NEXT:    and a3, a3, a5
-; RV32I-NEXT:    or a0, a3, a0
-; RV32I-NEXT:    or a0, a0, a1
-; RV32I-NEXT:    slli a1, a0, 1
-; RV32I-NEXT:    and a1, a1, a2
-; RV32I-NEXT:    srli a2, a0, 1
-; RV32I-NEXT:    and a2, a2, a4
-; RV32I-NEXT:    or a0, a2, a0
-; RV32I-NEXT:    or a0, a0, a1
-; RV32I-NEXT:    ret
-;
-; RV32B-LABEL: gorc3b_i32:
-; RV32B:       # %bb.0:
-; RV32B-NEXT:    orc.n a0, a0
-; RV32B-NEXT:    ret
-;
-; RV32ZBP-LABEL: gorc3b_i32:
-; RV32ZBP:       # %bb.0:
-; RV32ZBP-NEXT:    orc.n a0, a0
-; RV32ZBP-NEXT:    ret
-  %and1 = shl i32 %a, 1
-  %shl1 = and i32 %and1, -1431655766
-  %and1b = lshr i32 %a, 1
-  %shr1 = and i32 %and1b, 1431655765
-  %or1 = or i32 %shr1, %a
-  %or1b = or i32 %or1, %shl1
-  %and2 = shl i32 %or1b, 2
-  %shl2 = and i32 %and2, -858993460
-  %and2b = lshr i32 %or1b, 2
-  %shr2 = and i32 %and2b, 858993459
-  %or2 = or i32 %shr2, %or1b
-  %or2b = or i32 %or2, %shl2
-  %and3 = shl i32 %or2b, 1
-  %shl3 = and i32 %and3, -1431655766
-  %and3b = lshr i32 %or2b, 1
-  %shr3 = and i32 %and3b, 1431655765
-  %or3 = or i32 %shr3, %or2b
-  %or3b = or i32 %or3, %shl3
-  ret i32 %or3b
-}
-
-; gorc1, gorc2, gorc1 -> gorc2
-define i64 @gorc3b_i64(i64 %a) nounwind {
-; RV32I-LABEL: gorc3b_i64:
-; RV32I:       # %bb.0:
-; RV32I-NEXT:    slli a2, a0, 1
-; RV32I-NEXT:    slli a3, a1, 1
-; RV32I-NEXT:    lui a4, 699051
-; RV32I-NEXT:    addi a6, a4, -1366
-; RV32I-NEXT:    and a7, a3, a6
-; RV32I-NEXT:    and a2, a2, a6
-; RV32I-NEXT:    srli a5, a1, 1
-; RV32I-NEXT:    srli a4, a0, 1
-; RV32I-NEXT:    lui a3, 349525
-; RV32I-NEXT:    addi t0, a3, 1365
-; RV32I-NEXT:    and a4, a4, t0
-; RV32I-NEXT:    and a5, a5, t0
-; RV32I-NEXT:    or a1, a5, a1
-; RV32I-NEXT:    or a0, a4, a0
-; RV32I-NEXT:    or a0, a0, a2
-; RV32I-NEXT:    or a1, a1, a7
-; RV32I-NEXT:    slli a2, a1, 2
-; RV32I-NEXT:    slli a4, a0, 2
-; RV32I-NEXT:    lui a5, 838861
-; RV32I-NEXT:    addi a5, a5, -820
-; RV32I-NEXT:    and a7, a4, a5
-; RV32I-NEXT:    and a2, a2, a5
-; RV32I-NEXT:    srli a5, a0, 2
-; RV32I-NEXT:    srli a3, a1, 2
-; RV32I-NEXT:    lui a4, 209715
-; RV32I-NEXT:    addi a4, a4, 819
-; RV32I-NEXT:    and a3, a3, a4
-; RV32I-NEXT:    and a4, a5, a4
-; RV32I-NEXT:    or a0, a4, a0
-; RV32I-NEXT:    or a1, a3, a1
-; RV32I-NEXT:    or a1, a1, a2
-; RV32I-NEXT:    or a0, a0, a7
-; RV32I-NEXT:    slli a2, a0, 1
-; RV32I-NEXT:    slli a3, a1, 1
-; RV32I-NEXT:    and a3, a3, a6
-; RV32I-NEXT:    and a2, a2, a6
-; RV32I-NEXT:    srli a4, a1, 1
-; RV32I-NEXT:    srli a5, a0, 1
-; RV32I-NEXT:    and a5, a5, t0
-; RV32I-NEXT:    and a4, a4, t0
-; RV32I-NEXT:    or a1, a4, a1
-; RV32I-NEXT:    or a0, a5, a0
-; RV32I-NEXT:    or a0, a0, a2
-; RV32I-NEXT:    or a1, a1, a3
-; RV32I-NEXT:    ret
-;
-; RV32B-LABEL: gorc3b_i64:
-; RV32B:       # %bb.0:
-; RV32B-NEXT:    orc.n a0, a0
-; RV32B-NEXT:    orc.n a1, a1
-; RV32B-NEXT:    ret
-;
-; RV32ZBP-LABEL: gorc3b_i64:
-; RV32ZBP:       # %bb.0:
-; RV32ZBP-NEXT:    orc.n a0, a0
-; RV32ZBP-NEXT:    orc.n a1, a1
-; RV32ZBP-NEXT:    ret
-  %and1 = shl i64 %a, 1
-  %shl1 = and i64 %and1, -6148914691236517206
-  %and1b = lshr i64 %a, 1
-  %shr1 = and i64 %and1b, 6148914691236517205
-  %or1 = or i64 %shr1, %a
-  %or1b = or i64 %or1, %shl1
-  %and2 = shl i64 %or1b, 2
-  %shl2 = and i64 %and2, -3689348814741910324
-  %and2b = lshr i64 %or1b, 2
-  %shr2 = and i64 %and2b, 3689348814741910323
-  %or2 = or i64 %shr2, %or1b
-  %or2b = or i64 %or2, %shl2
-  %and3 = shl i64 %or2b, 1
-  %shl3 = and i64 %and3, -6148914691236517206
-  %and3b = lshr i64 %or2b, 1
-  %shr3 = and i64 %and3b, 6148914691236517205
-  %or3 = or i64 %shr3, %or2b
-  %or3b = or i64 %or3, %shl3
-  ret i64 %or3b
-}
-
-define i32 @grev1_i32(i32 %a) nounwind {
-; RV32I-LABEL: grev1_i32:
-; RV32I:       # %bb.0:
-; RV32I-NEXT:    slli a1, a0, 1
-; RV32I-NEXT:    lui a2, 699051
-; RV32I-NEXT:    addi a2, a2, -1366
-; RV32I-NEXT:    and a1, a1, a2
-; RV32I-NEXT:    srli a0, a0, 1
-; RV32I-NEXT:    lui a2, 349525
-; RV32I-NEXT:    addi a2, a2, 1365
-; RV32I-NEXT:    and a0, a0, a2
-; RV32I-NEXT:    or a0, a1, a0
-; RV32I-NEXT:    ret
-;
-; RV32B-LABEL: grev1_i32:
-; RV32B:       # %bb.0:
-; RV32B-NEXT:    rev.p a0, a0
-; RV32B-NEXT:    ret
-;
-; RV32ZBP-LABEL: grev1_i32:
-; RV32ZBP:       # %bb.0:
-; RV32ZBP-NEXT:    rev.p a0, a0
-; RV32ZBP-NEXT:    ret
-  %and = shl i32 %a, 1
-  %shl = and i32 %and, -1431655766
-  %and1 = lshr i32 %a, 1
-  %shr = and i32 %and1, 1431655765
-  %or = or i32 %shl, %shr
-  ret i32 %or
-}
-
-define i64 @grev1_i64(i64 %a) nounwind {
-; RV32I-LABEL: grev1_i64:
-; RV32I:       # %bb.0:
-; RV32I-NEXT:    slli a2, a0, 1
-; RV32I-NEXT:    slli a3, a1, 1
-; RV32I-NEXT:    lui a4, 699051
-; RV32I-NEXT:    addi a4, a4, -1366
-; RV32I-NEXT:    and a3, a3, a4
-; RV32I-NEXT:    and a2, a2, a4
-; RV32I-NEXT:    srli a0, a0, 1
-; RV32I-NEXT:    srli a1, a1, 1
-; RV32I-NEXT:    lui a4, 349525
-; RV32I-NEXT:    addi a4, a4, 1365
-; RV32I-NEXT:    and a1, a1, a4
-; RV32I-NEXT:    and a0, a0, a4
-; RV32I-NEXT:    or a0, a2, a0
-; RV32I-NEXT:    or a1, a3, a1
-; RV32I-NEXT:    ret
-;
-; RV32B-LABEL: grev1_i64:
-; RV32B:       # %bb.0:
-; RV32B-NEXT:    rev.p a0, a0
-; RV32B-NEXT:    rev.p a1, a1
-; RV32B-NEXT:    ret
-;
-; RV32ZBP-LABEL: grev1_i64:
-; RV32ZBP:       # %bb.0:
-; RV32ZBP-NEXT:    rev.p a0, a0
-; RV32ZBP-NEXT:    rev.p a1, a1
-; RV32ZBP-NEXT:    ret
-  %and = shl i64 %a, 1
-  %shl = and i64 %and, -6148914691236517206
-  %and1 = lshr i64 %a, 1
-  %shr = and i64 %and1, 6148914691236517205
-  %or = or i64 %shl, %shr
-  ret i64 %or
-}
-
-define i32 @grev2_i32(i32 %a) nounwind {
-; RV32I-LABEL: grev2_i32:
-; RV32I:       # %bb.0:
-; RV32I-NEXT:    slli a1, a0, 2
-; RV32I-NEXT:    lui a2, 838861
-; RV32I-NEXT:    addi a2, a2, -820
-; RV32I-NEXT:    and a1, a1, a2
-; RV32I-NEXT:    srli a0, a0, 2
-; RV32I-NEXT:    lui a2, 209715
-; RV32I-NEXT:    addi a2, a2, 819
-; RV32I-NEXT:    and a0, a0, a2
-; RV32I-NEXT:    or a0, a1, a0
-; RV32I-NEXT:    ret
-;
-; RV32B-LABEL: grev2_i32:
-; RV32B:       # %bb.0:
-; RV32B-NEXT:    rev2.n a0, a0
-; RV32B-NEXT:    ret
-;
-; RV32ZBP-LABEL: grev2_i32:
-; RV32ZBP:       # %bb.0:
-; RV32ZBP-NEXT:    rev2.n a0, a0
-; RV32ZBP-NEXT:    ret
-  %and = shl i32 %a, 2
-  %shl = and i32 %and, -858993460
-  %and1 = lshr i32 %a, 2
-  %shr = and i32 %and1, 858993459
-  %or = or i32 %shl, %shr
-  ret i32 %or
-}
-
-define i64 @grev2_i64(i64 %a) nounwind {
-; RV32I-LABEL: grev2_i64:
-; RV32I:       # %bb.0:
-; RV32I-NEXT:    slli a2, a0, 2
-; RV32I-NEXT:    slli a3, a1, 2
-; RV32I-NEXT:    lui a4, 838861
-; RV32I-NEXT:    addi a4, a4, -820
-; RV32I-NEXT:    and a3, a3, a4
-; RV32I-NEXT:    and a2, a2, a4
-; RV32I-NEXT:    srli a0, a0, 2
-; RV32I-NEXT:    srli a1, a1, 2
-; RV32I-NEXT:    lui a4, 209715
-; RV32I-NEXT:    addi a4, a4, 819
-; RV32I-NEXT:    and a1, a1, a4
-; RV32I-NEXT:    and a0, a0, a4
-; RV32I-NEXT:    or a0, a2, a0
-; RV32I-NEXT:    or a1, a3, a1
-; RV32I-NEXT:    ret
-;
-; RV32B-LABEL: grev2_i64:
-; RV32B:       # %bb.0:
-; RV32B-NEXT:    rev2.n a0, a0
-; RV32B-NEXT:    rev2.n a1, a1
-; RV32B-NEXT:    ret
-;
-; RV32ZBP-LABEL: grev2_i64:
-; RV32ZBP:       # %bb.0:
-; RV32ZBP-NEXT:    rev2.n a0, a0
-; RV32ZBP-NEXT:    rev2.n a1, a1
-; RV32ZBP-NEXT:    ret
-  %and = shl i64 %a, 2
-  %shl = and i64 %and, -3689348814741910324
-  %and1 = lshr i64 %a, 2
-  %shr = and i64 %and1, 3689348814741910323
-  %or = or i64 %shl, %shr
-  ret i64 %or
-}
-
-define i32 @grev3_i32(i32 %a) nounwind {
-; RV32I-LABEL: grev3_i32:
-; RV32I:       # %bb.0:
-; RV32I-NEXT:    slli a1, a0, 1
-; RV32I-NEXT:    lui a2, 699051
-; RV32I-NEXT:    addi a2, a2, -1366
-; RV32I-NEXT:    and a1, a1, a2
-; RV32I-NEXT:    srli a0, a0, 1
-; RV32I-NEXT:    lui a2, 349525
-; RV32I-NEXT:    addi a2, a2, 1365
-; RV32I-NEXT:    and a0, a0, a2
-; RV32I-NEXT:    or a0, a1, a0
-; RV32I-NEXT:    slli a1, a0, 2
-; RV32I-NEXT:    lui a2, 838861
-; RV32I-NEXT:    addi a2, a2, -820
-; RV32I-NEXT:    and a1, a1, a2
-; RV32I-NEXT:    srli a0, a0, 2
-; RV32I-NEXT:    lui a2, 209715
-; RV32I-NEXT:    addi a2, a2, 819
-; RV32I-NEXT:    and a0, a0, a2
-; RV32I-NEXT:    or a0, a1, a0
-; RV32I-NEXT:    ret
-;
-; RV32B-LABEL: grev3_i32:
-; RV32B:       # %bb.0:
-; RV32B-NEXT:    rev.n a0, a0
-; RV32B-NEXT:    ret
-;
-; RV32ZBP-LABEL: grev3_i32:
-; RV32ZBP:       # %bb.0:
-; RV32ZBP-NEXT:    rev.n a0, a0
-; RV32ZBP-NEXT:    ret
-  %and1 = shl i32 %a, 1
-  %shl1 = and i32 %and1, -1431655766
-  %and1b = lshr i32 %a, 1
-  %shr1 = and i32 %and1b, 1431655765
-  %or1 = or i32 %shl1, %shr1
-  %and2 = shl i32 %or1, 2
-  %shl2 = and i32 %and2, -858993460
-  %and2b = lshr i32 %or1, 2
-  %shr2 = and i32 %and2b, 858993459
-  %or2 = or i32 %shl2, %shr2
-  ret i32 %or2
-}
-
-define i64 @grev3_i64(i64 %a) nounwind {
-; RV32I-LABEL: grev3_i64:
-; RV32I:       # %bb.0:
-; RV32I-NEXT:    slli a2, a1, 1
-; RV32I-NEXT:    slli a3, a0, 1
-; RV32I-NEXT:    lui a4, 699051
-; RV32I-NEXT:    addi a4, a4, -1366
-; RV32I-NEXT:    and a3, a3, a4
-; RV32I-NEXT:    and a2, a2, a4
-; RV32I-NEXT:    srli a1, a1, 1
-; RV32I-NEXT:    srli a0, a0, 1
-; RV32I-NEXT:    lui a4, 349525
-; RV32I-NEXT:    addi a4, a4, 1365
-; RV32I-NEXT:    and a0, a0, a4
-; RV32I-NEXT:    and a1, a1, a4
-; RV32I-NEXT:    or a1, a2, a1
-; RV32I-NEXT:    or a0, a3, a0
-; RV32I-NEXT:    slli a2, a0, 2
-; RV32I-NEXT:    slli a3, a1, 2
-; RV32I-NEXT:    lui a4, 838861
-; RV32I-NEXT:    addi a4, a4, -820
-; RV32I-NEXT:    and a3, a3, a4
-; RV32I-NEXT:    and a2, a2, a4
-; RV32I-NEXT:    srli a0, a0, 2
-; RV32I-NEXT:    srli a1, a1, 2
-; RV32I-NEXT:    lui a4, 209715
-; RV32I-NEXT:    addi a4, a4, 819
-; RV32I-NEXT:    and a1, a1, a4
-; RV32I-NEXT:    and a0, a0, a4
-; RV32I-NEXT:    or a0, a2, a0
-; RV32I-NEXT:    or a1, a3, a1
-; RV32I-NEXT:    ret
-;
-; RV32B-LABEL: grev3_i64:
-; RV32B:       # %bb.0:
-; RV32B-NEXT:    rev.n a0, a0
-; RV32B-NEXT:    rev.n a1, a1
-; RV32B-NEXT:    ret
-;
-; RV32ZBP-LABEL: grev3_i64:
-; RV32ZBP:       # %bb.0:
-; RV32ZBP-NEXT:    rev.n a0, a0
-; RV32ZBP-NEXT:    rev.n a1, a1
-; RV32ZBP-NEXT:    ret
-  %and1 = shl i64 %a, 1
-  %shl1 = and i64 %and1, -6148914691236517206
-  %and1b = lshr i64 %a, 1
-  %shr1 = and i64 %and1b, 6148914691236517205
-  %or1 = or i64 %shl1, %shr1
-  %and2 = shl i64 %or1, 2
-  %shl2 = and i64 %and2, -3689348814741910324
-  %and2b = lshr i64 %or1, 2
-  %shr2 = and i64 %and2b, 3689348814741910323
-  %or2 = or i64 %shl2, %shr2
-  ret i64 %or2
-}
-
-define i32 @grev4_i32(i32 %a) nounwind {
-; RV32I-LABEL: grev4_i32:
-; RV32I:       # %bb.0:
-; RV32I-NEXT:    slli a1, a0, 4
-; RV32I-NEXT:    lui a2, 986895
-; RV32I-NEXT:    addi a2, a2, 240
-; RV32I-NEXT:    and a1, a1, a2
-; RV32I-NEXT:    srli a0, a0, 4
-; RV32I-NEXT:    lui a2, 61681
-; RV32I-NEXT:    addi a2, a2, -241
-; RV32I-NEXT:    and a0, a0, a2
-; RV32I-NEXT:    or a0, a1, a0
-; RV32I-NEXT:    ret
-;
-; RV32B-LABEL: grev4_i32:
-; RV32B:       # %bb.0:
-; RV32B-NEXT:    rev4.b a0, a0
-; RV32B-NEXT:    ret
-;
-; RV32ZBP-LABEL: grev4_i32:
-; RV32ZBP:       # %bb.0:
-; RV32ZBP-NEXT:    rev4.b a0, a0
-; RV32ZBP-NEXT:    ret
-  %and = shl i32 %a, 4
-  %shl = and i32 %and, -252645136
-  %and1 = lshr i32 %a, 4
-  %shr = and i32 %and1, 252645135
-  %or = or i32 %shl, %shr
-  ret i32 %or
-}
-
-define i64 @grev4_i64(i64 %a) nounwind {
-; RV32I-LABEL: grev4_i64:
-; RV32I:       # %bb.0:
-; RV32I-NEXT:    slli a2, a0, 4
-; RV32I-NEXT:    slli a3, a1, 4
-; RV32I-NEXT:    lui a4, 986895
-; RV32I-NEXT:    addi a4, a4, 240
-; RV32I-NEXT:    and a3, a3, a4
-; RV32I-NEXT:    and a2, a2, a4
-; RV32I-NEXT:    srli a0, a0, 4
-; RV32I-NEXT:    srli a1, a1, 4
-; RV32I-NEXT:    lui a4, 61681
-; RV32I-NEXT:    addi a4, a4, -241
-; RV32I-NEXT:    and a1, a1, a4
-; RV32I-NEXT:    and a0, a0, a4
-; RV32I-NEXT:    or a0, a2, a0
-; RV32I-NEXT:    or a1, a3, a1
-; RV32I-NEXT:    ret
-;
-; RV32B-LABEL: grev4_i64:
-; RV32B:       # %bb.0:
-; RV32B-NEXT:    rev4.b a0, a0
-; RV32B-NEXT:    rev4.b a1, a1
-; RV32B-NEXT:    ret
-;
-; RV32ZBP-LABEL: grev4_i64:
-; RV32ZBP:       # %bb.0:
-; RV32ZBP-NEXT:    rev4.b a0, a0
-; RV32ZBP-NEXT:    rev4.b a1, a1
-; RV32ZBP-NEXT:    ret
-  %and = shl i64 %a, 4
-  %shl = and i64 %and, -1085102592571150096
-  %and1 = lshr i64 %a, 4
-  %shr = and i64 %and1, 1085102592571150095
-  %or = or i64 %shl, %shr
-  ret i64 %or
-}
-
-define i32 @grev5_i32(i32 %a) nounwind {
-; RV32I-LABEL: grev5_i32:
-; RV32I:       # %bb.0:
-; RV32I-NEXT:    slli a1, a0, 1
-; RV32I-NEXT:    lui a2, 699051
-; RV32I-NEXT:    addi a2, a2, -1366
-; RV32I-NEXT:    and a1, a1, a2
-; RV32I-NEXT:    srli a0, a0, 1
-; RV32I-NEXT:    lui a2, 349525
-; RV32I-NEXT:    addi a2, a2, 1365
-; RV32I-NEXT:    and a0, a0, a2
-; RV32I-NEXT:    or a0, a1, a0
-; RV32I-NEXT:    slli a1, a0, 4
-; RV32I-NEXT:    lui a2, 986895
-; RV32I-NEXT:    addi a2, a2, 240
-; RV32I-NEXT:    and a1, a1, a2
-; RV32I-NEXT:    srli a0, a0, 4
-; RV32I-NEXT:    lui a2, 61681
-; RV32I-NEXT:    addi a2, a2, -241
-; RV32I-NEXT:    and a0, a0, a2
-; RV32I-NEXT:    or a0, a1, a0
-; RV32I-NEXT:    ret
-;
-; RV32B-LABEL: grev5_i32:
-; RV32B:       # %bb.0:
-; RV32B-NEXT:    grevi a0, a0, 5
-; RV32B-NEXT:    ret
-;
-; RV32ZBP-LABEL: grev5_i32:
-; RV32ZBP:       # %bb.0:
-; RV32ZBP-NEXT:    grevi a0, a0, 5
-; RV32ZBP-NEXT:    ret
-  %and1 = shl i32 %a, 1
-  %shl1 = and i32 %and1, -1431655766
-  %and1b = lshr i32 %a, 1
-  %shr1 = and i32 %and1b, 1431655765
-  %or1 = or i32 %shl1, %shr1
-  %and2 = shl i32 %or1, 4
-  %shl2 = and i32 %and2, -252645136
-  %and2b = lshr i32 %or1, 4
-  %shr2 = and i32 %and2b, 252645135
-  %or2 = or i32 %shl2, %shr2
-  ret i32 %or2
-}
-
-define i64 @grev5_i64(i64 %a) nounwind {
-; RV32I-LABEL: grev5_i64:
-; RV32I:       # %bb.0:
-; RV32I-NEXT:    slli a2, a1, 1
-; RV32I-NEXT:    slli a3, a0, 1
-; RV32I-NEXT:    lui a4, 699051
-; RV32I-NEXT:    addi a4, a4, -1366
-; RV32I-NEXT:    and a3, a3, a4
-; RV32I-NEXT:    and a2, a2, a4
-; RV32I-NEXT:    srli a1, a1, 1
-; RV32I-NEXT:    srli a0, a0, 1
-; RV32I-NEXT:    lui a4, 349525
-; RV32I-NEXT:    addi a4, a4, 1365
-; RV32I-NEXT:    and a0, a0, a4
-; RV32I-NEXT:    and a1, a1, a4
-; RV32I-NEXT:    or a1, a2, a1
-; RV32I-NEXT:    or a0, a3, a0
-; RV32I-NEXT:    slli a2, a0, 4
-; RV32I-NEXT:    slli a3, a1, 4
-; RV32I-NEXT:    lui a4, 986895
-; RV32I-NEXT:    addi a4, a4, 240
-; RV32I-NEXT:    and a3, a3, a4
-; RV32I-NEXT:    and a2, a2, a4
-; RV32I-NEXT:    srli a0, a0, 4
-; RV32I-NEXT:    srli a1, a1, 4
-; RV32I-NEXT:    lui a4, 61681
-; RV32I-NEXT:    addi a4, a4, -241
-; RV32I-NEXT:    and a1, a1, a4
-; RV32I-NEXT:    and a0, a0, a4
-; RV32I-NEXT:    or a0, a2, a0
-; RV32I-NEXT:    or a1, a3, a1
-; RV32I-NEXT:    ret
-;
-; RV32B-LABEL: grev5_i64:
-; RV32B:       # %bb.0:
-; RV32B-NEXT:    grevi a0, a0, 5
-; RV32B-NEXT:    grevi a1, a1, 5
-; RV32B-NEXT:    ret
-;
-; RV32ZBP-LABEL: grev5_i64:
-; RV32ZBP:       # %bb.0:
-; RV32ZBP-NEXT:    grevi a0, a0, 5
-; RV32ZBP-NEXT:    grevi a1, a1, 5
-; RV32ZBP-NEXT:    ret
-  %and1 = shl i64 %a, 1
-  %shl1 = and i64 %and1, -6148914691236517206
-  %and1b = lshr i64 %a, 1
-  %shr1 = and i64 %and1b, 6148914691236517205
-  %or1 = or i64 %shl1, %shr1
-
-  %and2 = shl i64 %or1, 4
-  %shl2 = and i64 %and2, -1085102592571150096
-  %and2b = lshr i64 %or1, 4
-  %shr2 = and i64 %and2b, 1085102592571150095
-  %or2 = or i64 %shl2, %shr2
-  ret i64 %or2
-}
-
-define i32 @grev6_i32(i32 %a) nounwind {
-; RV32I-LABEL: grev6_i32:
-; RV32I:       # %bb.0:
-; RV32I-NEXT:    slli a1, a0, 2
-; RV32I-NEXT:    lui a2, 838861
-; RV32I-NEXT:    addi a2, a2, -820
-; RV32I-NEXT:    and a1, a1, a2
-; RV32I-NEXT:    srli a0, a0, 2
-; RV32I-NEXT:    lui a2, 209715
-; RV32I-NEXT:    addi a2, a2, 819
-; RV32I-NEXT:    and a0, a0, a2
-; RV32I-NEXT:    or a0, a1, a0
-; RV32I-NEXT:    slli a1, a0, 4
-; RV32I-NEXT:    lui a2, 986895
-; RV32I-NEXT:    addi a2, a2, 240
-; RV32I-NEXT:    and a1, a1, a2
-; RV32I-NEXT:    srli a0, a0, 4
-; RV32I-NEXT:    lui a2, 61681
-; RV32I-NEXT:    addi a2, a2, -241
-; RV32I-NEXT:    and a0, a0, a2
-; RV32I-NEXT:    or a0, a1, a0
-; RV32I-NEXT:    ret
-;
-; RV32B-LABEL: grev6_i32:
-; RV32B:       # %bb.0:
-; RV32B-NEXT:    rev2.b a0, a0
-; RV32B-NEXT:    ret
-;
-; RV32ZBP-LABEL: grev6_i32:
-; RV32ZBP:       # %bb.0:
-; RV32ZBP-NEXT:    rev2.b a0, a0
-; RV32ZBP-NEXT:    ret
-  %and1 = shl i32 %a, 2
-  %shl1 = and i32 %and1, -858993460
-  %and1b = lshr i32 %a, 2
-  %shr1 = and i32 %and1b, 858993459
-  %or1 = or i32 %shl1, %shr1
-  %and2 = shl i32 %or1, 4
-  %shl2 = and i32 %and2, -252645136
-  %and2b = lshr i32 %or1, 4
-  %shr2 = and i32 %and2b, 252645135
-  %or2 = or i32 %shl2, %shr2
-  ret i32 %or2
-}
-
-define i64 @grev6_i64(i64 %a) nounwind {
-; RV32I-LABEL: grev6_i64:
-; RV32I:       # %bb.0:
-; RV32I-NEXT:    slli a2, a1, 2
-; RV32I-NEXT:    slli a3, a0, 2
-; RV32I-NEXT:    lui a4, 838861
-; RV32I-NEXT:    addi a4, a4, -820
-; RV32I-NEXT:    and a3, a3, a4
-; RV32I-NEXT:    and a2, a2, a4
-; RV32I-NEXT:    srli a1, a1, 2
-; RV32I-NEXT:    srli a0, a0, 2
-; RV32I-NEXT:    lui a4, 209715
-; RV32I-NEXT:    addi a4, a4, 819
-; RV32I-NEXT:    and a0, a0, a4
-; RV32I-NEXT:    and a1, a1, a4
-; RV32I-NEXT:    or a1, a2, a1
-; RV32I-NEXT:    or a0, a3, a0
-; RV32I-NEXT:    slli a2, a0, 4
-; RV32I-NEXT:    slli a3, a1, 4
-; RV32I-NEXT:    lui a4, 986895
-; RV32I-NEXT:    addi a4, a4, 240
-; RV32I-NEXT:    and a3, a3, a4
-; RV32I-NEXT:    and a2, a2, a4
-; RV32I-NEXT:    srli a0, a0, 4
-; RV32I-NEXT:    srli a1, a1, 4
-; RV32I-NEXT:    lui a4, 61681
-; RV32I-NEXT:    addi a4, a4, -241
-; RV32I-NEXT:    and a1, a1, a4
-; RV32I-NEXT:    and a0, a0, a4
-; RV32I-NEXT:    or a0, a2, a0
-; RV32I-NEXT:    or a1, a3, a1
-; RV32I-NEXT:    ret
-;
-; RV32B-LABEL: grev6_i64:
-; RV32B:       # %bb.0:
-; RV32B-NEXT:    rev2.b a0, a0
-; RV32B-NEXT:    rev2.b a1, a1
-; RV32B-NEXT:    ret
-;
-; RV32ZBP-LABEL: grev6_i64:
-; RV32ZBP:       # %bb.0:
-; RV32ZBP-NEXT:    rev2.b a0, a0
-; RV32ZBP-NEXT:    rev2.b a1, a1
-; RV32ZBP-NEXT:    ret
-  %and1 = shl i64 %a, 2
-  %shl1 = and i64 %and1, -3689348814741910324
-  %and1b = lshr i64 %a, 2
-  %shr1 = and i64 %and1b, 3689348814741910323
-  %or1 = or i64 %shl1, %shr1
-  %and2 = shl i64 %or1, 4
-  %shl2 = and i64 %and2, -1085102592571150096
-  %and2b = lshr i64 %or1, 4
-  %shr2 = and i64 %and2b, 1085102592571150095
-  %or2 = or i64 %shl2, %shr2
-  ret i64 %or2
-}
-
-define i32 @grev7_i32(i32 %a) nounwind {
-; RV32I-LABEL: grev7_i32:
-; RV32I:       # %bb.0:
-; RV32I-NEXT:    slli a1, a0, 1
-; RV32I-NEXT:    lui a2, 699051
-; RV32I-NEXT:    addi a2, a2, -1366
-; RV32I-NEXT:    and a1, a1, a2
-; RV32I-NEXT:    srli a0, a0, 1
-; RV32I-NEXT:    lui a2, 349525
-; RV32I-NEXT:    addi a2, a2, 1365
-; RV32I-NEXT:    and a0, a0, a2
-; RV32I-NEXT:    or a0, a1, a0
-; RV32I-NEXT:    slli a1, a0, 2
-; RV32I-NEXT:    lui a2, 838861
-; RV32I-NEXT:    addi a2, a2, -820
-; RV32I-NEXT:    and a1, a1, a2
-; RV32I-NEXT:    srli a0, a0, 2
-; RV32I-NEXT:    lui a2, 209715
-; RV32I-NEXT:    addi a2, a2, 819
-; RV32I-NEXT:    and a0, a0, a2
-; RV32I-NEXT:    or a0, a1, a0
-; RV32I-NEXT:    slli a1, a0, 4
-; RV32I-NEXT:    lui a2, 986895
-; RV32I-NEXT:    addi a2, a2, 240
-; RV32I-NEXT:    and a1, a1, a2
-; RV32I-NEXT:    srli a0, a0, 4
-; RV32I-NEXT:    lui a2, 61681
-; RV32I-NEXT:    addi a2, a2, -241
-; RV32I-NEXT:    and a0, a0, a2
-; RV32I-NEXT:    or a0, a1, a0
-; RV32I-NEXT:    ret
-;
-; RV32B-LABEL: grev7_i32:
-; RV32B:       # %bb.0:
-; RV32B-NEXT:    rev.b a0, a0
-; RV32B-NEXT:    ret
-;
-; RV32ZBP-LABEL: grev7_i32:
-; RV32ZBP:       # %bb.0:
-; RV32ZBP-NEXT:    rev.b a0, a0
-; RV32ZBP-NEXT:    ret
-  %and1 = shl i32 %a, 1
-  %shl1 = and i32 %and1, -1431655766
-  %and1b = lshr i32 %a, 1
-  %shr1 = and i32 %and1b, 1431655765
-  %or1 = or i32 %shl1, %shr1
-  %and2 = shl i32 %or1, 2
-  %shl2 = and i32 %and2, -858993460
-  %and2b = lshr i32 %or1, 2
-  %shr2 = and i32 %and2b, 858993459
-  %or2 = or i32 %shl2, %shr2
-  %and3 = shl i32 %or2, 4
-  %shl3 = and i32 %and3, -252645136
-  %and3b = lshr i32 %or2, 4
-  %shr3 = and i32 %and3b, 252645135
-  %or3 = or i32 %shl3, %shr3
-  ret i32 %or3
-}
-
-define i64 @grev7_i64(i64 %a) nounwind {
-; RV32I-LABEL: grev7_i64:
-; RV32I:       # %bb.0:
-; RV32I-NEXT:    slli a2, a0, 1
-; RV32I-NEXT:    slli a3, a1, 1
-; RV32I-NEXT:    lui a4, 699051
-; RV32I-NEXT:    addi a4, a4, -1366
-; RV32I-NEXT:    and a3, a3, a4
-; RV32I-NEXT:    and a2, a2, a4
-; RV32I-NEXT:    srli a0, a0, 1
-; RV32I-NEXT:    srli a1, a1, 1
-; RV32I-NEXT:    lui a4, 349525
-; RV32I-NEXT:    addi a4, a4, 1365
-; RV32I-NEXT:    and a1, a1, a4
-; RV32I-NEXT:    and a0, a0, a4
-; RV32I-NEXT:    or a0, a2, a0
-; RV32I-NEXT:    or a1, a3, a1
-; RV32I-NEXT:    slli a2, a1, 2
-; RV32I-NEXT:    slli a3, a0, 2
-; RV32I-NEXT:    lui a4, 838861
-; RV32I-NEXT:    addi a4, a4, -820
-; RV32I-NEXT:    and a3, a3, a4
-; RV32I-NEXT:    and a2, a2, a4
-; RV32I-NEXT:    srli a1, a1, 2
-; RV32I-NEXT:    srli a0, a0, 2
-; RV32I-NEXT:    lui a4, 209715
-; RV32I-NEXT:    addi a4, a4, 819
-; RV32I-NEXT:    and a0, a0, a4
-; RV32I-NEXT:    and a1, a1, a4
-; RV32I-NEXT:    or a1, a2, a1
-; RV32I-NEXT:    or a0, a3, a0
-; RV32I-NEXT:    slli a2, a0, 4
-; RV32I-NEXT:    slli a3, a1, 4
-; RV32I-NEXT:    lui a4, 986895
-; RV32I-NEXT:    addi a4, a4, 240
-; RV32I-NEXT:    and a3, a3, a4
-; RV32I-NEXT:    and a2, a2, a4
-; RV32I-NEXT:    srli a0, a0, 4
-; RV32I-NEXT:    srli a1, a1, 4
-; RV32I-NEXT:    lui a4, 61681
-; RV32I-NEXT:    addi a4, a4, -241
-; RV32I-NEXT:    and a1, a1, a4
-; RV32I-NEXT:    and a0, a0, a4
-; RV32I-NEXT:    or a0, a2, a0
-; RV32I-NEXT:    or a1, a3, a1
-; RV32I-NEXT:    ret
-;
-; RV32B-LABEL: grev7_i64:
-; RV32B:       # %bb.0:
-; RV32B-NEXT:    rev.b a0, a0
-; RV32B-NEXT:    rev.b a1, a1
-; RV32B-NEXT:    ret
-;
-; RV32ZBP-LABEL: grev7_i64:
-; RV32ZBP:       # %bb.0:
-; RV32ZBP-NEXT:    rev.b a0, a0
-; RV32ZBP-NEXT:    rev.b a1, a1
-; RV32ZBP-NEXT:    ret
-  %and1 = shl i64 %a, 1
-  %shl1 = and i64 %and1, -6148914691236517206
-  %and1b = lshr i64 %a, 1
-  %shr1 = and i64 %and1b, 6148914691236517205
-  %or1 = or i64 %shl1, %shr1
-  %and2 = shl i64 %or1, 2
-  %shl2 = and i64 %and2, -3689348814741910324
-  %and2b = lshr i64 %or1, 2
-  %shr2 = and i64 %and2b, 3689348814741910323
-  %or2 = or i64 %shl2, %shr2
-  %and3 = shl i64 %or2, 4
-  %shl3 = and i64 %and3, -1085102592571150096
-  %and3b = lshr i64 %or2, 4
-  %shr3 = and i64 %and3b, 1085102592571150095
-  %or3 = or i64 %shl3, %shr3
-  ret i64 %or3
-}
-
-define i32 @grev8_i32(i32 %a) nounwind {
-; RV32I-LABEL: grev8_i32:
-; RV32I:       # %bb.0:
-; RV32I-NEXT:    slli a1, a0, 8
-; RV32I-NEXT:    lui a2, 1044496
-; RV32I-NEXT:    addi a2, a2, -256
-; RV32I-NEXT:    and a1, a1, a2
-; RV32I-NEXT:    srli a0, a0, 8
-; RV32I-NEXT:    lui a2, 4080
-; RV32I-NEXT:    addi a2, a2, 255
-; RV32I-NEXT:    and a0, a0, a2
-; RV32I-NEXT:    or a0, a1, a0
-; RV32I-NEXT:    ret
-;
-; RV32B-LABEL: grev8_i32:
-; RV32B:       # %bb.0:
-; RV32B-NEXT:    rev8.h a0, a0
-; RV32B-NEXT:    ret
-;
-; RV32ZBP-LABEL: grev8_i32:
-; RV32ZBP:       # %bb.0:
-; RV32ZBP-NEXT:    rev8.h a0, a0
-; RV32ZBP-NEXT:    ret
-  %and = shl i32 %a, 8
-  %shl = and i32 %and, -16711936
-  %and1 = lshr i32 %a, 8
-  %shr = and i32 %and1, 16711935
-  %or = or i32 %shl, %shr
-  ret i32 %or
-}
-
-define i64 @grev8_i64(i64 %a) nounwind {
-; RV32I-LABEL: grev8_i64:
-; RV32I:       # %bb.0:
-; RV32I-NEXT:    slli a2, a0, 8
-; RV32I-NEXT:    slli a3, a1, 8
-; RV32I-NEXT:    lui a4, 1044496
-; RV32I-NEXT:    addi a4, a4, -256
-; RV32I-NEXT:    and a3, a3, a4
-; RV32I-NEXT:    and a2, a2, a4
-; RV32I-NEXT:    srli a0, a0, 8
-; RV32I-NEXT:    srli a1, a1, 8
-; RV32I-NEXT:    lui a4, 4080
-; RV32I-NEXT:    addi a4, a4, 255
-; RV32I-NEXT:    and a1, a1, a4
-; RV32I-NEXT:    and a0, a0, a4
-; RV32I-NEXT:    or a0, a2, a0
-; RV32I-NEXT:    or a1, a3, a1
-; RV32I-NEXT:    ret
-;
-; RV32B-LABEL: grev8_i64:
-; RV32B:       # %bb.0:
-; RV32B-NEXT:    rev8.h a0, a0
-; RV32B-NEXT:    rev8.h a1, a1
-; RV32B-NEXT:    ret
-;
-; RV32ZBP-LABEL: grev8_i64:
-; RV32ZBP:       # %bb.0:
-; RV32ZBP-NEXT:    rev8.h a0, a0
-; RV32ZBP-NEXT:    rev8.h a1, a1
-; RV32ZBP-NEXT:    ret
-  %and = shl i64 %a, 8
-  %shl = and i64 %and, -71777214294589696
-  %and1 = lshr i64 %a, 8
-  %shr = and i64 %and1, 71777214294589695
-  %or = or i64 %shl, %shr
-  ret i64 %or
-}
-
-define i32 @grev16_i32(i32 %a) nounwind {
-; RV32I-LABEL: grev16_i32:
-; RV32I:       # %bb.0:
-; RV32I-NEXT:    slli a1, a0, 16
-; RV32I-NEXT:    srli a0, a0, 16
-; RV32I-NEXT:    or a0, a1, a0
-; RV32I-NEXT:    ret
-;
-; RV32B-LABEL: grev16_i32:
-; RV32B:       # %bb.0:
-; RV32B-NEXT:    rori a0, a0, 16
-; RV32B-NEXT:    ret
-;
-; RV32ZBP-LABEL: grev16_i32:
-; RV32ZBP:       # %bb.0:
-; RV32ZBP-NEXT:    rori a0, a0, 16
-; RV32ZBP-NEXT:    ret
-  %shl = shl i32 %a, 16
-  %shr = lshr i32 %a, 16
-  %or = or i32 %shl, %shr
-  ret i32 %or
-}
-
-
-define i32 @grev3b_i32(i32 %a) nounwind {
-; RV32I-LABEL: grev3b_i32:
-; RV32I:       # %bb.0:
-; RV32I-NEXT:    slli a1, a0, 2
-; RV32I-NEXT:    lui a2, 838861
-; RV32I-NEXT:    addi a2, a2, -820
-; RV32I-NEXT:    and a1, a1, a2
-; RV32I-NEXT:    srli a0, a0, 2
-; RV32I-NEXT:    lui a2, 209715
-; RV32I-NEXT:    addi a2, a2, 819
-; RV32I-NEXT:    and a0, a0, a2
-; RV32I-NEXT:    or a0, a1, a0
-; RV32I-NEXT:    slli a1, a0, 1
-; RV32I-NEXT:    lui a2, 699051
-; RV32I-NEXT:    addi a2, a2, -1366
-; RV32I-NEXT:    and a1, a1, a2
-; RV32I-NEXT:    srli a0, a0, 1
-; RV32I-NEXT:    lui a2, 349525
-; RV32I-NEXT:    addi a2, a2, 1365
-; RV32I-NEXT:    and a0, a0, a2
-; RV32I-NEXT:    or a0, a1, a0
-; RV32I-NEXT:    ret
-;
-; RV32B-LABEL: grev3b_i32:
-; RV32B:       # %bb.0:
-; RV32B-NEXT:    rev.n a0, a0
-; RV32B-NEXT:    ret
-;
-; RV32ZBP-LABEL: grev3b_i32:
-; RV32ZBP:       # %bb.0:
-; RV32ZBP-NEXT:    rev.n a0, a0
-; RV32ZBP-NEXT:    ret
-  %and2 = shl i32 %a, 2
-  %shl2 = and i32 %and2, -858993460
-  %and2b = lshr i32 %a, 2
-  %shr2 = and i32 %and2b, 858993459
-  %or2 = or i32 %shl2, %shr2
-  %and1 = shl i32 %or2, 1
-  %shl1 = and i32 %and1, -1431655766
-  %and1b = lshr i32 %or2, 1
-  %shr1 = and i32 %and1b, 1431655765
-  %or1 = or i32 %shl1, %shr1
-  ret i32 %or1
-}
-
-define i64 @grev3b_i64(i64 %a) nounwind {
-; RV32I-LABEL: grev3b_i64:
-; RV32I:       # %bb.0:
-; RV32I-NEXT:    slli a2, a1, 2
-; RV32I-NEXT:    slli a3, a0, 2
-; RV32I-NEXT:    lui a4, 838861
-; RV32I-NEXT:    addi a4, a4, -820
-; RV32I-NEXT:    and a3, a3, a4
-; RV32I-NEXT:    and a2, a2, a4
-; RV32I-NEXT:    srli a1, a1, 2
-; RV32I-NEXT:    srli a0, a0, 2
-; RV32I-NEXT:    lui a4, 209715
-; RV32I-NEXT:    addi a4, a4, 819
-; RV32I-NEXT:    and a0, a0, a4
-; RV32I-NEXT:    and a1, a1, a4
-; RV32I-NEXT:    or a1, a2, a1
-; RV32I-NEXT:    or a0, a3, a0
-; RV32I-NEXT:    slli a2, a0, 1
-; RV32I-NEXT:    slli a3, a1, 1
-; RV32I-NEXT:    lui a4, 699051
-; RV32I-NEXT:    addi a4, a4, -1366
-; RV32I-NEXT:    and a3, a3, a4
-; RV32I-NEXT:    and a2, a2, a4
-; RV32I-NEXT:    srli a0, a0, 1
-; RV32I-NEXT:    srli a1, a1, 1
-; RV32I-NEXT:    lui a4, 349525
-; RV32I-NEXT:    addi a4, a4, 1365
-; RV32I-NEXT:    and a1, a1, a4
-; RV32I-NEXT:    and a0, a0, a4
-; RV32I-NEXT:    or a0, a2, a0
-; RV32I-NEXT:    or a1, a3, a1
-; RV32I-NEXT:    ret
-;
-; RV32B-LABEL: grev3b_i64:
-; RV32B:       # %bb.0:
-; RV32B-NEXT:    rev.n a0, a0
-; RV32B-NEXT:    rev.n a1, a1
-; RV32B-NEXT:    ret
-;
-; RV32ZBP-LABEL: grev3b_i64:
-; RV32ZBP:       # %bb.0:
-; RV32ZBP-NEXT:    rev.n a0, a0
-; RV32ZBP-NEXT:    rev.n a1, a1
-; RV32ZBP-NEXT:    ret
-  %and2 = shl i64 %a, 2
-  %shl2 = and i64 %and2, -3689348814741910324
-  %and2b = lshr i64 %a, 2
-  %shr2 = and i64 %and2b, 3689348814741910323
-  %or2 = or i64 %shl2, %shr2
-  %and1 = shl i64 %or2, 1
-  %shl1 = and i64 %and1, -6148914691236517206
-  %and1b = lshr i64 %or2, 1
-  %shr1 = and i64 %and1b, 6148914691236517205
-  %or1 = or i64 %shl1, %shr1
-  ret i64 %or1
-}
-
-; grev1, grev2, grev1 -> grev2
-define i32 @grev2b_i32(i32 %a) nounwind {
-; RV32I-LABEL: grev2b_i32:
-; RV32I:       # %bb.0:
-; RV32I-NEXT:    slli a1, a0, 1
-; RV32I-NEXT:    lui a2, 699051
-; RV32I-NEXT:    addi a2, a2, -1366
-; RV32I-NEXT:    and a1, a1, a2
-; RV32I-NEXT:    srli a0, a0, 1
-; RV32I-NEXT:    lui a3, 349525
-; RV32I-NEXT:    addi a3, a3, 1365
-; RV32I-NEXT:    and a0, a0, a3
-; RV32I-NEXT:    or a0, a1, a0
-; RV32I-NEXT:    slli a1, a0, 2
-; RV32I-NEXT:    lui a4, 838861
-; RV32I-NEXT:    addi a4, a4, -820
-; RV32I-NEXT:    and a1, a1, a4
-; RV32I-NEXT:    srli a0, a0, 2
-; RV32I-NEXT:    lui a4, 209715
-; RV32I-NEXT:    addi a4, a4, 819
-; RV32I-NEXT:    and a0, a0, a4
-; RV32I-NEXT:    or a0, a1, a0
-; RV32I-NEXT:    slli a1, a0, 1
-; RV32I-NEXT:    and a1, a1, a2
-; RV32I-NEXT:    srli a0, a0, 1
-; RV32I-NEXT:    and a0, a0, a3
-; RV32I-NEXT:    or a0, a1, a0
-; RV32I-NEXT:    ret
-;
-; RV32B-LABEL: grev2b_i32:
-; RV32B:       # %bb.0:
-; RV32B-NEXT:    rev2.n a0, a0
-; RV32B-NEXT:    ret
-;
-; RV32ZBP-LABEL: grev2b_i32:
-; RV32ZBP:       # %bb.0:
-; RV32ZBP-NEXT:    rev2.n a0, a0
-; RV32ZBP-NEXT:    ret
-  %and1 = shl i32 %a, 1
-  %shl1 = and i32 %and1, -1431655766
-  %and1b = lshr i32 %a, 1
-  %shr1 = and i32 %and1b, 1431655765
-  %or1 = or i32 %shl1, %shr1
-  %and2 = shl i32 %or1, 2
-  %shl2 = and i32 %and2, -858993460
-  %and2b = lshr i32 %or1, 2
-  %shr2 = and i32 %and2b, 858993459
-  %or2 = or i32 %shl2, %shr2
-  %and3 = shl i32 %or2, 1
-  %shl3 = and i32 %and3, -1431655766
-  %and3b = lshr i32 %or2, 1
-  %shr3 = and i32 %and3b, 1431655765
-  %or3 = or i32 %shl3, %shr3
-  ret i32 %or3
-}
-
-; grev1, grev2, grev1 -> grev2
-define i64 @grev2b_i64(i64 %a) nounwind {
-; RV32I-LABEL: grev2b_i64:
-; RV32I:       # %bb.0:
-; RV32I-NEXT:    slli a2, a0, 1
-; RV32I-NEXT:    slli a3, a1, 1
-; RV32I-NEXT:    lui a4, 699051
-; RV32I-NEXT:    addi a6, a4, -1366
-; RV32I-NEXT:    and a3, a3, a6
-; RV32I-NEXT:    and a2, a2, a6
-; RV32I-NEXT:    srli a0, a0, 1
-; RV32I-NEXT:    srli a1, a1, 1
-; RV32I-NEXT:    lui a5, 349525
-; RV32I-NEXT:    addi a5, a5, 1365
-; RV32I-NEXT:    and a1, a1, a5
-; RV32I-NEXT:    and a0, a0, a5
-; RV32I-NEXT:    or a0, a2, a0
-; RV32I-NEXT:    or a1, a3, a1
-; RV32I-NEXT:    slli a2, a1, 2
-; RV32I-NEXT:    slli a3, a0, 2
-; RV32I-NEXT:    lui a4, 838861
-; RV32I-NEXT:    addi a4, a4, -820
-; RV32I-NEXT:    and a3, a3, a4
-; RV32I-NEXT:    and a2, a2, a4
-; RV32I-NEXT:    srli a1, a1, 2
-; RV32I-NEXT:    srli a0, a0, 2
-; RV32I-NEXT:    lui a4, 209715
-; RV32I-NEXT:    addi a4, a4, 819
-; RV32I-NEXT:    and a0, a0, a4
-; RV32I-NEXT:    and a1, a1, a4
-; RV32I-NEXT:    or a1, a2, a1
-; RV32I-NEXT:    or a0, a3, a0
-; RV32I-NEXT:    slli a2, a0, 1
-; RV32I-NEXT:    slli a3, a1, 1
-; RV32I-NEXT:    and a3, a3, a6
-; RV32I-NEXT:    and a2, a2, a6
-; RV32I-NEXT:    srli a0, a0, 1
-; RV32I-NEXT:    srli a1, a1, 1
-; RV32I-NEXT:    and a1, a1, a5
-; RV32I-NEXT:    and a0, a0, a5
-; RV32I-NEXT:    or a0, a2, a0
-; RV32I-NEXT:    or a1, a3, a1
-; RV32I-NEXT:    ret
-;
-; RV32B-LABEL: grev2b_i64:
-; RV32B:       # %bb.0:
-; RV32B-NEXT:    rev2.n a0, a0
-; RV32B-NEXT:    rev2.n a1, a1
-; RV32B-NEXT:    ret
-;
-; RV32ZBP-LABEL: grev2b_i64:
-; RV32ZBP:       # %bb.0:
-; RV32ZBP-NEXT:    rev2.n a0, a0
-; RV32ZBP-NEXT:    rev2.n a1, a1
-; RV32ZBP-NEXT:    ret
-  %and1 = shl i64 %a, 1
-  %shl1 = and i64 %and1, -6148914691236517206
-  %and1b = lshr i64 %a, 1
-  %shr1 = and i64 %and1b, 6148914691236517205
-  %or1 = or i64 %shl1, %shr1
-  %and2 = shl i64 %or1, 2
-  %shl2 = and i64 %and2, -3689348814741910324
-  %and2b = lshr i64 %or1, 2
-  %shr2 = and i64 %and2b, 3689348814741910323
-  %or2 = or i64 %shl2, %shr2
-  %and3 = shl i64 %or2, 1
-  %shl3 = and i64 %and3, -6148914691236517206
-  %and3b = lshr i64 %or2, 1
-  %shr3 = and i64 %and3b, 6148914691236517205
-  %or3 = or i64 %shl3, %shr3
-  ret i64 %or3
-}
-
-; grev1, grev2, grev1, grev2 -> identity
-define i32 @grev0_i32(i32 %a) nounwind {
-; RV32I-LABEL: grev0_i32:
-; RV32I:       # %bb.0:
-; RV32I-NEXT:    slli a1, a0, 1
-; RV32I-NEXT:    lui a2, 699051
-; RV32I-NEXT:    addi a2, a2, -1366
-; RV32I-NEXT:    and a1, a1, a2
-; RV32I-NEXT:    srli a0, a0, 1
-; RV32I-NEXT:    lui a3, 349525
-; RV32I-NEXT:    addi a3, a3, 1365
-; RV32I-NEXT:    and a0, a0, a3
-; RV32I-NEXT:    or a0, a1, a0
-; RV32I-NEXT:    slli a1, a0, 2
-; RV32I-NEXT:    lui a4, 838861
-; RV32I-NEXT:    addi a4, a4, -820
-; RV32I-NEXT:    and a1, a1, a4
-; RV32I-NEXT:    srli a0, a0, 2
-; RV32I-NEXT:    lui a5, 209715
-; RV32I-NEXT:    addi a5, a5, 819
-; RV32I-NEXT:    and a0, a0, a5
-; RV32I-NEXT:    or a0, a1, a0
-; RV32I-NEXT:    slli a1, a0, 1
-; RV32I-NEXT:    and a1, a1, a2
-; RV32I-NEXT:    srli a0, a0, 1
-; RV32I-NEXT:    and a0, a0, a3
-; RV32I-NEXT:    or a0, a1, a0
-; RV32I-NEXT:    slli a1, a0, 2
-; RV32I-NEXT:    and a1, a1, a4
-; RV32I-NEXT:    srli a0, a0, 2
-; RV32I-NEXT:    and a0, a0, a5
-; RV32I-NEXT:    or a0, a1, a0
-; RV32I-NEXT:    ret
-;
-; RV32B-LABEL: grev0_i32:
-; RV32B:       # %bb.0:
-; RV32B-NEXT:    ret
-;
-; RV32ZBP-LABEL: grev0_i32:
-; RV32ZBP:       # %bb.0:
-; RV32ZBP-NEXT:    ret
-  %and1 = shl i32 %a, 1
-  %shl1 = and i32 %and1, -1431655766
-  %and1b = lshr i32 %a, 1
-  %shr1 = and i32 %and1b, 1431655765
-  %or1 = or i32 %shl1, %shr1
-  %and2 = shl i32 %or1, 2
-  %shl2 = and i32 %and2, -858993460
-  %and2b = lshr i32 %or1, 2
-  %shr2 = and i32 %and2b, 858993459
-  %or2 = or i32 %shl2, %shr2
-  %and3 = shl i32 %or2, 1
-  %shl3 = and i32 %and3, -1431655766
-  %and3b = lshr i32 %or2, 1
-  %shr3 = and i32 %and3b, 1431655765
-  %or3 = or i32 %shl3, %shr3
-  %and4 = shl i32 %or3, 2
-  %shl4 = and i32 %and4, -858993460
-  %and4b = lshr i32 %or3, 2
-  %shr4 = and i32 %and4b, 858993459
-  %or4 = or i32 %shl4, %shr4
-  ret i32 %or4
-}
-
-; grev1, grev2, grev1, grev2 -> identity
-define i64 @grev0_i64(i64 %a) nounwind {
-; RV32I-LABEL: grev0_i64:
-; RV32I:       # %bb.0:
-; RV32I-NEXT:    slli a2, a1, 1
-; RV32I-NEXT:    slli a3, a0, 1
-; RV32I-NEXT:    lui a4, 699051
-; RV32I-NEXT:    addi a6, a4, -1366
-; RV32I-NEXT:    and a3, a3, a6
-; RV32I-NEXT:    and a2, a2, a6
-; RV32I-NEXT:    srli a1, a1, 1
-; RV32I-NEXT:    srli a0, a0, 1
-; RV32I-NEXT:    lui a5, 349525
-; RV32I-NEXT:    addi a7, a5, 1365
-; RV32I-NEXT:    and a0, a0, a7
-; RV32I-NEXT:    and a1, a1, a7
-; RV32I-NEXT:    or a1, a2, a1
-; RV32I-NEXT:    or a0, a3, a0
-; RV32I-NEXT:    slli a2, a0, 2
-; RV32I-NEXT:    slli a3, a1, 2
-; RV32I-NEXT:    lui a4, 838861
-; RV32I-NEXT:    addi a4, a4, -820
-; RV32I-NEXT:    and a3, a3, a4
-; RV32I-NEXT:    and a2, a2, a4
-; RV32I-NEXT:    srli a0, a0, 2
-; RV32I-NEXT:    srli a1, a1, 2
-; RV32I-NEXT:    lui a5, 209715
-; RV32I-NEXT:    addi a5, a5, 819
-; RV32I-NEXT:    and a1, a1, a5
-; RV32I-NEXT:    and a0, a0, a5
-; RV32I-NEXT:    or a0, a2, a0
-; RV32I-NEXT:    or a1, a3, a1
-; RV32I-NEXT:    slli a2, a1, 1
-; RV32I-NEXT:    slli a3, a0, 1
-; RV32I-NEXT:    and a3, a3, a6
-; RV32I-NEXT:    and a2, a2, a6
-; RV32I-NEXT:    srli a1, a1, 1
-; RV32I-NEXT:    srli a0, a0, 1
-; RV32I-NEXT:    and a0, a0, a7
-; RV32I-NEXT:    and a1, a1, a7
-; RV32I-NEXT:    or a1, a2, a1
-; RV32I-NEXT:    or a0, a3, a0
-; RV32I-NEXT:    slli a2, a0, 2
-; RV32I-NEXT:    slli a3, a1, 2
-; RV32I-NEXT:    and a3, a3, a4
-; RV32I-NEXT:    and a2, a2, a4
-; RV32I-NEXT:    srli a0, a0, 2
-; RV32I-NEXT:    srli a1, a1, 2
-; RV32I-NEXT:    and a1, a1, a5
-; RV32I-NEXT:    and a0, a0, a5
-; RV32I-NEXT:    or a0, a2, a0
-; RV32I-NEXT:    or a1, a3, a1
-; RV32I-NEXT:    ret
-;
-; RV32B-LABEL: grev0_i64:
-; RV32B:       # %bb.0:
-; RV32B-NEXT:    ret
-;
-; RV32ZBP-LABEL: grev0_i64:
-; RV32ZBP:       # %bb.0:
-; RV32ZBP-NEXT:    ret
-  %and1 = shl i64 %a, 1
-  %shl1 = and i64 %and1, -6148914691236517206
-  %and1b = lshr i64 %a, 1
-  %shr1 = and i64 %and1b, 6148914691236517205
-  %or1 = or i64 %shl1, %shr1
-  %and2 = shl i64 %or1, 2
-  %shl2 = and i64 %and2, -3689348814741910324
-  %and2b = lshr i64 %or1, 2
-  %shr2 = and i64 %and2b, 3689348814741910323
-  %or2 = or i64 %shl2, %shr2
-  %and3 = shl i64 %or2, 1
-  %shl3 = and i64 %and3, -6148914691236517206
-  %and3b = lshr i64 %or2, 1
-  %shr3 = and i64 %and3b, 6148914691236517205
-  %or3 = or i64 %shl3, %shr3
-  %and4 = shl i64 %or3, 2
-  %shl4 = and i64 %and4, -3689348814741910324
-  %and4b = lshr i64 %or3, 2
-  %shr4 = and i64 %and4b, 3689348814741910323
-  %or4 = or i64 %shl4, %shr4
-  ret i64 %or4
-}
-
-declare i32 @llvm.fshl.i32(i32, i32, i32)
-declare i32 @llvm.fshr.i32(i32, i32, i32)
-
-define signext i32 @grev16_i32_fshl(i32 signext %a) nounwind {
-; RV32I-LABEL: grev16_i32_fshl:
-; RV32I:       # %bb.0:
-; RV32I-NEXT:    srli a1, a0, 16
-; RV32I-NEXT:    slli a0, a0, 16
-; RV32I-NEXT:    or a0, a0, a1
-; RV32I-NEXT:    ret
-;
-; RV32B-LABEL: grev16_i32_fshl:
-; RV32B:       # %bb.0:
-; RV32B-NEXT:    rori a0, a0, 16
-; RV32B-NEXT:    ret
-;
-; RV32ZBP-LABEL: grev16_i32_fshl:
-; RV32ZBP:       # %bb.0:
-; RV32ZBP-NEXT:    rori a0, a0, 16
-; RV32ZBP-NEXT:    ret
-  %or = tail call i32 @llvm.fshl.i32(i32 %a, i32 %a, i32 16)
-  ret i32 %or
-}
-
-define signext i32 @grev16_i32_fshr(i32 signext %a) nounwind {
-; RV32I-LABEL: grev16_i32_fshr:
-; RV32I:       # %bb.0:
-; RV32I-NEXT:    slli a1, a0, 16
-; RV32I-NEXT:    srli a0, a0, 16
-; RV32I-NEXT:    or a0, a0, a1
-; RV32I-NEXT:    ret
-;
-; RV32B-LABEL: grev16_i32_fshr:
-; RV32B:       # %bb.0:
-; RV32B-NEXT:    rori a0, a0, 16
-; RV32B-NEXT:    ret
-;
-; RV32ZBP-LABEL: grev16_i32_fshr:
-; RV32ZBP:       # %bb.0:
-; RV32ZBP-NEXT:    rori a0, a0, 16
-; RV32ZBP-NEXT:    ret
-  %or = tail call i32 @llvm.fshr.i32(i32 %a, i32 %a, i32 16)
-  ret i32 %or
-}
-
-define i64 @grev16_i64(i64 %a) nounwind {
-; RV32I-LABEL: grev16_i64:
-; RV32I:       # %bb.0:
-; RV32I-NEXT:    slli a2, a1, 16
-; RV32I-NEXT:    srli a3, a0, 16
-; RV32I-NEXT:    slli a0, a0, 16
-; RV32I-NEXT:    or a0, a0, a3
-; RV32I-NEXT:    srli a1, a1, 16
-; RV32I-NEXT:    or a1, a2, a1
-; RV32I-NEXT:    ret
-;
-; RV32B-LABEL: grev16_i64:
-; RV32B:       # %bb.0:
-; RV32B-NEXT:    rori a0, a0, 16
-; RV32B-NEXT:    rori a1, a1, 16
-; RV32B-NEXT:    ret
-;
-; RV32ZBP-LABEL: grev16_i64:
-; RV32ZBP:       # %bb.0:
-; RV32ZBP-NEXT:    rori a0, a0, 16
-; RV32ZBP-NEXT:    rori a1, a1, 16
-; RV32ZBP-NEXT:    ret
-  %and = shl i64 %a, 16
-  %shl = and i64 %and, -281470681808896
-  %and1 = lshr i64 %a, 16
-  %shr = and i64 %and1, 281470681808895
-  %or = or i64 %shl, %shr
-  ret i64 %or
-}
-
-declare i16 @llvm.bswap.i16(i16)
-
-define zeroext i16 @bswap_i16(i16 zeroext %a) nounwind {
-; RV32I-LABEL: bswap_i16:
-; RV32I:       # %bb.0:
-; RV32I-NEXT:    srli a1, a0, 8
-; RV32I-NEXT:    slli a0, a0, 8
-; RV32I-NEXT:    or a0, a0, a1
-; RV32I-NEXT:    lui a1, 16
-; RV32I-NEXT:    addi a1, a1, -1
-; RV32I-NEXT:    and a0, a0, a1
-; RV32I-NEXT:    ret
-;
-; RV32B-LABEL: bswap_i16:
-; RV32B:       # %bb.0:
-; RV32B-NEXT:    rev8.h a0, a0
-; RV32B-NEXT:    ret
-;
-; RV32ZBP-LABEL: bswap_i16:
-; RV32ZBP:       # %bb.0:
-; RV32ZBP-NEXT:    rev8.h a0, a0
-; RV32ZBP-NEXT:    ret
-  %1 = tail call i16 @llvm.bswap.i16(i16 %a)
-  ret i16 %1
-}
-
-declare i32 @llvm.bswap.i32(i32)
-
-define i32 @bswap_i32(i32 %a) nounwind {
-; RV32I-LABEL: bswap_i32:
-; RV32I:       # %bb.0:
-; RV32I-NEXT:    srli a1, a0, 8
-; RV32I-NEXT:    lui a2, 16
-; RV32I-NEXT:    addi a2, a2, -256
-; RV32I-NEXT:    and a1, a1, a2
-; RV32I-NEXT:    srli a2, a0, 24
-; RV32I-NEXT:    or a1, a1, a2
-; RV32I-NEXT:    slli a2, a0, 8
-; RV32I-NEXT:    lui a3, 4080
-; RV32I-NEXT:    and a2, a2, a3
-; RV32I-NEXT:    slli a0, a0, 24
-; RV32I-NEXT:    or a0, a0, a2
-; RV32I-NEXT:    or a0, a0, a1
-; RV32I-NEXT:    ret
-;
-; RV32B-LABEL: bswap_i32:
-; RV32B:       # %bb.0:
-; RV32B-NEXT:    rev8 a0, a0
-; RV32B-NEXT:    ret
-;
-; RV32ZBP-LABEL: bswap_i32:
-; RV32ZBP:       # %bb.0:
-; RV32ZBP-NEXT:    rev8 a0, a0
-; RV32ZBP-NEXT:    ret
-  %1 = tail call i32 @llvm.bswap.i32(i32 %a)
-  ret i32 %1
-}
-
-declare i64 @llvm.bswap.i64(i64)
-
-define i64 @bswap_i64(i64 %a) {
-; RV32I-LABEL: bswap_i64:
-; RV32I:       # %bb.0:
-; RV32I-NEXT:    srli a2, a1, 8
-; RV32I-NEXT:    lui a3, 16
-; RV32I-NEXT:    addi a3, a3, -256
-; RV32I-NEXT:    and a2, a2, a3
-; RV32I-NEXT:    srli a4, a1, 24
-; RV32I-NEXT:    or a2, a2, a4
-; RV32I-NEXT:    slli a4, a1, 8
-; RV32I-NEXT:    lui a5, 4080
-; RV32I-NEXT:    and a4, a4, a5
-; RV32I-NEXT:    slli a1, a1, 24
-; RV32I-NEXT:    or a1, a1, a4
-; RV32I-NEXT:    or a2, a1, a2
-; RV32I-NEXT:    srli a1, a0, 8
-; RV32I-NEXT:    and a1, a1, a3
-; RV32I-NEXT:    srli a3, a0, 24
-; RV32I-NEXT:    or a1, a1, a3
-; RV32I-NEXT:    slli a3, a0, 8
-; RV32I-NEXT:    and a3, a3, a5
-; RV32I-NEXT:    slli a0, a0, 24
-; RV32I-NEXT:    or a0, a0, a3
-; RV32I-NEXT:    or a1, a0, a1
-; RV32I-NEXT:    mv a0, a2
-; RV32I-NEXT:    ret
-;
-; RV32B-LABEL: bswap_i64:
-; RV32B:       # %bb.0:
-; RV32B-NEXT:    rev8 a2, a1
-; RV32B-NEXT:    rev8 a1, a0
-; RV32B-NEXT:    mv a0, a2
-; RV32B-NEXT:    ret
-;
-; RV32ZBP-LABEL: bswap_i64:
-; RV32ZBP:       # %bb.0:
-; RV32ZBP-NEXT:    rev8 a2, a1
-; RV32ZBP-NEXT:    rev8 a1, a0
-; RV32ZBP-NEXT:    mv a0, a2
-; RV32ZBP-NEXT:    ret
-  %1 = call i64 @llvm.bswap.i64(i64 %a)
-  ret i64 %1
-}
-
-declare i8 @llvm.bitreverse.i8(i8)
-
-define zeroext i8 @bitreverse_i8(i8 zeroext %a) nounwind {
-; RV32I-LABEL: bitreverse_i8:
-; RV32I:       # %bb.0:
-; RV32I-NEXT:    srli a1, a0, 4
-; RV32I-NEXT:    andi a0, a0, 15
-; RV32I-NEXT:    slli a0, a0, 4
-; RV32I-NEXT:    or a0, a1, a0
-; RV32I-NEXT:    andi a1, a0, 51
-; RV32I-NEXT:    slli a1, a1, 2
-; RV32I-NEXT:    srli a0, a0, 2
-; RV32I-NEXT:    andi a0, a0, 51
-; RV32I-NEXT:    or a0, a0, a1
-; RV32I-NEXT:    andi a1, a0, 85
-; RV32I-NEXT:    slli a1, a1, 1
-; RV32I-NEXT:    srli a0, a0, 1
-; RV32I-NEXT:    andi a0, a0, 85
-; RV32I-NEXT:    or a0, a0, a1
-; RV32I-NEXT:    ret
-;
-; RV32B-LABEL: bitreverse_i8:
-; RV32B:       # %bb.0:
-; RV32B-NEXT:    rev.b a0, a0
-; RV32B-NEXT:    ret
-;
-; RV32ZBP-LABEL: bitreverse_i8:
-; RV32ZBP:       # %bb.0:
-; RV32ZBP-NEXT:    rev.b a0, a0
-; RV32ZBP-NEXT:    ret
-  %1 = tail call i8 @llvm.bitreverse.i8(i8 %a)
-  ret i8 %1
-}
-
-declare i16 @llvm.bitreverse.i16(i16)
-
-define zeroext i16 @bitreverse_i16(i16 zeroext %a) nounwind {
-; RV32I-LABEL: bitreverse_i16:
-; RV32I:       # %bb.0:
-; RV32I-NEXT:    srli a1, a0, 8
-; RV32I-NEXT:    slli a0, a0, 8
-; RV32I-NEXT:    or a0, a0, a1
-; RV32I-NEXT:    srli a1, a0, 4
-; RV32I-NEXT:    lui a2, 1
-; RV32I-NEXT:    addi a2, a2, -241
-; RV32I-NEXT:    and a1, a1, a2
-; RV32I-NEXT:    and a0, a0, a2
-; RV32I-NEXT:    slli a0, a0, 4
-; RV32I-NEXT:    or a0, a1, a0
-; RV32I-NEXT:    srli a1, a0, 2
-; RV32I-NEXT:    lui a2, 3
-; RV32I-NEXT:    addi a2, a2, 819
-; RV32I-NEXT:    and a1, a1, a2
-; RV32I-NEXT:    and a0, a0, a2
-; RV32I-NEXT:    slli a0, a0, 2
-; RV32I-NEXT:    or a0, a1, a0
-; RV32I-NEXT:    srli a1, a0, 1
-; RV32I-NEXT:    lui a2, 5
-; RV32I-NEXT:    addi a2, a2, 1365
-; RV32I-NEXT:    and a1, a1, a2
-; RV32I-NEXT:    and a0, a0, a2
-; RV32I-NEXT:    slli a0, a0, 1
-; RV32I-NEXT:    or a0, a1, a0
-; RV32I-NEXT:    ret
-;
-; RV32B-LABEL: bitreverse_i16:
-; RV32B:       # %bb.0:
-; RV32B-NEXT:    rev.h a0, a0
-; RV32B-NEXT:    ret
-;
-; RV32ZBP-LABEL: bitreverse_i16:
-; RV32ZBP:       # %bb.0:
-; RV32ZBP-NEXT:    rev.h a0, a0
-; RV32ZBP-NEXT:    ret
-  %1 = tail call i16 @llvm.bitreverse.i16(i16 %a)
-  ret i16 %1
-}
-
-declare i32 @llvm.bitreverse.i32(i32)
-
-define i32 @bitreverse_i32(i32 %a) nounwind {
-; RV32I-LABEL: bitreverse_i32:
-; RV32I:       # %bb.0:
-; RV32I-NEXT:    srli a1, a0, 8
-; RV32I-NEXT:    lui a2, 16
-; RV32I-NEXT:    addi a2, a2, -256
-; RV32I-NEXT:    and a1, a1, a2
-; RV32I-NEXT:    srli a2, a0, 24
-; RV32I-NEXT:    or a1, a1, a2
-; RV32I-NEXT:    slli a2, a0, 8
-; RV32I-NEXT:    lui a3, 4080
-; RV32I-NEXT:    and a2, a2, a3
-; RV32I-NEXT:    slli a0, a0, 24
-; RV32I-NEXT:    or a0, a0, a2
-; RV32I-NEXT:    or a0, a0, a1
-; RV32I-NEXT:    srli a1, a0, 4
-; RV32I-NEXT:    lui a2, 61681
-; RV32I-NEXT:    addi a2, a2, -241
-; RV32I-NEXT:    and a1, a1, a2
-; RV32I-NEXT:    and a0, a0, a2
-; RV32I-NEXT:    slli a0, a0, 4
-; RV32I-NEXT:    or a0, a1, a0
-; RV32I-NEXT:    srli a1, a0, 2
-; RV32I-NEXT:    lui a2, 209715
-; RV32I-NEXT:    addi a2, a2, 819
-; RV32I-NEXT:    and a1, a1, a2
-; RV32I-NEXT:    and a0, a0, a2
-; RV32I-NEXT:    slli a0, a0, 2
-; RV32I-NEXT:    or a0, a1, a0
-; RV32I-NEXT:    srli a1, a0, 1
-; RV32I-NEXT:    lui a2, 349525
-; RV32I-NEXT:    addi a2, a2, 1365
-; RV32I-NEXT:    and a1, a1, a2
-; RV32I-NEXT:    and a0, a0, a2
-; RV32I-NEXT:    slli a0, a0, 1
-; RV32I-NEXT:    or a0, a1, a0
-; RV32I-NEXT:    ret
-;
-; RV32B-LABEL: bitreverse_i32:
-; RV32B:       # %bb.0:
-; RV32B-NEXT:    rev a0, a0
-; RV32B-NEXT:    ret
-;
-; RV32ZBP-LABEL: bitreverse_i32:
-; RV32ZBP:       # %bb.0:
-; RV32ZBP-NEXT:    rev a0, a0
-; RV32ZBP-NEXT:    ret
-  %1 = tail call i32 @llvm.bitreverse.i32(i32 %a)
-  ret i32 %1
-}
-
-declare i64 @llvm.bitreverse.i64(i64)
-
-define i64 @bitreverse_i64(i64 %a) nounwind {
-; RV32I-LABEL: bitreverse_i64:
-; RV32I:       # %bb.0:
-; RV32I-NEXT:    srli a2, a1, 8
-; RV32I-NEXT:    lui a3, 16
-; RV32I-NEXT:    addi a6, a3, -256
-; RV32I-NEXT:    and a2, a2, a6
-; RV32I-NEXT:    srli a4, a1, 24
-; RV32I-NEXT:    or a2, a2, a4
-; RV32I-NEXT:    slli a4, a1, 8
-; RV32I-NEXT:    lui a7, 4080
-; RV32I-NEXT:    and a4, a4, a7
-; RV32I-NEXT:    slli a1, a1, 24
-; RV32I-NEXT:    or a1, a1, a4
-; RV32I-NEXT:    or a1, a1, a2
-; RV32I-NEXT:    srli a2, a1, 4
-; RV32I-NEXT:    lui a4, 61681
-; RV32I-NEXT:    addi a4, a4, -241
-; RV32I-NEXT:    and a2, a2, a4
-; RV32I-NEXT:    and a1, a1, a4
-; RV32I-NEXT:    slli a1, a1, 4
-; RV32I-NEXT:    or a1, a2, a1
-; RV32I-NEXT:    srli a2, a1, 2
-; RV32I-NEXT:    lui a3, 209715
-; RV32I-NEXT:    addi a3, a3, 819
-; RV32I-NEXT:    and a2, a2, a3
-; RV32I-NEXT:    and a1, a1, a3
-; RV32I-NEXT:    slli a1, a1, 2
-; RV32I-NEXT:    or a1, a2, a1
-; RV32I-NEXT:    srli a2, a1, 1
-; RV32I-NEXT:    lui a5, 349525
-; RV32I-NEXT:    addi a5, a5, 1365
-; RV32I-NEXT:    and a2, a2, a5
-; RV32I-NEXT:    and a1, a1, a5
-; RV32I-NEXT:    slli a1, a1, 1
-; RV32I-NEXT:    or t0, a2, a1
-; RV32I-NEXT:    srli a1, a0, 8
-; RV32I-NEXT:    and a1, a1, a6
-; RV32I-NEXT:    srli a2, a0, 24
-; RV32I-NEXT:    or a1, a1, a2
-; RV32I-NEXT:    slli a2, a0, 8
-; RV32I-NEXT:    and a2, a2, a7
-; RV32I-NEXT:    slli a0, a0, 24
-; RV32I-NEXT:    or a0, a0, a2
-; RV32I-NEXT:    or a0, a0, a1
-; RV32I-NEXT:    srli a1, a0, 4
-; RV32I-NEXT:    and a1, a1, a4
-; RV32I-NEXT:    and a0, a0, a4
-; RV32I-NEXT:    slli a0, a0, 4
-; RV32I-NEXT:    or a0, a1, a0
-; RV32I-NEXT:    srli a1, a0, 2
-; RV32I-NEXT:    and a1, a1, a3
-; RV32I-NEXT:    and a0, a0, a3
-; RV32I-NEXT:    slli a0, a0, 2
-; RV32I-NEXT:    or a0, a1, a0
-; RV32I-NEXT:    srli a1, a0, 1
-; RV32I-NEXT:    and a1, a1, a5
-; RV32I-NEXT:    and a0, a0, a5
-; RV32I-NEXT:    slli a0, a0, 1
-; RV32I-NEXT:    or a1, a1, a0
-; RV32I-NEXT:    mv a0, t0
-; RV32I-NEXT:    ret
-;
-; RV32B-LABEL: bitreverse_i64:
-; RV32B:       # %bb.0:
-; RV32B-NEXT:    rev a2, a1
-; RV32B-NEXT:    rev a1, a0
-; RV32B-NEXT:    mv a0, a2
-; RV32B-NEXT:    ret
-;
-; RV32ZBP-LABEL: bitreverse_i64:
-; RV32ZBP:       # %bb.0:
-; RV32ZBP-NEXT:    rev a2, a1
-; RV32ZBP-NEXT:    rev a1, a0
-; RV32ZBP-NEXT:    mv a0, a2
-; RV32ZBP-NEXT:    ret
-  %1 = call i64 @llvm.bitreverse.i64(i64 %a)
-  ret i64 %1
-}
-
-define i32 @bswap_rotr_i32(i32 %a) {
-; RV32I-LABEL: bswap_rotr_i32:
-; RV32I:       # %bb.0:
-; RV32I-NEXT:    srli a1, a0, 8
-; RV32I-NEXT:    lui a2, 16
-; RV32I-NEXT:    addi a2, a2, -256
-; RV32I-NEXT:    and a1, a1, a2
-; RV32I-NEXT:    srli a2, a0, 24
-; RV32I-NEXT:    or a1, a1, a2
-; RV32I-NEXT:    slli a2, a0, 8
-; RV32I-NEXT:    lui a3, 4080
-; RV32I-NEXT:    and a2, a2, a3
-; RV32I-NEXT:    slli a0, a0, 24
-; RV32I-NEXT:    or a0, a0, a2
-; RV32I-NEXT:    or a0, a0, a1
-; RV32I-NEXT:    slli a1, a0, 16
-; RV32I-NEXT:    srli a0, a0, 16
-; RV32I-NEXT:    or a0, a0, a1
-; RV32I-NEXT:    ret
-;
-; RV32B-LABEL: bswap_rotr_i32:
-; RV32B:       # %bb.0:
-; RV32B-NEXT:    rev8.h a0, a0
-; RV32B-NEXT:    ret
-;
-; RV32ZBP-LABEL: bswap_rotr_i32:
-; RV32ZBP:       # %bb.0:
-; RV32ZBP-NEXT:    rev8.h a0, a0
-; RV32ZBP-NEXT:    ret
-  %1 = call i32 @llvm.bswap.i32(i32 %a)
-  %2 = call i32 @llvm.fshr.i32(i32 %1, i32 %1, i32 16)
-  ret i32 %2
-}
-
-define i32 @bswap_rotl_i32(i32 %a) {
-; RV32I-LABEL: bswap_rotl_i32:
-; RV32I:       # %bb.0:
-; RV32I-NEXT:    srli a1, a0, 8
-; RV32I-NEXT:    lui a2, 16
-; RV32I-NEXT:    addi a2, a2, -256
-; RV32I-NEXT:    and a1, a1, a2
-; RV32I-NEXT:    srli a2, a0, 24
-; RV32I-NEXT:    or a1, a1, a2
-; RV32I-NEXT:    slli a2, a0, 8
-; RV32I-NEXT:    lui a3, 4080
-; RV32I-NEXT:    and a2, a2, a3
-; RV32I-NEXT:    slli a0, a0, 24
-; RV32I-NEXT:    or a0, a0, a2
-; RV32I-NEXT:    or a0, a0, a1
-; RV32I-NEXT:    srli a1, a0, 16
-; RV32I-NEXT:    slli a0, a0, 16
-; RV32I-NEXT:    or a0, a0, a1
-; RV32I-NEXT:    ret
-;
-; RV32B-LABEL: bswap_rotl_i32:
-; RV32B:       # %bb.0:
-; RV32B-NEXT:    rev8.h a0, a0
-; RV32B-NEXT:    ret
-;
-; RV32ZBP-LABEL: bswap_rotl_i32:
-; RV32ZBP:       # %bb.0:
-; RV32ZBP-NEXT:    rev8.h a0, a0
-; RV32ZBP-NEXT:    ret
-  %1 = call i32 @llvm.bswap.i32(i32 %a)
-  %2 = call i32 @llvm.fshl.i32(i32 %1, i32 %1, i32 16)
-  ret i32 %2
-}
-
-define i32 @bitreverse_bswap_i32(i32 %a) {
-; RV32I-LABEL: bitreverse_bswap_i32:
-; RV32I:       # %bb.0:
-; RV32I-NEXT:    srli a1, a0, 8
-; RV32I-NEXT:    lui a2, 16
-; RV32I-NEXT:    addi a2, a2, -256
-; RV32I-NEXT:    and a1, a1, a2
-; RV32I-NEXT:    srli a3, a0, 24
-; RV32I-NEXT:    or a1, a1, a3
-; RV32I-NEXT:    slli a3, a0, 8
-; RV32I-NEXT:    lui a4, 4080
-; RV32I-NEXT:    and a3, a3, a4
-; RV32I-NEXT:    slli a0, a0, 24
-; RV32I-NEXT:    or a0, a0, a3
-; RV32I-NEXT:    or a0, a0, a1
-; RV32I-NEXT:    srli a1, a0, 4
-; RV32I-NEXT:    lui a3, 61681
-; RV32I-NEXT:    addi a3, a3, -241
-; RV32I-NEXT:    and a1, a1, a3
-; RV32I-NEXT:    and a0, a0, a3
-; RV32I-NEXT:    slli a0, a0, 4
-; RV32I-NEXT:    or a0, a1, a0
-; RV32I-NEXT:    srli a1, a0, 2
-; RV32I-NEXT:    lui a3, 209715
-; RV32I-NEXT:    addi a3, a3, 819
-; RV32I-NEXT:    and a1, a1, a3
-; RV32I-NEXT:    and a0, a0, a3
-; RV32I-NEXT:    slli a0, a0, 2
-; RV32I-NEXT:    or a0, a1, a0
-; RV32I-NEXT:    srli a1, a0, 1
-; RV32I-NEXT:    lui a3, 349525
-; RV32I-NEXT:    addi a3, a3, 1365
-; RV32I-NEXT:    and a1, a1, a3
-; RV32I-NEXT:    and a0, a0, a3
-; RV32I-NEXT:    slli a0, a0, 1
-; RV32I-NEXT:    or a0, a1, a0
-; RV32I-NEXT:    srli a1, a0, 8
-; RV32I-NEXT:    and a1, a1, a2
-; RV32I-NEXT:    srli a2, a0, 24
-; RV32I-NEXT:    or a1, a1, a2
-; RV32I-NEXT:    slli a2, a0, 8
-; RV32I-NEXT:    and a2, a2, a4
-; RV32I-NEXT:    slli a0, a0, 24
-; RV32I-NEXT:    or a0, a0, a2
-; RV32I-NEXT:    or a0, a0, a1
-; RV32I-NEXT:    ret
-;
-; RV32B-LABEL: bitreverse_bswap_i32:
-; RV32B:       # %bb.0:
-; RV32B-NEXT:    rev.b a0, a0
-; RV32B-NEXT:    ret
-;
-; RV32ZBP-LABEL: bitreverse_bswap_i32:
-; RV32ZBP:       # %bb.0:
-; RV32ZBP-NEXT:    rev.b a0, a0
-; RV32ZBP-NEXT:    ret
-  %1 = call i32 @llvm.bitreverse.i32(i32 %a)
-  %2 = call i32 @llvm.bswap.i32(i32 %1)
-  ret i32 %2
-}
-
-define i64 @bitreverse_bswap_i64(i64 %a) {
-; RV32I-LABEL: bitreverse_bswap_i64:
-; RV32I:       # %bb.0:
-; RV32I-NEXT:    srli a3, a1, 8
-; RV32I-NEXT:    lui a2, 16
-; RV32I-NEXT:    addi a6, a2, -256
-; RV32I-NEXT:    and a3, a3, a6
-; RV32I-NEXT:    srli a4, a1, 24
-; RV32I-NEXT:    or a4, a3, a4
-; RV32I-NEXT:    slli a5, a1, 8
-; RV32I-NEXT:    lui a7, 4080
-; RV32I-NEXT:    and a5, a5, a7
-; RV32I-NEXT:    slli a1, a1, 24
-; RV32I-NEXT:    or a1, a1, a5
-; RV32I-NEXT:    or a1, a1, a4
-; RV32I-NEXT:    srli a4, a1, 4
-; RV32I-NEXT:    lui a5, 61681
-; RV32I-NEXT:    addi t0, a5, -241
-; RV32I-NEXT:    and a4, a4, t0
-; RV32I-NEXT:    and a1, a1, t0
-; RV32I-NEXT:    slli a1, a1, 4
-; RV32I-NEXT:    or a1, a4, a1
-; RV32I-NEXT:    srli a4, a1, 2
-; RV32I-NEXT:    lui a2, 209715
-; RV32I-NEXT:    addi a2, a2, 819
-; RV32I-NEXT:    and a4, a4, a2
-; RV32I-NEXT:    and a1, a1, a2
-; RV32I-NEXT:    slli a1, a1, 2
-; RV32I-NEXT:    or a1, a4, a1
-; RV32I-NEXT:    srli a4, a1, 1
-; RV32I-NEXT:    lui a3, 349525
-; RV32I-NEXT:    addi a3, a3, 1365
-; RV32I-NEXT:    and a4, a4, a3
-; RV32I-NEXT:    and a1, a1, a3
-; RV32I-NEXT:    slli a1, a1, 1
-; RV32I-NEXT:    or a1, a4, a1
-; RV32I-NEXT:    srli a4, a0, 8
-; RV32I-NEXT:    and a4, a4, a6
-; RV32I-NEXT:    srli a5, a0, 24
-; RV32I-NEXT:    or a4, a4, a5
-; RV32I-NEXT:    slli a5, a0, 8
-; RV32I-NEXT:    and a5, a5, a7
-; RV32I-NEXT:    slli a0, a0, 24
-; RV32I-NEXT:    or a0, a0, a5
-; RV32I-NEXT:    or a0, a0, a4
-; RV32I-NEXT:    srli a4, a0, 4
-; RV32I-NEXT:    and a4, a4, t0
-; RV32I-NEXT:    and a0, a0, t0
-; RV32I-NEXT:    slli a0, a0, 4
-; RV32I-NEXT:    or a0, a4, a0
-; RV32I-NEXT:    srli a4, a0, 2
-; RV32I-NEXT:    and a4, a4, a2
-; RV32I-NEXT:    and a0, a0, a2
-; RV32I-NEXT:    slli a0, a0, 2
-; RV32I-NEXT:    or a0, a4, a0
-; RV32I-NEXT:    srli a2, a0, 1
-; RV32I-NEXT:    and a2, a2, a3
-; RV32I-NEXT:    and a0, a0, a3
-; RV32I-NEXT:    slli a0, a0, 1
-; RV32I-NEXT:    or a0, a2, a0
-; RV32I-NEXT:    srli a2, a0, 8
-; RV32I-NEXT:    and a2, a2, a6
-; RV32I-NEXT:    srli a3, a0, 24
-; RV32I-NEXT:    or a2, a2, a3
-; RV32I-NEXT:    slli a3, a0, 8
-; RV32I-NEXT:    and a3, a3, a7
-; RV32I-NEXT:    slli a0, a0, 24
-; RV32I-NEXT:    or a0, a0, a3
-; RV32I-NEXT:    or a0, a0, a2
-; RV32I-NEXT:    srli a2, a1, 8
-; RV32I-NEXT:    and a2, a2, a6
-; RV32I-NEXT:    srli a3, a1, 24
-; RV32I-NEXT:    or a2, a2, a3
-; RV32I-NEXT:    slli a3, a1, 8
-; RV32I-NEXT:    and a3, a3, a7
-; RV32I-NEXT:    slli a1, a1, 24
-; RV32I-NEXT:    or a1, a1, a3
-; RV32I-NEXT:    or a1, a1, a2
-; RV32I-NEXT:    ret
-;
-; RV32B-LABEL: bitreverse_bswap_i64:
-; RV32B:       # %bb.0:
-; RV32B-NEXT:    rev.b a0, a0
-; RV32B-NEXT:    rev.b a1, a1
-; RV32B-NEXT:    ret
-;
-; RV32ZBP-LABEL: bitreverse_bswap_i64:
-; RV32ZBP:       # %bb.0:
-; RV32ZBP-NEXT:    rev.b a0, a0
-; RV32ZBP-NEXT:    rev.b a1, a1
-; RV32ZBP-NEXT:    ret
-  %1 = call i64 @llvm.bitreverse.i64(i64 %a)
-  %2 = call i64 @llvm.bswap.i64(i64 %1)
-  ret i64 %2
-}
-
-define i32 @shfl1_i32(i32 %a, i32 %b) nounwind {
-; RV32I-LABEL: shfl1_i32:
-; RV32I:       # %bb.0:
-; RV32I-NEXT:    lui a1, 629146
-; RV32I-NEXT:    addi a1, a1, -1639
-; RV32I-NEXT:    and a1, a0, a1
-; RV32I-NEXT:    slli a2, a0, 1
-; RV32I-NEXT:    lui a3, 279620
-; RV32I-NEXT:    addi a3, a3, 1092
-; RV32I-NEXT:    and a2, a2, a3
-; RV32I-NEXT:    or a1, a2, a1
-; RV32I-NEXT:    srli a0, a0, 1
-; RV32I-NEXT:    lui a2, 139810
-; RV32I-NEXT:    addi a2, a2, 546
-; RV32I-NEXT:    and a0, a0, a2
-; RV32I-NEXT:    or a0, a1, a0
-; RV32I-NEXT:    ret
-;
-; RV32B-LABEL: shfl1_i32:
-; RV32B:       # %bb.0:
-; RV32B-NEXT:    zip.n a0, a0
-; RV32B-NEXT:    ret
-;
-; RV32ZBP-LABEL: shfl1_i32:
-; RV32ZBP:       # %bb.0:
-; RV32ZBP-NEXT:    zip.n a0, a0
-; RV32ZBP-NEXT:    ret
-  %and = and i32 %a, -1717986919
-  %shl = shl i32 %a, 1
-  %and1 = and i32 %shl, 1145324612
-  %or = or i32 %and1, %and
-  %shr = lshr i32 %a, 1
-  %and2 = and i32 %shr, 572662306
-  %or3 = or i32 %or, %and2
-  ret i32 %or3
-}
-
-define i64 @shfl1_i64(i64 %a, i64 %b) nounwind {
-; RV32I-LABEL: shfl1_i64:
-; RV32I:       # %bb.0:
-; RV32I-NEXT:    lui a2, 629146
-; RV32I-NEXT:    addi a2, a2, -1639
-; RV32I-NEXT:    and a6, a0, a2
-; RV32I-NEXT:    and a2, a1, a2
-; RV32I-NEXT:    slli a4, a1, 1
-; RV32I-NEXT:    slli a5, a0, 1
-; RV32I-NEXT:    lui a3, 279620
-; RV32I-NEXT:    addi a3, a3, 1092
-; RV32I-NEXT:    and a5, a5, a3
-; RV32I-NEXT:    and a3, a4, a3
-; RV32I-NEXT:    or a2, a2, a3
-; RV32I-NEXT:    or a3, a6, a5
-; RV32I-NEXT:    srli a0, a0, 1
-; RV32I-NEXT:    srli a1, a1, 1
-; RV32I-NEXT:    lui a4, 139810
-; RV32I-NEXT:    addi a4, a4, 546
-; RV32I-NEXT:    and a1, a1, a4
-; RV32I-NEXT:    and a0, a0, a4
-; RV32I-NEXT:    or a0, a3, a0
-; RV32I-NEXT:    or a1, a2, a1
-; RV32I-NEXT:    ret
-;
-; RV32B-LABEL: shfl1_i64:
-; RV32B:       # %bb.0:
-; RV32B-NEXT:    zip.n a0, a0
-; RV32B-NEXT:    zip.n a1, a1
-; RV32B-NEXT:    ret
-;
-; RV32ZBP-LABEL: shfl1_i64:
-; RV32ZBP:       # %bb.0:
-; RV32ZBP-NEXT:    zip.n a0, a0
-; RV32ZBP-NEXT:    zip.n a1, a1
-; RV32ZBP-NEXT:    ret
-  %and = and i64 %a, -7378697629483820647
-  %shl = shl i64 %a, 1
-  %and1 = and i64 %shl, 4919131752989213764
-  %or = or i64 %and, %and1
-  %shr = lshr i64 %a, 1
-  %and2 = and i64 %shr, 2459565876494606882
-  %or3 = or i64 %or, %and2
-  ret i64 %or3
-}
-
-define i32 @shfl2_i32(i32 %a, i32 %b) nounwind {
-; RV32I-LABEL: shfl2_i32:
-; RV32I:       # %bb.0:
-; RV32I-NEXT:    lui a1, 801852
-; RV32I-NEXT:    addi a1, a1, 963
-; RV32I-NEXT:    and a1, a0, a1
-; RV32I-NEXT:    slli a2, a0, 2
-; RV32I-NEXT:    lui a3, 197379
-; RV32I-NEXT:    addi a3, a3, 48
-; RV32I-NEXT:    and a2, a2, a3
-; RV32I-NEXT:    or a1, a2, a1
-; RV32I-NEXT:    srli a0, a0, 2
-; RV32I-NEXT:    lui a2, 49345
-; RV32I-NEXT:    addi a2, a2, -1012
-; RV32I-NEXT:    and a0, a0, a2
-; RV32I-NEXT:    or a0, a0, a1
-; RV32I-NEXT:    ret
-;
-; RV32B-LABEL: shfl2_i32:
-; RV32B:       # %bb.0:
-; RV32B-NEXT:    zip2.b a0, a0
-; RV32B-NEXT:    ret
-;
-; RV32ZBP-LABEL: shfl2_i32:
-; RV32ZBP:       # %bb.0:
-; RV32ZBP-NEXT:    zip2.b a0, a0
-; RV32ZBP-NEXT:    ret
-  %and = and i32 %a, -1010580541
-  %shl = shl i32 %a, 2
-  %and1 = and i32 %shl, 808464432
-  %or = or i32 %and1, %and
-  %shr = lshr i32 %a, 2
-  %and2 = and i32 %shr, 202116108
-  %or3 = or i32 %and2, %or
-  ret i32 %or3
-}
-
-define i64 @shfl2_i64(i64 %a, i64 %b) nounwind {
-; RV32I-LABEL: shfl2_i64:
-; RV32I:       # %bb.0:
-; RV32I-NEXT:    lui a2, 801852
-; RV32I-NEXT:    addi a2, a2, 963
-; RV32I-NEXT:    and a6, a0, a2
-; RV32I-NEXT:    and a2, a1, a2
-; RV32I-NEXT:    slli a4, a1, 2
-; RV32I-NEXT:    slli a5, a0, 2
-; RV32I-NEXT:    lui a3, 197379
-; RV32I-NEXT:    addi a3, a3, 48
-; RV32I-NEXT:    and a5, a5, a3
-; RV32I-NEXT:    and a3, a4, a3
-; RV32I-NEXT:    or a2, a2, a3
-; RV32I-NEXT:    or a3, a6, a5
-; RV32I-NEXT:    srli a0, a0, 2
-; RV32I-NEXT:    srli a1, a1, 2
-; RV32I-NEXT:    lui a4, 49345
-; RV32I-NEXT:    addi a4, a4, -1012
-; RV32I-NEXT:    and a1, a1, a4
-; RV32I-NEXT:    and a0, a0, a4
-; RV32I-NEXT:    or a0, a0, a3
-; RV32I-NEXT:    or a1, a1, a2
-; RV32I-NEXT:    ret
-;
-; RV32B-LABEL: shfl2_i64:
-; RV32B:       # %bb.0:
-; RV32B-NEXT:    zip2.b a0, a0
-; RV32B-NEXT:    zip2.b a1, a1
-; RV32B-NEXT:    ret
-;
-; RV32ZBP-LABEL: shfl2_i64:
-; RV32ZBP:       # %bb.0:
-; RV32ZBP-NEXT:    zip2.b a0, a0
-; RV32ZBP-NEXT:    zip2.b a1, a1
-; RV32ZBP-NEXT:    ret
-  %and = and i64 %a, -4340410370284600381
-  %shl = shl i64 %a, 2
-  %and1 = and i64 %shl, 3472328296227680304
-  %or = or i64 %and, %and1
-  %shr = lshr i64 %a, 2
-  %and2 = and i64 %shr, 868082074056920076
-  %or3 = or i64 %and2, %or
-  ret i64 %or3
-}
-
-define i32 @shfl4_i32(i32 %a, i32 %b) nounwind {
-; RV32I-LABEL: shfl4_i32:
-; RV32I:       # %bb.0:
-; RV32I-NEXT:    lui a1, 983295
-; RV32I-NEXT:    addi a1, a1, 15
-; RV32I-NEXT:    and a1, a0, a1
-; RV32I-NEXT:    slli a2, a0, 4
-; RV32I-NEXT:    lui a3, 61441
-; RV32I-NEXT:    addi a3, a3, -256
-; RV32I-NEXT:    and a2, a2, a3
-; RV32I-NEXT:    srli a0, a0, 4
-; RV32I-NEXT:    lui a3, 3840
-; RV32I-NEXT:    addi a3, a3, 240
-; RV32I-NEXT:    and a0, a0, a3
-; RV32I-NEXT:    or a0, a0, a1
-; RV32I-NEXT:    or a0, a0, a2
-; RV32I-NEXT:    ret
-;
-; RV32B-LABEL: shfl4_i32:
-; RV32B:       # %bb.0:
-; RV32B-NEXT:    zip4.h a0, a0
-; RV32B-NEXT:    ret
-;
-; RV32ZBP-LABEL: shfl4_i32:
-; RV32ZBP:       # %bb.0:
-; RV32ZBP-NEXT:    zip4.h a0, a0
-; RV32ZBP-NEXT:    ret
-  %and = and i32 %a, -267390961
-  %shl = shl i32 %a, 4
-  %and1 = and i32 %shl, 251662080
-  %shr = lshr i32 %a, 4
-  %and2 = and i32 %shr, 15728880
-  %or = or i32 %and2, %and
-  %or3 = or i32 %or, %and1
-  ret i32 %or3
-}
-
-define i64 @shfl4_i64(i64 %a, i64 %b) nounwind {
-; RV32I-LABEL: shfl4_i64:
-; RV32I:       # %bb.0:
-; RV32I-NEXT:    lui a2, 983295
-; RV32I-NEXT:    addi a2, a2, 15
-; RV32I-NEXT:    and a6, a1, a2
-; RV32I-NEXT:    and a2, a0, a2
-; RV32I-NEXT:    slli a4, a1, 4
-; RV32I-NEXT:    slli a5, a0, 4
-; RV32I-NEXT:    lui a3, 61441
-; RV32I-NEXT:    addi a3, a3, -256
-; RV32I-NEXT:    and a5, a5, a3
-; RV32I-NEXT:    and a3, a4, a3
-; RV32I-NEXT:    srli a1, a1, 4
-; RV32I-NEXT:    srli a0, a0, 4
-; RV32I-NEXT:    lui a4, 3840
-; RV32I-NEXT:    addi a4, a4, 240
-; RV32I-NEXT:    and a0, a0, a4
-; RV32I-NEXT:    and a1, a1, a4
-; RV32I-NEXT:    or a1, a3, a1
-; RV32I-NEXT:    or a0, a5, a0
-; RV32I-NEXT:    or a0, a0, a2
-; RV32I-NEXT:    or a1, a1, a6
-; RV32I-NEXT:    ret
-;
-; RV32B-LABEL: shfl4_i64:
-; RV32B:       # %bb.0:
-; RV32B-NEXT:    zip4.h a0, a0
-; RV32B-NEXT:    zip4.h a1, a1
-; RV32B-NEXT:    ret
-;
-; RV32ZBP-LABEL: shfl4_i64:
-; RV32ZBP:       # %bb.0:
-; RV32ZBP-NEXT:    zip4.h a0, a0
-; RV32ZBP-NEXT:    zip4.h a1, a1
-; RV32ZBP-NEXT:    ret
-  %and = and i64 %a, -1148435428713435121
-  %shl = shl i64 %a, 4
-  %and1 = and i64 %shl, 1080880403494997760
-  %shr = lshr i64 %a, 4
-  %and2 = and i64 %shr, 67555025218437360
-  %or = or i64 %and1, %and2
-  %or3 = or i64 %or, %and
-  ret i64 %or3
-}
-
-define i32 @shfl8_i32(i32 %a, i32 %b) nounwind {
-; RV32I-LABEL: shfl8_i32:
-; RV32I:       # %bb.0:
-; RV32I-NEXT:    lui a1, 1044480
-; RV32I-NEXT:    addi a1, a1, 255
-; RV32I-NEXT:    and a1, a0, a1
-; RV32I-NEXT:    slli a2, a0, 8
-; RV32I-NEXT:    lui a3, 4080
-; RV32I-NEXT:    and a2, a2, a3
-; RV32I-NEXT:    srli a0, a0, 8
-; RV32I-NEXT:    lui a3, 16
-; RV32I-NEXT:    addi a3, a3, -256
-; RV32I-NEXT:    and a0, a0, a3
-; RV32I-NEXT:    or a0, a1, a0
-; RV32I-NEXT:    or a0, a0, a2
-; RV32I-NEXT:    ret
-;
-; RV32B-LABEL: shfl8_i32:
-; RV32B:       # %bb.0:
-; RV32B-NEXT:    zip8 a0, a0
-; RV32B-NEXT:    ret
-;
-; RV32ZBP-LABEL: shfl8_i32:
-; RV32ZBP:       # %bb.0:
-; RV32ZBP-NEXT:    zip8 a0, a0
-; RV32ZBP-NEXT:    ret
-  %and = and i32 %a, -16776961
-  %shl = shl i32 %a, 8
-  %and1 = and i32 %shl, 16711680
-  %shr = lshr i32 %a, 8
-  %and2 = and i32 %shr, 65280
-  %or = or i32 %and, %and2
-  %or3 = or i32 %or, %and1
-  ret i32 %or3
-}
-
-define i64 @shfl8_i64(i64 %a, i64 %b) nounwind {
-; RV32I-LABEL: shfl8_i64:
-; RV32I:       # %bb.0:
-; RV32I-NEXT:    lui a2, 1044480
-; RV32I-NEXT:    addi a2, a2, 255
-; RV32I-NEXT:    and a6, a0, a2
-; RV32I-NEXT:    and a2, a1, a2
-; RV32I-NEXT:    slli a4, a0, 8
-; RV32I-NEXT:    slli a5, a1, 8
-; RV32I-NEXT:    lui a3, 4080
-; RV32I-NEXT:    and a5, a5, a3
-; RV32I-NEXT:    and a3, a4, a3
-; RV32I-NEXT:    srli a1, a1, 8
-; RV32I-NEXT:    srli a0, a0, 8
-; RV32I-NEXT:    lui a4, 16
-; RV32I-NEXT:    addi a4, a4, -256
-; RV32I-NEXT:    and a0, a0, a4
-; RV32I-NEXT:    and a1, a1, a4
-; RV32I-NEXT:    or a1, a1, a2
-; RV32I-NEXT:    or a0, a0, a6
-; RV32I-NEXT:    or a0, a3, a0
-; RV32I-NEXT:    or a1, a5, a1
-; RV32I-NEXT:    ret
-;
-; RV32B-LABEL: shfl8_i64:
-; RV32B:       # %bb.0:
-; RV32B-NEXT:    zip8 a0, a0
-; RV32B-NEXT:    zip8 a1, a1
-; RV32B-NEXT:    ret
-;
-; RV32ZBP-LABEL: shfl8_i64:
-; RV32ZBP:       # %bb.0:
-; RV32ZBP-NEXT:    zip8 a0, a0
-; RV32ZBP-NEXT:    zip8 a1, a1
-; RV32ZBP-NEXT:    ret
-  %and = and i64 %a, -72056494543077121
-  %shl = shl i64 %a, 8
-  %and1 = and i64 %shl, 71776119077928960
-  %shr = lshr i64 %a, 8
-  %and2 = and i64 %shr, 280375465148160
-  %or = or i64 %and2, %and
-  %or3 = or i64 %and1, %or
-  ret i64 %or3
-}
-
-define i32 @pack_i32(i32 %a, i32 %b) nounwind {
-; RV32I-LABEL: pack_i32:
-; RV32I:       # %bb.0:
-; RV32I-NEXT:    lui a2, 16
-; RV32I-NEXT:    addi a2, a2, -1
-; RV32I-NEXT:    and a0, a0, a2
-; RV32I-NEXT:    slli a1, a1, 16
-; RV32I-NEXT:    or a0, a1, a0
-; RV32I-NEXT:    ret
-;
-; RV32B-LABEL: pack_i32:
-; RV32B:       # %bb.0:
-; RV32B-NEXT:    pack a0, a0, a1
-; RV32B-NEXT:    ret
-;
-; RV32ZBP-LABEL: pack_i32:
-; RV32ZBP:       # %bb.0:
-; RV32ZBP-NEXT:    pack a0, a0, a1
-; RV32ZBP-NEXT:    ret
-  %shl = and i32 %a, 65535
-  %shl1 = shl i32 %b, 16
-  %or = or i32 %shl1, %shl
-  ret i32 %or
-}
-
-; As we are not matching directly i64 code patterns on RV32 some i64 patterns
-; don't have yet any matching bit manipulation instructions on RV32.
-; This test is presented here in case future expansions of the experimental-b
-; extension introduce instructions suitable for this pattern.
-
-define i64 @pack_i64(i64 %a, i64 %b) nounwind {
-; RV32I-LABEL: pack_i64:
-; RV32I:       # %bb.0:
-; RV32I-NEXT:    mv a1, a2
-; RV32I-NEXT:    ret
-;
-; RV32B-LABEL: pack_i64:
-; RV32B:       # %bb.0:
-; RV32B-NEXT:    mv a1, a2
-; RV32B-NEXT:    ret
-;
-; RV32ZBP-LABEL: pack_i64:
-; RV32ZBP:       # %bb.0:
-; RV32ZBP-NEXT:    mv a1, a2
-; RV32ZBP-NEXT:    ret
-  %shl = and i64 %a, 4294967295
-  %shl1 = shl i64 %b, 32
-  %or = or i64 %shl1, %shl
-  ret i64 %or
-}
-
-define i32 @packu_i32(i32 %a, i32 %b) nounwind {
-; RV32I-LABEL: packu_i32:
-; RV32I:       # %bb.0:
-; RV32I-NEXT:    srli a0, a0, 16
-; RV32I-NEXT:    lui a2, 1048560
-; RV32I-NEXT:    and a1, a1, a2
-; RV32I-NEXT:    or a0, a1, a0
-; RV32I-NEXT:    ret
-;
-; RV32B-LABEL: packu_i32:
-; RV32B:       # %bb.0:
-; RV32B-NEXT:    packu a0, a0, a1
-; RV32B-NEXT:    ret
-;
-; RV32ZBP-LABEL: packu_i32:
-; RV32ZBP:       # %bb.0:
-; RV32ZBP-NEXT:    packu a0, a0, a1
-; RV32ZBP-NEXT:    ret
-  %shr = lshr i32 %a, 16
-  %shr1 = and i32 %b, -65536
-  %or = or i32 %shr1, %shr
-  ret i32 %or
-}
-
-; As we are not matching directly i64 code patterns on RV32 some i64 patterns
-; don't have yet any matching bit manipulation instructions on RV32.
-; This test is presented here in case future expansions of the experimental-b
-; extension introduce instructions suitable for this pattern.
-
-define i64 @packu_i64(i64 %a, i64 %b) nounwind {
-; RV32I-LABEL: packu_i64:
-; RV32I:       # %bb.0:
-; RV32I-NEXT:    mv a0, a1
-; RV32I-NEXT:    mv a1, a3
-; RV32I-NEXT:    ret
-;
-; RV32B-LABEL: packu_i64:
-; RV32B:       # %bb.0:
-; RV32B-NEXT:    mv a0, a1
-; RV32B-NEXT:    mv a1, a3
-; RV32B-NEXT:    ret
-;
-; RV32ZBP-LABEL: packu_i64:
-; RV32ZBP:       # %bb.0:
-; RV32ZBP-NEXT:    mv a0, a1
-; RV32ZBP-NEXT:    mv a1, a3
-; RV32ZBP-NEXT:    ret
-  %shr = lshr i64 %a, 32
-  %shr1 = and i64 %b, -4294967296
-  %or = or i64 %shr1, %shr
-  ret i64 %or
-}
-
-define i32 @packh_i32(i32 %a, i32 %b) nounwind {
-; RV32I-LABEL: packh_i32:
-; RV32I:       # %bb.0:
-; RV32I-NEXT:    andi a0, a0, 255
-; RV32I-NEXT:    slli a1, a1, 24
-; RV32I-NEXT:    srli a1, a1, 16
-; RV32I-NEXT:    or a0, a1, a0
-; RV32I-NEXT:    ret
-;
-; RV32B-LABEL: packh_i32:
-; RV32B:       # %bb.0:
-; RV32B-NEXT:    packh a0, a0, a1
-; RV32B-NEXT:    ret
-;
-; RV32ZBP-LABEL: packh_i32:
-; RV32ZBP:       # %bb.0:
-; RV32ZBP-NEXT:    packh a0, a0, a1
-; RV32ZBP-NEXT:    ret
-  %and = and i32 %a, 255
-  %and1 = shl i32 %b, 8
-  %shl = and i32 %and1, 65280
-  %or = or i32 %shl, %and
-  ret i32 %or
-}
-
-define i64 @packh_i64(i64 %a, i64 %b) nounwind {
-; RV32I-LABEL: packh_i64:
-; RV32I:       # %bb.0:
-; RV32I-NEXT:    andi a0, a0, 255
-; RV32I-NEXT:    slli a1, a2, 24
-; RV32I-NEXT:    srli a1, a1, 16
-; RV32I-NEXT:    or a0, a1, a0
-; RV32I-NEXT:    mv a1, zero
-; RV32I-NEXT:    ret
-;
-; RV32B-LABEL: packh_i64:
-; RV32B:       # %bb.0:
-; RV32B-NEXT:    packh a0, a0, a2
-; RV32B-NEXT:    mv a1, zero
-; RV32B-NEXT:    ret
-;
-; RV32ZBP-LABEL: packh_i64:
-; RV32ZBP:       # %bb.0:
-; RV32ZBP-NEXT:    packh a0, a0, a2
-; RV32ZBP-NEXT:    mv a1, zero
-; RV32ZBP-NEXT:    ret
-  %and = and i64 %a, 255
-  %and1 = shl i64 %b, 8
-  %shl = and i64 %and1, 65280
-  %or = or i64 %shl, %and
-  ret i64 %or
-}
-
-define i32 @zexth_i32(i32 %a) nounwind {
-; RV32I-LABEL: zexth_i32:
-; RV32I:       # %bb.0:
-; RV32I-NEXT:    lui a1, 16
-; RV32I-NEXT:    addi a1, a1, -1
-; RV32I-NEXT:    and a0, a0, a1
-; RV32I-NEXT:    ret
-;
-; RV32B-LABEL: zexth_i32:
-; RV32B:       # %bb.0:
-; RV32B-NEXT:    zext.h a0, a0
-; RV32B-NEXT:    ret
-;
-; RV32ZBP-LABEL: zexth_i32:
-; RV32ZBP:       # %bb.0:
-; RV32ZBP-NEXT:    zext.h a0, a0
-; RV32ZBP-NEXT:    ret
-  %and = and i32 %a, 65535
-  ret i32 %and
-}
-
-define i64 @zexth_i64(i64 %a) nounwind {
-; RV32I-LABEL: zexth_i64:
-; RV32I:       # %bb.0:
-; RV32I-NEXT:    lui a1, 16
-; RV32I-NEXT:    addi a1, a1, -1
-; RV32I-NEXT:    and a0, a0, a1
-; RV32I-NEXT:    mv a1, zero
-; RV32I-NEXT:    ret
-;
-; RV32B-LABEL: zexth_i64:
-; RV32B:       # %bb.0:
-; RV32B-NEXT:    zext.h a0, a0
-; RV32B-NEXT:    mv a1, zero
-; RV32B-NEXT:    ret
-;
-; RV32ZBP-LABEL: zexth_i64:
-; RV32ZBP:       # %bb.0:
-; RV32ZBP-NEXT:    zext.h a0, a0
-; RV32ZBP-NEXT:    mv a1, zero
-; RV32ZBP-NEXT:    ret
-  %and = and i64 %a, 65535
-  ret i64 %and
-}
diff --git a/llvm/test/CodeGen/RISCV/rv32zbr.ll b/llvm/test/CodeGen/RISCV/rv32zbr.ll
deleted file mode 100644
index d4f88990a01e..000000000000
--- a/llvm/test/CodeGen/RISCV/rv32zbr.ll
+++ /dev/null
@@ -1,69 +0,0 @@
-; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
-; RUN: llc -mtriple=riscv32 -mattr=experimental-zbr -verify-machineinstrs < %s \
-; RUN:   | FileCheck %s -check-prefix=RV32ZBR
-
-declare i32 @llvm.riscv.crc32.b.i32(i32)
-
-define i32 @crc32b(i32 %a) nounwind {
-; RV32ZBR-LABEL: crc32b:
-; RV32ZBR:       # %bb.0:
-; RV32ZBR-NEXT:    crc32.b a0, a0
-; RV32ZBR-NEXT:    ret
-  %tmp = call i32 @llvm.riscv.crc32.b.i32(i32 %a)
- ret i32 %tmp
-}
-
-declare i32 @llvm.riscv.crc32.h.i32(i32)
-
-define i32 @crc32h(i32 %a) nounwind {
-; RV32ZBR-LABEL: crc32h:
-; RV32ZBR:       # %bb.0:
-; RV32ZBR-NEXT:    crc32.h a0, a0
-; RV32ZBR-NEXT:    ret
-  %tmp = call i32 @llvm.riscv.crc32.h.i32(i32 %a)
- ret i32 %tmp
-}
-
-declare i32 @llvm.riscv.crc32.w.i32(i32)
-
-define i32 @crc32w(i32 %a) nounwind {
-; RV32ZBR-LABEL: crc32w:
-; RV32ZBR:       # %bb.0:
-; RV32ZBR-NEXT:    crc32.w a0, a0
-; RV32ZBR-NEXT:    ret
-  %tmp = call i32 @llvm.riscv.crc32.w.i32(i32 %a)
- ret i32 %tmp
-}
-
-declare i32 @llvm.riscv.crc32c.b.i32(i32)
-
-define i32 @crc32cb(i32 %a) nounwind {
-; RV32ZBR-LABEL: crc32cb:
-; RV32ZBR:       # %bb.0:
-; RV32ZBR-NEXT:    crc32c.b a0, a0
-; RV32ZBR-NEXT:    ret
-  %tmp = call i32 @llvm.riscv.crc32c.b.i32(i32 %a)
- ret i32 %tmp
-}
-
-declare i32 @llvm.riscv.crc32c.h.i32(i32)
-
-define i32 @crc32ch(i32 %a) nounwind {
-; RV32ZBR-LABEL: crc32ch:
-; RV32ZBR:       # %bb.0:
-; RV32ZBR-NEXT:    crc32c.h a0, a0
-; RV32ZBR-NEXT:    ret
-  %tmp = call i32 @llvm.riscv.crc32c.h.i32(i32 %a)
- ret i32 %tmp
-}
-
-declare i32 @llvm.riscv.crc32c.w.i32(i32)
-
-define i32 @crc32cw(i32 %a) nounwind {
-; RV32ZBR-LABEL: crc32cw:
-; RV32ZBR:       # %bb.0:
-; RV32ZBR-NEXT:    crc32c.w a0, a0
-; RV32ZBR-NEXT:    ret
-  %tmp = call i32 @llvm.riscv.crc32c.w.i32(i32 %a)
- ret i32 %tmp
-}
diff --git a/llvm/test/CodeGen/RISCV/rv32zbt.ll b/llvm/test/CodeGen/RISCV/rv32zbt.ll
deleted file mode 100644
index 06f2a264b5d1..000000000000
--- a/llvm/test/CodeGen/RISCV/rv32zbt.ll
+++ /dev/null
@@ -1,824 +0,0 @@
-; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
-; RUN: llc -mtriple=riscv32 -verify-machineinstrs < %s \
-; RUN:   | FileCheck %s -check-prefix=RV32I
-; RUN: llc -mtriple=riscv32 -mattr=+experimental-b -verify-machineinstrs < %s \
-; RUN:   | FileCheck %s -check-prefix=RV32B
-; RUN: llc -mtriple=riscv32 -mattr=+experimental-zbt -verify-machineinstrs < %s \
-; RUN:   | FileCheck %s -check-prefix=RV32ZBT
-
-define i32 @cmix_i32(i32 %a, i32 %b, i32 %c) nounwind {
-; RV32I-LABEL: cmix_i32:
-; RV32I:       # %bb.0:
-; RV32I-NEXT:    and a0, a1, a0
-; RV32I-NEXT:    not a1, a1
-; RV32I-NEXT:    and a1, a1, a2
-; RV32I-NEXT:    or a0, a1, a0
-; RV32I-NEXT:    ret
-;
-; RV32B-LABEL: cmix_i32:
-; RV32B:       # %bb.0:
-; RV32B-NEXT:    cmix a0, a1, a0, a2
-; RV32B-NEXT:    ret
-;
-; RV32ZBT-LABEL: cmix_i32:
-; RV32ZBT:       # %bb.0:
-; RV32ZBT-NEXT:    cmix a0, a1, a0, a2
-; RV32ZBT-NEXT:    ret
-  %and = and i32 %b, %a
-  %neg = xor i32 %b, -1
-  %and1 = and i32 %neg, %c
-  %or = or i32 %and1, %and
-  ret i32 %or
-}
-
-define i64 @cmix_i64(i64 %a, i64 %b, i64 %c) nounwind {
-; RV32I-LABEL: cmix_i64:
-; RV32I:       # %bb.0:
-; RV32I-NEXT:    and a1, a3, a1
-; RV32I-NEXT:    and a0, a2, a0
-; RV32I-NEXT:    not a2, a2
-; RV32I-NEXT:    not a3, a3
-; RV32I-NEXT:    and a3, a3, a5
-; RV32I-NEXT:    and a2, a2, a4
-; RV32I-NEXT:    or a0, a2, a0
-; RV32I-NEXT:    or a1, a3, a1
-; RV32I-NEXT:    ret
-;
-; RV32B-LABEL: cmix_i64:
-; RV32B:       # %bb.0:
-; RV32B-NEXT:    cmix a0, a2, a0, a4
-; RV32B-NEXT:    cmix a1, a3, a1, a5
-; RV32B-NEXT:    ret
-;
-; RV32ZBT-LABEL: cmix_i64:
-; RV32ZBT:       # %bb.0:
-; RV32ZBT-NEXT:    cmix a0, a2, a0, a4
-; RV32ZBT-NEXT:    cmix a1, a3, a1, a5
-; RV32ZBT-NEXT:    ret
-  %and = and i64 %b, %a
-  %neg = xor i64 %b, -1
-  %and1 = and i64 %neg, %c
-  %or = or i64 %and1, %and
-  ret i64 %or
-}
-
-define i32 @cmov_i32(i32 %a, i32 %b, i32 %c) nounwind {
-; RV32I-LABEL: cmov_i32:
-; RV32I:       # %bb.0:
-; RV32I-NEXT:    beqz a1, .LBB2_2
-; RV32I-NEXT:  # %bb.1:
-; RV32I-NEXT:    mv a2, a0
-; RV32I-NEXT:  .LBB2_2:
-; RV32I-NEXT:    mv a0, a2
-; RV32I-NEXT:    ret
-;
-; RV32B-LABEL: cmov_i32:
-; RV32B:       # %bb.0:
-; RV32B-NEXT:    cmov a0, a1, a0, a2
-; RV32B-NEXT:    ret
-;
-; RV32ZBT-LABEL: cmov_i32:
-; RV32ZBT:       # %bb.0:
-; RV32ZBT-NEXT:    cmov a0, a1, a0, a2
-; RV32ZBT-NEXT:    ret
-  %tobool.not = icmp eq i32 %b, 0
-  %cond = select i1 %tobool.not, i32 %c, i32 %a
-  ret i32 %cond
-}
-
-define i32 @cmov_sle_i32(i32 %a, i32 %b, i32 %c, i32 %d) nounwind {
-; RV32I-LABEL: cmov_sle_i32:
-; RV32I:       # %bb.0:
-; RV32I-NEXT:    bge a2, a1, .LBB3_2
-; RV32I-NEXT:  # %bb.1:
-; RV32I-NEXT:    mv a0, a3
-; RV32I-NEXT:  .LBB3_2:
-; RV32I-NEXT:    ret
-;
-; RV32B-LABEL: cmov_sle_i32:
-; RV32B:       # %bb.0:
-; RV32B-NEXT:    slt a1, a2, a1
-; RV32B-NEXT:    cmov a0, a1, a3, a0
-; RV32B-NEXT:    ret
-;
-; RV32ZBT-LABEL: cmov_sle_i32:
-; RV32ZBT:       # %bb.0:
-; RV32ZBT-NEXT:    slt a1, a2, a1
-; RV32ZBT-NEXT:    cmov a0, a1, a3, a0
-; RV32ZBT-NEXT:    ret
-  %tobool = icmp sle i32 %b, %c
-  %cond = select i1 %tobool, i32 %a, i32 %d
-  ret i32 %cond
-}
-
-define i32 @cmov_sge_i32(i32 %a, i32 %b, i32 %c, i32 %d) nounwind {
-; RV32I-LABEL: cmov_sge_i32:
-; RV32I:       # %bb.0:
-; RV32I-NEXT:    bge a1, a2, .LBB4_2
-; RV32I-NEXT:  # %bb.1:
-; RV32I-NEXT:    mv a0, a3
-; RV32I-NEXT:  .LBB4_2:
-; RV32I-NEXT:    ret
-;
-; RV32B-LABEL: cmov_sge_i32:
-; RV32B:       # %bb.0:
-; RV32B-NEXT:    slt a1, a1, a2
-; RV32B-NEXT:    cmov a0, a1, a3, a0
-; RV32B-NEXT:    ret
-;
-; RV32ZBT-LABEL: cmov_sge_i32:
-; RV32ZBT:       # %bb.0:
-; RV32ZBT-NEXT:    slt a1, a1, a2
-; RV32ZBT-NEXT:    cmov a0, a1, a3, a0
-; RV32ZBT-NEXT:    ret
-  %tobool = icmp sge i32 %b, %c
-  %cond = select i1 %tobool, i32 %a, i32 %d
-  ret i32 %cond
-}
-
-define i32 @cmov_ule_i32(i32 %a, i32 %b, i32 %c, i32 %d) nounwind {
-; RV32I-LABEL: cmov_ule_i32:
-; RV32I:       # %bb.0:
-; RV32I-NEXT:    bgeu a2, a1, .LBB5_2
-; RV32I-NEXT:  # %bb.1:
-; RV32I-NEXT:    mv a0, a3
-; RV32I-NEXT:  .LBB5_2:
-; RV32I-NEXT:    ret
-;
-; RV32B-LABEL: cmov_ule_i32:
-; RV32B:       # %bb.0:
-; RV32B-NEXT:    sltu a1, a2, a1
-; RV32B-NEXT:    cmov a0, a1, a3, a0
-; RV32B-NEXT:    ret
-;
-; RV32ZBT-LABEL: cmov_ule_i32:
-; RV32ZBT:       # %bb.0:
-; RV32ZBT-NEXT:    sltu a1, a2, a1
-; RV32ZBT-NEXT:    cmov a0, a1, a3, a0
-; RV32ZBT-NEXT:    ret
-  %tobool = icmp ule i32 %b, %c
-  %cond = select i1 %tobool, i32 %a, i32 %d
-  ret i32 %cond
-}
-
-define i32 @cmov_uge_i32(i32 %a, i32 %b, i32 %c, i32 %d) nounwind {
-; RV32I-LABEL: cmov_uge_i32:
-; RV32I:       # %bb.0:
-; RV32I-NEXT:    bgeu a1, a2, .LBB6_2
-; RV32I-NEXT:  # %bb.1:
-; RV32I-NEXT:    mv a0, a3
-; RV32I-NEXT:  .LBB6_2:
-; RV32I-NEXT:    ret
-;
-; RV32B-LABEL: cmov_uge_i32:
-; RV32B:       # %bb.0:
-; RV32B-NEXT:    sltu a1, a1, a2
-; RV32B-NEXT:    cmov a0, a1, a3, a0
-; RV32B-NEXT:    ret
-;
-; RV32ZBT-LABEL: cmov_uge_i32:
-; RV32ZBT:       # %bb.0:
-; RV32ZBT-NEXT:    sltu a1, a1, a2
-; RV32ZBT-NEXT:    cmov a0, a1, a3, a0
-; RV32ZBT-NEXT:    ret
-  %tobool = icmp uge i32 %b, %c
-  %cond = select i1 %tobool, i32 %a, i32 %d
-  ret i32 %cond
-}
-
-define i64 @cmov_i64(i64 %a, i64 %b, i64 %c) nounwind {
-; RV32I-LABEL: cmov_i64:
-; RV32I:       # %bb.0:
-; RV32I-NEXT:    or a2, a2, a3
-; RV32I-NEXT:    beqz a2, .LBB7_2
-; RV32I-NEXT:  # %bb.1:
-; RV32I-NEXT:    mv a4, a0
-; RV32I-NEXT:    mv a5, a1
-; RV32I-NEXT:  .LBB7_2:
-; RV32I-NEXT:    mv a0, a4
-; RV32I-NEXT:    mv a1, a5
-; RV32I-NEXT:    ret
-;
-; RV32B-LABEL: cmov_i64:
-; RV32B:       # %bb.0:
-; RV32B-NEXT:    or a2, a2, a3
-; RV32B-NEXT:    cmov a0, a2, a0, a4
-; RV32B-NEXT:    cmov a1, a2, a1, a5
-; RV32B-NEXT:    ret
-;
-; RV32ZBT-LABEL: cmov_i64:
-; RV32ZBT:       # %bb.0:
-; RV32ZBT-NEXT:    or a2, a2, a3
-; RV32ZBT-NEXT:    cmov a0, a2, a0, a4
-; RV32ZBT-NEXT:    cmov a1, a2, a1, a5
-; RV32ZBT-NEXT:    ret
-  %tobool.not = icmp eq i64 %b, 0
-  %cond = select i1 %tobool.not, i64 %c, i64 %a
-  ret i64 %cond
-}
-
-define i64 @cmov_sle_i64(i64 %a, i64 %b, i64 %c, i64 %d) nounwind {
-; RV32I-LABEL: cmov_sle_i64:
-; RV32I:       # %bb.0:
-; RV32I-NEXT:    beq a3, a5, .LBB8_2
-; RV32I-NEXT:  # %bb.1:
-; RV32I-NEXT:    slt a2, a5, a3
-; RV32I-NEXT:    xori a2, a2, 1
-; RV32I-NEXT:    beqz a2, .LBB8_3
-; RV32I-NEXT:    j .LBB8_4
-; RV32I-NEXT:  .LBB8_2:
-; RV32I-NEXT:    sltu a2, a4, a2
-; RV32I-NEXT:    xori a2, a2, 1
-; RV32I-NEXT:    bnez a2, .LBB8_4
-; RV32I-NEXT:  .LBB8_3:
-; RV32I-NEXT:    mv a0, a6
-; RV32I-NEXT:    mv a1, a7
-; RV32I-NEXT:  .LBB8_4:
-; RV32I-NEXT:    ret
-;
-; RV32B-LABEL: cmov_sle_i64:
-; RV32B:       # %bb.0:
-; RV32B-NEXT:    xor t0, a3, a5
-; RV32B-NEXT:    sltu a2, a4, a2
-; RV32B-NEXT:    xori a2, a2, 1
-; RV32B-NEXT:    slt a3, a5, a3
-; RV32B-NEXT:    xori a3, a3, 1
-; RV32B-NEXT:    cmov a2, t0, a3, a2
-; RV32B-NEXT:    cmov a0, a2, a0, a6
-; RV32B-NEXT:    cmov a1, a2, a1, a7
-; RV32B-NEXT:    ret
-;
-; RV32ZBT-LABEL: cmov_sle_i64:
-; RV32ZBT:       # %bb.0:
-; RV32ZBT-NEXT:    xor t0, a3, a5
-; RV32ZBT-NEXT:    sltu a2, a4, a2
-; RV32ZBT-NEXT:    xori a2, a2, 1
-; RV32ZBT-NEXT:    slt a3, a5, a3
-; RV32ZBT-NEXT:    xori a3, a3, 1
-; RV32ZBT-NEXT:    cmov a2, t0, a3, a2
-; RV32ZBT-NEXT:    cmov a0, a2, a0, a6
-; RV32ZBT-NEXT:    cmov a1, a2, a1, a7
-; RV32ZBT-NEXT:    ret
-  %tobool = icmp sle i64 %b, %c
-  %cond = select i1 %tobool, i64 %a, i64 %d
-  ret i64 %cond
-}
-
-define i64 @cmov_sge_i64(i64 %a, i64 %b, i64 %c, i64 %d) nounwind {
-; RV32I-LABEL: cmov_sge_i64:
-; RV32I:       # %bb.0:
-; RV32I-NEXT:    beq a3, a5, .LBB9_2
-; RV32I-NEXT:  # %bb.1:
-; RV32I-NEXT:    slt a2, a3, a5
-; RV32I-NEXT:    xori a2, a2, 1
-; RV32I-NEXT:    beqz a2, .LBB9_3
-; RV32I-NEXT:    j .LBB9_4
-; RV32I-NEXT:  .LBB9_2:
-; RV32I-NEXT:    sltu a2, a2, a4
-; RV32I-NEXT:    xori a2, a2, 1
-; RV32I-NEXT:    bnez a2, .LBB9_4
-; RV32I-NEXT:  .LBB9_3:
-; RV32I-NEXT:    mv a0, a6
-; RV32I-NEXT:    mv a1, a7
-; RV32I-NEXT:  .LBB9_4:
-; RV32I-NEXT:    ret
-;
-; RV32B-LABEL: cmov_sge_i64:
-; RV32B:       # %bb.0:
-; RV32B-NEXT:    xor t0, a3, a5
-; RV32B-NEXT:    sltu a2, a2, a4
-; RV32B-NEXT:    xori a2, a2, 1
-; RV32B-NEXT:    slt a3, a3, a5
-; RV32B-NEXT:    xori a3, a3, 1
-; RV32B-NEXT:    cmov a2, t0, a3, a2
-; RV32B-NEXT:    cmov a0, a2, a0, a6
-; RV32B-NEXT:    cmov a1, a2, a1, a7
-; RV32B-NEXT:    ret
-;
-; RV32ZBT-LABEL: cmov_sge_i64:
-; RV32ZBT:       # %bb.0:
-; RV32ZBT-NEXT:    xor t0, a3, a5
-; RV32ZBT-NEXT:    sltu a2, a2, a4
-; RV32ZBT-NEXT:    xori a2, a2, 1
-; RV32ZBT-NEXT:    slt a3, a3, a5
-; RV32ZBT-NEXT:    xori a3, a3, 1
-; RV32ZBT-NEXT:    cmov a2, t0, a3, a2
-; RV32ZBT-NEXT:    cmov a0, a2, a0, a6
-; RV32ZBT-NEXT:    cmov a1, a2, a1, a7
-; RV32ZBT-NEXT:    ret
-  %tobool = icmp sge i64 %b, %c
-  %cond = select i1 %tobool, i64 %a, i64 %d
-  ret i64 %cond
-}
-
-define i64 @cmov_ule_i64(i64 %a, i64 %b, i64 %c, i64 %d) nounwind {
-; RV32I-LABEL: cmov_ule_i64:
-; RV32I:       # %bb.0:
-; RV32I-NEXT:    beq a3, a5, .LBB10_2
-; RV32I-NEXT:  # %bb.1:
-; RV32I-NEXT:    sltu a2, a5, a3
-; RV32I-NEXT:    xori a2, a2, 1
-; RV32I-NEXT:    beqz a2, .LBB10_3
-; RV32I-NEXT:    j .LBB10_4
-; RV32I-NEXT:  .LBB10_2:
-; RV32I-NEXT:    sltu a2, a4, a2
-; RV32I-NEXT:    xori a2, a2, 1
-; RV32I-NEXT:    bnez a2, .LBB10_4
-; RV32I-NEXT:  .LBB10_3:
-; RV32I-NEXT:    mv a0, a6
-; RV32I-NEXT:    mv a1, a7
-; RV32I-NEXT:  .LBB10_4:
-; RV32I-NEXT:    ret
-;
-; RV32B-LABEL: cmov_ule_i64:
-; RV32B:       # %bb.0:
-; RV32B-NEXT:    xor t0, a3, a5
-; RV32B-NEXT:    sltu a2, a4, a2
-; RV32B-NEXT:    xori a2, a2, 1
-; RV32B-NEXT:    sltu a3, a5, a3
-; RV32B-NEXT:    xori a3, a3, 1
-; RV32B-NEXT:    cmov a2, t0, a3, a2
-; RV32B-NEXT:    cmov a0, a2, a0, a6
-; RV32B-NEXT:    cmov a1, a2, a1, a7
-; RV32B-NEXT:    ret
-;
-; RV32ZBT-LABEL: cmov_ule_i64:
-; RV32ZBT:       # %bb.0:
-; RV32ZBT-NEXT:    xor t0, a3, a5
-; RV32ZBT-NEXT:    sltu a2, a4, a2
-; RV32ZBT-NEXT:    xori a2, a2, 1
-; RV32ZBT-NEXT:    sltu a3, a5, a3
-; RV32ZBT-NEXT:    xori a3, a3, 1
-; RV32ZBT-NEXT:    cmov a2, t0, a3, a2
-; RV32ZBT-NEXT:    cmov a0, a2, a0, a6
-; RV32ZBT-NEXT:    cmov a1, a2, a1, a7
-; RV32ZBT-NEXT:    ret
-  %tobool = icmp ule i64 %b, %c
-  %cond = select i1 %tobool, i64 %a, i64 %d
-  ret i64 %cond
-}
-
-define i64 @cmov_uge_i64(i64 %a, i64 %b, i64 %c, i64 %d) nounwind {
-; RV32I-LABEL: cmov_uge_i64:
-; RV32I:       # %bb.0:
-; RV32I-NEXT:    beq a3, a5, .LBB11_2
-; RV32I-NEXT:  # %bb.1:
-; RV32I-NEXT:    sltu a2, a3, a5
-; RV32I-NEXT:    xori a2, a2, 1
-; RV32I-NEXT:    beqz a2, .LBB11_3
-; RV32I-NEXT:    j .LBB11_4
-; RV32I-NEXT:  .LBB11_2:
-; RV32I-NEXT:    sltu a2, a2, a4
-; RV32I-NEXT:    xori a2, a2, 1
-; RV32I-NEXT:    bnez a2, .LBB11_4
-; RV32I-NEXT:  .LBB11_3:
-; RV32I-NEXT:    mv a0, a6
-; RV32I-NEXT:    mv a1, a7
-; RV32I-NEXT:  .LBB11_4:
-; RV32I-NEXT:    ret
-;
-; RV32B-LABEL: cmov_uge_i64:
-; RV32B:       # %bb.0:
-; RV32B-NEXT:    xor t0, a3, a5
-; RV32B-NEXT:    sltu a2, a2, a4
-; RV32B-NEXT:    xori a2, a2, 1
-; RV32B-NEXT:    sltu a3, a3, a5
-; RV32B-NEXT:    xori a3, a3, 1
-; RV32B-NEXT:    cmov a2, t0, a3, a2
-; RV32B-NEXT:    cmov a0, a2, a0, a6
-; RV32B-NEXT:    cmov a1, a2, a1, a7
-; RV32B-NEXT:    ret
-;
-; RV32ZBT-LABEL: cmov_uge_i64:
-; RV32ZBT:       # %bb.0:
-; RV32ZBT-NEXT:    xor t0, a3, a5
-; RV32ZBT-NEXT:    sltu a2, a2, a4
-; RV32ZBT-NEXT:    xori a2, a2, 1
-; RV32ZBT-NEXT:    sltu a3, a3, a5
-; RV32ZBT-NEXT:    xori a3, a3, 1
-; RV32ZBT-NEXT:    cmov a2, t0, a3, a2
-; RV32ZBT-NEXT:    cmov a0, a2, a0, a6
-; RV32ZBT-NEXT:    cmov a1, a2, a1, a7
-; RV32ZBT-NEXT:    ret
-  %tobool = icmp uge i64 %b, %c
-  %cond = select i1 %tobool, i64 %a, i64 %d
-  ret i64 %cond
-}
-
-declare i32 @llvm.fshl.i32(i32, i32, i32)
-
-define i32 @fshl_i32(i32 %a, i32 %b, i32 %c) nounwind {
-; RV32I-LABEL: fshl_i32:
-; RV32I:       # %bb.0:
-; RV32I-NEXT:    sll a0, a0, a2
-; RV32I-NEXT:    not a2, a2
-; RV32I-NEXT:    srli a1, a1, 1
-; RV32I-NEXT:    srl a1, a1, a2
-; RV32I-NEXT:    or a0, a0, a1
-; RV32I-NEXT:    ret
-;
-; RV32B-LABEL: fshl_i32:
-; RV32B:       # %bb.0:
-; RV32B-NEXT:    andi a2, a2, 31
-; RV32B-NEXT:    fsl a0, a0, a1, a2
-; RV32B-NEXT:    ret
-;
-; RV32ZBT-LABEL: fshl_i32:
-; RV32ZBT:       # %bb.0:
-; RV32ZBT-NEXT:    andi a2, a2, 31
-; RV32ZBT-NEXT:    fsl a0, a0, a1, a2
-; RV32ZBT-NEXT:    ret
-  %1 = tail call i32 @llvm.fshl.i32(i32 %a, i32 %b, i32 %c)
-  ret i32 %1
-}
-
-; As we are not matching directly i64 code patterns on RV32 some i64 patterns
-; don't have yet an efficient pattern-matching with bit manipulation
-; instructions on RV32.
-; This test is presented here in case future expansions of the experimental-b
-; extension introduce instructions that can match more efficiently this pattern.
-
-declare i64 @llvm.fshl.i64(i64, i64, i64)
-
-define i64 @fshl_i64(i64 %a, i64 %b, i64 %c) nounwind {
-; RV32I-LABEL: fshl_i64:
-; RV32I:       # %bb.0:
-; RV32I-NEXT:    andi a5, a4, 63
-; RV32I-NEXT:    addi a7, a5, -32
-; RV32I-NEXT:    addi a6, zero, 31
-; RV32I-NEXT:    bltz a7, .LBB13_2
-; RV32I-NEXT:  # %bb.1:
-; RV32I-NEXT:    sll a1, a0, a7
-; RV32I-NEXT:    j .LBB13_3
-; RV32I-NEXT:  .LBB13_2:
-; RV32I-NEXT:    sll t0, a1, a4
-; RV32I-NEXT:    sub a5, a6, a5
-; RV32I-NEXT:    srli a1, a0, 1
-; RV32I-NEXT:    srl a1, a1, a5
-; RV32I-NEXT:    or a1, t0, a1
-; RV32I-NEXT:  .LBB13_3:
-; RV32I-NEXT:    not t0, a4
-; RV32I-NEXT:    andi t3, t0, 63
-; RV32I-NEXT:    addi t2, t3, -32
-; RV32I-NEXT:    srli t1, a3, 1
-; RV32I-NEXT:    bltz t2, .LBB13_5
-; RV32I-NEXT:  # %bb.4:
-; RV32I-NEXT:    srl a2, t1, t2
-; RV32I-NEXT:    bltz a7, .LBB13_6
-; RV32I-NEXT:    j .LBB13_7
-; RV32I-NEXT:  .LBB13_5:
-; RV32I-NEXT:    srl a5, t1, t0
-; RV32I-NEXT:    or a1, a1, a5
-; RV32I-NEXT:    slli a3, a3, 31
-; RV32I-NEXT:    srli a2, a2, 1
-; RV32I-NEXT:    or a2, a2, a3
-; RV32I-NEXT:    srl a2, a2, t0
-; RV32I-NEXT:    sub a3, a6, t3
-; RV32I-NEXT:    slli a5, t1, 1
-; RV32I-NEXT:    sll a3, a5, a3
-; RV32I-NEXT:    or a2, a2, a3
-; RV32I-NEXT:    bgez a7, .LBB13_7
-; RV32I-NEXT:  .LBB13_6:
-; RV32I-NEXT:    sll a0, a0, a4
-; RV32I-NEXT:    or a2, a2, a0
-; RV32I-NEXT:  .LBB13_7:
-; RV32I-NEXT:    mv a0, a2
-; RV32I-NEXT:    ret
-;
-; RV32B-LABEL: fshl_i64:
-; RV32B:       # %bb.0:
-; RV32B-NEXT:    sll a7, a1, a4
-; RV32B-NEXT:    andi a5, a4, 63
-; RV32B-NEXT:    addi a6, zero, 31
-; RV32B-NEXT:    sub t0, a6, a5
-; RV32B-NEXT:    srli a1, a0, 1
-; RV32B-NEXT:    srl a1, a1, t0
-; RV32B-NEXT:    or t0, a7, a1
-; RV32B-NEXT:    addi a7, a5, -32
-; RV32B-NEXT:    sll a5, a0, a7
-; RV32B-NEXT:    slti a1, a7, 0
-; RV32B-NEXT:    cmov t1, a1, t0, a5
-; RV32B-NEXT:    not t0, a4
-; RV32B-NEXT:    srli a5, a3, 1
-; RV32B-NEXT:    srl t2, a5, t0
-; RV32B-NEXT:    addi a1, zero, 63
-; RV32B-NEXT:    andn t3, a1, a4
-; RV32B-NEXT:    addi t4, t3, -32
-; RV32B-NEXT:    srai a1, t4, 31
-; RV32B-NEXT:    and a1, a1, t2
-; RV32B-NEXT:    or a1, t1, a1
-; RV32B-NEXT:    fsri a2, a2, a3, 1
-; RV32B-NEXT:    srl t0, a2, t0
-; RV32B-NEXT:    sub a3, a6, t3
-; RV32B-NEXT:    slli a2, a5, 1
-; RV32B-NEXT:    sll a2, a2, a3
-; RV32B-NEXT:    or a2, t0, a2
-; RV32B-NEXT:    srl a3, a5, t4
-; RV32B-NEXT:    slti a5, t4, 0
-; RV32B-NEXT:    cmov a2, a5, a2, a3
-; RV32B-NEXT:    sll a0, a0, a4
-; RV32B-NEXT:    srai a3, a7, 31
-; RV32B-NEXT:    and a0, a3, a0
-; RV32B-NEXT:    or a0, a0, a2
-; RV32B-NEXT:    ret
-;
-; RV32ZBT-LABEL: fshl_i64:
-; RV32ZBT:       # %bb.0:
-; RV32ZBT-NEXT:    sll a7, a1, a4
-; RV32ZBT-NEXT:    andi a5, a4, 63
-; RV32ZBT-NEXT:    addi a6, zero, 31
-; RV32ZBT-NEXT:    sub t0, a6, a5
-; RV32ZBT-NEXT:    srli a1, a0, 1
-; RV32ZBT-NEXT:    srl a1, a1, t0
-; RV32ZBT-NEXT:    or t0, a7, a1
-; RV32ZBT-NEXT:    addi a7, a5, -32
-; RV32ZBT-NEXT:    sll a5, a0, a7
-; RV32ZBT-NEXT:    slti a1, a7, 0
-; RV32ZBT-NEXT:    cmov t1, a1, t0, a5
-; RV32ZBT-NEXT:    not t0, a4
-; RV32ZBT-NEXT:    srli a5, a3, 1
-; RV32ZBT-NEXT:    srl t4, a5, t0
-; RV32ZBT-NEXT:    andi t2, t0, 63
-; RV32ZBT-NEXT:    addi t3, t2, -32
-; RV32ZBT-NEXT:    srai a1, t3, 31
-; RV32ZBT-NEXT:    and a1, a1, t4
-; RV32ZBT-NEXT:    or a1, t1, a1
-; RV32ZBT-NEXT:    fsri a2, a2, a3, 1
-; RV32ZBT-NEXT:    srl t0, a2, t0
-; RV32ZBT-NEXT:    sub a3, a6, t2
-; RV32ZBT-NEXT:    slli a2, a5, 1
-; RV32ZBT-NEXT:    sll a2, a2, a3
-; RV32ZBT-NEXT:    or a2, t0, a2
-; RV32ZBT-NEXT:    srl a3, a5, t3
-; RV32ZBT-NEXT:    slti a5, t3, 0
-; RV32ZBT-NEXT:    cmov a2, a5, a2, a3
-; RV32ZBT-NEXT:    sll a0, a0, a4
-; RV32ZBT-NEXT:    srai a3, a7, 31
-; RV32ZBT-NEXT:    and a0, a3, a0
-; RV32ZBT-NEXT:    or a0, a0, a2
-; RV32ZBT-NEXT:    ret
-  %1 = tail call i64 @llvm.fshl.i64(i64 %a, i64 %b, i64 %c)
-  ret i64 %1
-}
-
-declare i32 @llvm.fshr.i32(i32, i32, i32)
-
-define i32 @fshr_i32(i32 %a, i32 %b, i32 %c) nounwind {
-; RV32I-LABEL: fshr_i32:
-; RV32I:       # %bb.0:
-; RV32I-NEXT:    srl a1, a1, a2
-; RV32I-NEXT:    not a2, a2
-; RV32I-NEXT:    slli a0, a0, 1
-; RV32I-NEXT:    sll a0, a0, a2
-; RV32I-NEXT:    or a0, a0, a1
-; RV32I-NEXT:    ret
-;
-; RV32B-LABEL: fshr_i32:
-; RV32B:       # %bb.0:
-; RV32B-NEXT:    andi a2, a2, 31
-; RV32B-NEXT:    fsr a0, a1, a0, a2
-; RV32B-NEXT:    ret
-;
-; RV32ZBT-LABEL: fshr_i32:
-; RV32ZBT:       # %bb.0:
-; RV32ZBT-NEXT:    andi a2, a2, 31
-; RV32ZBT-NEXT:    fsr a0, a1, a0, a2
-; RV32ZBT-NEXT:    ret
-  %1 = tail call i32 @llvm.fshr.i32(i32 %a, i32 %b, i32 %c)
-  ret i32 %1
-}
-
-; As we are not matching directly i64 code patterns on RV32 some i64 patterns
-; don't have yet an efficient pattern-matching with bit manipulation
-; instructions on RV32.
-; This test is presented here in case future expansions of the experimental-b
-; extension introduce instructions that can match more efficiently this pattern.
-
-declare i64 @llvm.fshr.i64(i64, i64, i64)
-
-define i64 @fshr_i64(i64 %a, i64 %b, i64 %c) nounwind {
-; RV32I-LABEL: fshr_i64:
-; RV32I:       # %bb.0:
-; RV32I-NEXT:    mv t1, a0
-; RV32I-NEXT:    andi a0, a4, 63
-; RV32I-NEXT:    addi a6, a0, -32
-; RV32I-NEXT:    addi a7, zero, 31
-; RV32I-NEXT:    bltz a6, .LBB15_2
-; RV32I-NEXT:  # %bb.1:
-; RV32I-NEXT:    srl a0, a3, a6
-; RV32I-NEXT:    j .LBB15_3
-; RV32I-NEXT:  .LBB15_2:
-; RV32I-NEXT:    srl a2, a2, a4
-; RV32I-NEXT:    sub a0, a7, a0
-; RV32I-NEXT:    slli a5, a3, 1
-; RV32I-NEXT:    sll a0, a5, a0
-; RV32I-NEXT:    or a0, a2, a0
-; RV32I-NEXT:  .LBB15_3:
-; RV32I-NEXT:    not t0, a4
-; RV32I-NEXT:    andi a2, t0, 63
-; RV32I-NEXT:    addi t2, a2, -32
-; RV32I-NEXT:    slli a5, t1, 1
-; RV32I-NEXT:    bltz t2, .LBB15_5
-; RV32I-NEXT:  # %bb.4:
-; RV32I-NEXT:    sll a1, a5, t2
-; RV32I-NEXT:    bltz a6, .LBB15_6
-; RV32I-NEXT:    j .LBB15_7
-; RV32I-NEXT:  .LBB15_5:
-; RV32I-NEXT:    sll a5, a5, t0
-; RV32I-NEXT:    or a0, a0, a5
-; RV32I-NEXT:    lui a5, 524288
-; RV32I-NEXT:    addi a5, a5, -1
-; RV32I-NEXT:    and a5, t1, a5
-; RV32I-NEXT:    sub a2, a7, a2
-; RV32I-NEXT:    srl a2, a5, a2
-; RV32I-NEXT:    srli a5, t1, 31
-; RV32I-NEXT:    slli a1, a1, 1
-; RV32I-NEXT:    or a1, a1, a5
-; RV32I-NEXT:    sll a1, a1, t0
-; RV32I-NEXT:    or a1, a1, a2
-; RV32I-NEXT:    bgez a6, .LBB15_7
-; RV32I-NEXT:  .LBB15_6:
-; RV32I-NEXT:    srl a2, a3, a4
-; RV32I-NEXT:    or a1, a1, a2
-; RV32I-NEXT:  .LBB15_7:
-; RV32I-NEXT:    ret
-;
-; RV32B-LABEL: fshr_i64:
-; RV32B:       # %bb.0:
-; RV32B-NEXT:    srl a7, a2, a4
-; RV32B-NEXT:    andi a5, a4, 63
-; RV32B-NEXT:    addi a6, zero, 31
-; RV32B-NEXT:    sub t0, a6, a5
-; RV32B-NEXT:    slli a2, a3, 1
-; RV32B-NEXT:    sll a2, a2, t0
-; RV32B-NEXT:    or t0, a7, a2
-; RV32B-NEXT:    addi a7, a5, -32
-; RV32B-NEXT:    srl a5, a3, a7
-; RV32B-NEXT:    slti a2, a7, 0
-; RV32B-NEXT:    cmov t1, a2, t0, a5
-; RV32B-NEXT:    not t0, a4
-; RV32B-NEXT:    slli t4, a0, 1
-; RV32B-NEXT:    sll t2, t4, t0
-; RV32B-NEXT:    addi a2, zero, 63
-; RV32B-NEXT:    andn a2, a2, a4
-; RV32B-NEXT:    addi t3, a2, -32
-; RV32B-NEXT:    srai a5, t3, 31
-; RV32B-NEXT:    and a5, a5, t2
-; RV32B-NEXT:    or t1, a5, t1
-; RV32B-NEXT:    fsri a1, a0, a1, 31
-; RV32B-NEXT:    sll a1, a1, t0
-; RV32B-NEXT:    sub a2, a6, a2
-; RV32B-NEXT:    bclri a0, a0, 31
-; RV32B-NEXT:    srl a0, a0, a2
-; RV32B-NEXT:    or a0, a1, a0
-; RV32B-NEXT:    sll a1, t4, t3
-; RV32B-NEXT:    slti a2, t3, 0
-; RV32B-NEXT:    cmov a0, a2, a0, a1
-; RV32B-NEXT:    srl a1, a3, a4
-; RV32B-NEXT:    srai a2, a7, 31
-; RV32B-NEXT:    and a1, a2, a1
-; RV32B-NEXT:    or a1, a0, a1
-; RV32B-NEXT:    mv a0, t1
-; RV32B-NEXT:    ret
-;
-; RV32ZBT-LABEL: fshr_i64:
-; RV32ZBT:       # %bb.0:
-; RV32ZBT-NEXT:    srl a7, a2, a4
-; RV32ZBT-NEXT:    andi a5, a4, 63
-; RV32ZBT-NEXT:    addi a6, zero, 31
-; RV32ZBT-NEXT:    sub t0, a6, a5
-; RV32ZBT-NEXT:    slli a2, a3, 1
-; RV32ZBT-NEXT:    sll a2, a2, t0
-; RV32ZBT-NEXT:    or t0, a7, a2
-; RV32ZBT-NEXT:    addi a7, a5, -32
-; RV32ZBT-NEXT:    srl a5, a3, a7
-; RV32ZBT-NEXT:    slti a2, a7, 0
-; RV32ZBT-NEXT:    cmov t1, a2, t0, a5
-; RV32ZBT-NEXT:    not t0, a4
-; RV32ZBT-NEXT:    slli t4, a0, 1
-; RV32ZBT-NEXT:    sll t2, t4, t0
-; RV32ZBT-NEXT:    andi a2, t0, 63
-; RV32ZBT-NEXT:    addi t3, a2, -32
-; RV32ZBT-NEXT:    srai a5, t3, 31
-; RV32ZBT-NEXT:    and a5, a5, t2
-; RV32ZBT-NEXT:    or t1, a5, t1
-; RV32ZBT-NEXT:    lui a5, 524288
-; RV32ZBT-NEXT:    addi a5, a5, -1
-; RV32ZBT-NEXT:    and a5, a0, a5
-; RV32ZBT-NEXT:    sub a2, a6, a2
-; RV32ZBT-NEXT:    srl a2, a5, a2
-; RV32ZBT-NEXT:    fsri a0, a0, a1, 31
-; RV32ZBT-NEXT:    sll a0, a0, t0
-; RV32ZBT-NEXT:    or a0, a0, a2
-; RV32ZBT-NEXT:    sll a1, t4, t3
-; RV32ZBT-NEXT:    slti a2, t3, 0
-; RV32ZBT-NEXT:    cmov a0, a2, a0, a1
-; RV32ZBT-NEXT:    srl a1, a3, a4
-; RV32ZBT-NEXT:    srai a2, a7, 31
-; RV32ZBT-NEXT:    and a1, a2, a1
-; RV32ZBT-NEXT:    or a1, a0, a1
-; RV32ZBT-NEXT:    mv a0, t1
-; RV32ZBT-NEXT:    ret
-  %1 = tail call i64 @llvm.fshr.i64(i64 %a, i64 %b, i64 %c)
-  ret i64 %1
-}
-
-define i32 @fshri_i32(i32 %a, i32 %b) nounwind {
-; RV32I-LABEL: fshri_i32:
-; RV32I:       # %bb.0:
-; RV32I-NEXT:    srli a1, a1, 5
-; RV32I-NEXT:    slli a0, a0, 27
-; RV32I-NEXT:    or a0, a0, a1
-; RV32I-NEXT:    ret
-;
-; RV32B-LABEL: fshri_i32:
-; RV32B:       # %bb.0:
-; RV32B-NEXT:    fsri a0, a1, a0, 5
-; RV32B-NEXT:    ret
-;
-; RV32ZBT-LABEL: fshri_i32:
-; RV32ZBT:       # %bb.0:
-; RV32ZBT-NEXT:    fsri a0, a1, a0, 5
-; RV32ZBT-NEXT:    ret
-  %1 = tail call i32 @llvm.fshr.i32(i32 %a, i32 %b, i32 5)
-  ret i32 %1
-}
-
-define i64 @fshri_i64(i64 %a, i64 %b) nounwind {
-; RV32I-LABEL: fshri_i64:
-; RV32I:       # %bb.0:
-; RV32I-NEXT:    slli a1, a3, 27
-; RV32I-NEXT:    srli a2, a2, 5
-; RV32I-NEXT:    or a2, a2, a1
-; RV32I-NEXT:    srli a1, a3, 5
-; RV32I-NEXT:    slli a0, a0, 27
-; RV32I-NEXT:    or a1, a0, a1
-; RV32I-NEXT:    mv a0, a2
-; RV32I-NEXT:    ret
-;
-; RV32B-LABEL: fshri_i64:
-; RV32B:       # %bb.0:
-; RV32B-NEXT:    fsri a2, a2, a3, 5
-; RV32B-NEXT:    fsri a1, a3, a0, 5
-; RV32B-NEXT:    mv a0, a2
-; RV32B-NEXT:    ret
-;
-; RV32ZBT-LABEL: fshri_i64:
-; RV32ZBT:       # %bb.0:
-; RV32ZBT-NEXT:    fsri a2, a2, a3, 5
-; RV32ZBT-NEXT:    fsri a1, a3, a0, 5
-; RV32ZBT-NEXT:    mv a0, a2
-; RV32ZBT-NEXT:    ret
-  %1 = tail call i64 @llvm.fshr.i64(i64 %a, i64 %b, i64 5)
-  ret i64 %1
-}
-
-define i32 @fshli_i32(i32 %a, i32 %b) nounwind {
-; RV32I-LABEL: fshli_i32:
-; RV32I:       # %bb.0:
-; RV32I-NEXT:    srli a1, a1, 27
-; RV32I-NEXT:    slli a0, a0, 5
-; RV32I-NEXT:    or a0, a0, a1
-; RV32I-NEXT:    ret
-;
-; RV32B-LABEL: fshli_i32:
-; RV32B:       # %bb.0:
-; RV32B-NEXT:    fsri a0, a1, a0, 27
-; RV32B-NEXT:    ret
-;
-; RV32ZBT-LABEL: fshli_i32:
-; RV32ZBT:       # %bb.0:
-; RV32ZBT-NEXT:    fsri a0, a1, a0, 27
-; RV32ZBT-NEXT:    ret
-  %1 = tail call i32 @llvm.fshl.i32(i32 %a, i32 %b, i32 5)
-  ret i32 %1
-}
-
-define i64 @fshli_i64(i64 %a, i64 %b) nounwind {
-; RV32I-LABEL: fshli_i64:
-; RV32I:       # %bb.0:
-; RV32I-NEXT:    srli a2, a3, 27
-; RV32I-NEXT:    slli a3, a0, 5
-; RV32I-NEXT:    or a2, a3, a2
-; RV32I-NEXT:    srli a0, a0, 27
-; RV32I-NEXT:    slli a1, a1, 5
-; RV32I-NEXT:    or a1, a1, a0
-; RV32I-NEXT:    mv a0, a2
-; RV32I-NEXT:    ret
-;
-; RV32B-LABEL: fshli_i64:
-; RV32B:       # %bb.0:
-; RV32B-NEXT:    fsri a2, a3, a0, 27
-; RV32B-NEXT:    fsri a1, a0, a1, 27
-; RV32B-NEXT:    mv a0, a2
-; RV32B-NEXT:    ret
-;
-; RV32ZBT-LABEL: fshli_i64:
-; RV32ZBT:       # %bb.0:
-; RV32ZBT-NEXT:    fsri a2, a3, a0, 27
-; RV32ZBT-NEXT:    fsri a1, a0, a1, 27
-; RV32ZBT-NEXT:    mv a0, a2
-; RV32ZBT-NEXT:    ret
-  %1 = tail call i64 @llvm.fshl.i64(i64 %a, i64 %b, i64 5)
-  ret i64 %1
-}
diff --git a/llvm/test/CodeGen/RISCV/rv64zbb-zbp.ll b/llvm/test/CodeGen/RISCV/rv64zbb-zbp.ll
deleted file mode 100644
index af4f677205bd..000000000000
--- a/llvm/test/CodeGen/RISCV/rv64zbb-zbp.ll
+++ /dev/null
@@ -1,779 +0,0 @@
-; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
-; RUN: llc -mtriple=riscv64 -verify-machineinstrs < %s \
-; RUN:   | FileCheck %s -check-prefix=RV64I
-; RUN: llc -mtriple=riscv64 -mattr=+experimental-b -verify-machineinstrs < %s \
-; RUN:   | FileCheck %s -check-prefix=RV64B
-; RUN: llc -mtriple=riscv64 -mattr=+experimental-zbb -verify-machineinstrs < %s \
-; RUN:   | FileCheck %s -check-prefix=RV64ZBB
-; RUN: llc -mtriple=riscv64 -mattr=+experimental-zbp -verify-machineinstrs < %s \
-; RUN:   | FileCheck %s -check-prefix=RV64ZBP
-
-define signext i32 @andn_i32(i32 signext %a, i32 signext %b) nounwind {
-; RV64I-LABEL: andn_i32:
-; RV64I:       # %bb.0:
-; RV64I-NEXT:    not a1, a1
-; RV64I-NEXT:    and a0, a1, a0
-; RV64I-NEXT:    ret
-;
-; RV64B-LABEL: andn_i32:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    andn a0, a0, a1
-; RV64B-NEXT:    ret
-;
-; RV64ZBB-LABEL: andn_i32:
-; RV64ZBB:       # %bb.0:
-; RV64ZBB-NEXT:    andn a0, a0, a1
-; RV64ZBB-NEXT:    ret
-;
-; RV64ZBP-LABEL: andn_i32:
-; RV64ZBP:       # %bb.0:
-; RV64ZBP-NEXT:    andn a0, a0, a1
-; RV64ZBP-NEXT:    ret
-  %neg = xor i32 %b, -1
-  %and = and i32 %neg, %a
-  ret i32 %and
-}
-
-define i64 @andn_i64(i64 %a, i64 %b) nounwind {
-; RV64I-LABEL: andn_i64:
-; RV64I:       # %bb.0:
-; RV64I-NEXT:    not a1, a1
-; RV64I-NEXT:    and a0, a1, a0
-; RV64I-NEXT:    ret
-;
-; RV64B-LABEL: andn_i64:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    andn a0, a0, a1
-; RV64B-NEXT:    ret
-;
-; RV64ZBB-LABEL: andn_i64:
-; RV64ZBB:       # %bb.0:
-; RV64ZBB-NEXT:    andn a0, a0, a1
-; RV64ZBB-NEXT:    ret
-;
-; RV64ZBP-LABEL: andn_i64:
-; RV64ZBP:       # %bb.0:
-; RV64ZBP-NEXT:    andn a0, a0, a1
-; RV64ZBP-NEXT:    ret
-  %neg = xor i64 %b, -1
-  %and = and i64 %neg, %a
-  ret i64 %and
-}
-
-define signext i32 @orn_i32(i32 signext %a, i32 signext %b) nounwind {
-; RV64I-LABEL: orn_i32:
-; RV64I:       # %bb.0:
-; RV64I-NEXT:    not a1, a1
-; RV64I-NEXT:    or a0, a1, a0
-; RV64I-NEXT:    ret
-;
-; RV64B-LABEL: orn_i32:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    orn a0, a0, a1
-; RV64B-NEXT:    ret
-;
-; RV64ZBB-LABEL: orn_i32:
-; RV64ZBB:       # %bb.0:
-; RV64ZBB-NEXT:    orn a0, a0, a1
-; RV64ZBB-NEXT:    ret
-;
-; RV64ZBP-LABEL: orn_i32:
-; RV64ZBP:       # %bb.0:
-; RV64ZBP-NEXT:    orn a0, a0, a1
-; RV64ZBP-NEXT:    ret
-  %neg = xor i32 %b, -1
-  %or = or i32 %neg, %a
-  ret i32 %or
-}
-
-define i64 @orn_i64(i64 %a, i64 %b) nounwind {
-; RV64I-LABEL: orn_i64:
-; RV64I:       # %bb.0:
-; RV64I-NEXT:    not a1, a1
-; RV64I-NEXT:    or a0, a1, a0
-; RV64I-NEXT:    ret
-;
-; RV64B-LABEL: orn_i64:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    orn a0, a0, a1
-; RV64B-NEXT:    ret
-;
-; RV64ZBB-LABEL: orn_i64:
-; RV64ZBB:       # %bb.0:
-; RV64ZBB-NEXT:    orn a0, a0, a1
-; RV64ZBB-NEXT:    ret
-;
-; RV64ZBP-LABEL: orn_i64:
-; RV64ZBP:       # %bb.0:
-; RV64ZBP-NEXT:    orn a0, a0, a1
-; RV64ZBP-NEXT:    ret
-  %neg = xor i64 %b, -1
-  %or = or i64 %neg, %a
-  ret i64 %or
-}
-
-define signext i32 @xnor_i32(i32 signext %a, i32 signext %b) nounwind {
-; RV64I-LABEL: xnor_i32:
-; RV64I:       # %bb.0:
-; RV64I-NEXT:    xor a0, a0, a1
-; RV64I-NEXT:    not a0, a0
-; RV64I-NEXT:    ret
-;
-; RV64B-LABEL: xnor_i32:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    xnor a0, a0, a1
-; RV64B-NEXT:    ret
-;
-; RV64ZBB-LABEL: xnor_i32:
-; RV64ZBB:       # %bb.0:
-; RV64ZBB-NEXT:    xnor a0, a0, a1
-; RV64ZBB-NEXT:    ret
-;
-; RV64ZBP-LABEL: xnor_i32:
-; RV64ZBP:       # %bb.0:
-; RV64ZBP-NEXT:    xnor a0, a0, a1
-; RV64ZBP-NEXT:    ret
-  %neg = xor i32 %a, -1
-  %xor = xor i32 %neg, %b
-  ret i32 %xor
-}
-
-define i64 @xnor_i64(i64 %a, i64 %b) nounwind {
-; RV64I-LABEL: xnor_i64:
-; RV64I:       # %bb.0:
-; RV64I-NEXT:    xor a0, a0, a1
-; RV64I-NEXT:    not a0, a0
-; RV64I-NEXT:    ret
-;
-; RV64B-LABEL: xnor_i64:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    xnor a0, a0, a1
-; RV64B-NEXT:    ret
-;
-; RV64ZBB-LABEL: xnor_i64:
-; RV64ZBB:       # %bb.0:
-; RV64ZBB-NEXT:    xnor a0, a0, a1
-; RV64ZBB-NEXT:    ret
-;
-; RV64ZBP-LABEL: xnor_i64:
-; RV64ZBP:       # %bb.0:
-; RV64ZBP-NEXT:    xnor a0, a0, a1
-; RV64ZBP-NEXT:    ret
-  %neg = xor i64 %a, -1
-  %xor = xor i64 %neg, %b
-  ret i64 %xor
-}
-
-declare i32 @llvm.fshl.i32(i32, i32, i32)
-
-define signext i32 @rol_i32(i32 signext %a, i32 signext %b) nounwind {
-; RV64I-LABEL: rol_i32:
-; RV64I:       # %bb.0:
-; RV64I-NEXT:    sllw a2, a0, a1
-; RV64I-NEXT:    negw a1, a1
-; RV64I-NEXT:    srlw a0, a0, a1
-; RV64I-NEXT:    or a0, a2, a0
-; RV64I-NEXT:    ret
-;
-; RV64B-LABEL: rol_i32:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    rolw a0, a0, a1
-; RV64B-NEXT:    ret
-;
-; RV64ZBB-LABEL: rol_i32:
-; RV64ZBB:       # %bb.0:
-; RV64ZBB-NEXT:    rolw a0, a0, a1
-; RV64ZBB-NEXT:    ret
-;
-; RV64ZBP-LABEL: rol_i32:
-; RV64ZBP:       # %bb.0:
-; RV64ZBP-NEXT:    rolw a0, a0, a1
-; RV64ZBP-NEXT:    ret
-  %1 = tail call i32 @llvm.fshl.i32(i32 %a, i32 %a, i32 %b)
-  ret i32 %1
-}
-
-; Similar to rol_i32, but doesn't sign extend the result.
-define void @rol_i32_nosext(i32 signext %a, i32 signext %b, i32* %x) nounwind {
-; RV64I-LABEL: rol_i32_nosext:
-; RV64I:       # %bb.0:
-; RV64I-NEXT:    sllw a3, a0, a1
-; RV64I-NEXT:    negw a1, a1
-; RV64I-NEXT:    srlw a0, a0, a1
-; RV64I-NEXT:    or a0, a3, a0
-; RV64I-NEXT:    sw a0, 0(a2)
-; RV64I-NEXT:    ret
-;
-; RV64B-LABEL: rol_i32_nosext:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    rolw a0, a0, a1
-; RV64B-NEXT:    sw a0, 0(a2)
-; RV64B-NEXT:    ret
-;
-; RV64ZBB-LABEL: rol_i32_nosext:
-; RV64ZBB:       # %bb.0:
-; RV64ZBB-NEXT:    rolw a0, a0, a1
-; RV64ZBB-NEXT:    sw a0, 0(a2)
-; RV64ZBB-NEXT:    ret
-;
-; RV64ZBP-LABEL: rol_i32_nosext:
-; RV64ZBP:       # %bb.0:
-; RV64ZBP-NEXT:    rolw a0, a0, a1
-; RV64ZBP-NEXT:    sw a0, 0(a2)
-; RV64ZBP-NEXT:    ret
-  %1 = tail call i32 @llvm.fshl.i32(i32 %a, i32 %a, i32 %b)
-  store i32 %1, i32* %x
-  ret void
-}
-
-define signext i32 @rol_i32_neg_constant_rhs(i32 signext %a) nounwind {
-; RV64I-LABEL: rol_i32_neg_constant_rhs:
-; RV64I:       # %bb.0:
-; RV64I-NEXT:    addi a1, zero, -2
-; RV64I-NEXT:    sllw a2, a1, a0
-; RV64I-NEXT:    negw a0, a0
-; RV64I-NEXT:    srlw a0, a1, a0
-; RV64I-NEXT:    or a0, a2, a0
-; RV64I-NEXT:    ret
-;
-; RV64B-LABEL: rol_i32_neg_constant_rhs:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    addi a1, zero, -2
-; RV64B-NEXT:    rolw a0, a1, a0
-; RV64B-NEXT:    ret
-;
-; RV64ZBB-LABEL: rol_i32_neg_constant_rhs:
-; RV64ZBB:       # %bb.0:
-; RV64ZBB-NEXT:    addi a1, zero, -2
-; RV64ZBB-NEXT:    rolw a0, a1, a0
-; RV64ZBB-NEXT:    ret
-;
-; RV64ZBP-LABEL: rol_i32_neg_constant_rhs:
-; RV64ZBP:       # %bb.0:
-; RV64ZBP-NEXT:    addi a1, zero, -2
-; RV64ZBP-NEXT:    rolw a0, a1, a0
-; RV64ZBP-NEXT:    ret
-  %1 = tail call i32 @llvm.fshl.i32(i32 -2, i32 -2, i32 %a)
-  ret i32 %1
-}
-
-declare i64 @llvm.fshl.i64(i64, i64, i64)
-
-define i64 @rol_i64(i64 %a, i64 %b) nounwind {
-; RV64I-LABEL: rol_i64:
-; RV64I:       # %bb.0:
-; RV64I-NEXT:    sll a2, a0, a1
-; RV64I-NEXT:    neg a1, a1
-; RV64I-NEXT:    srl a0, a0, a1
-; RV64I-NEXT:    or a0, a2, a0
-; RV64I-NEXT:    ret
-;
-; RV64B-LABEL: rol_i64:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    rol a0, a0, a1
-; RV64B-NEXT:    ret
-;
-; RV64ZBB-LABEL: rol_i64:
-; RV64ZBB:       # %bb.0:
-; RV64ZBB-NEXT:    rol a0, a0, a1
-; RV64ZBB-NEXT:    ret
-;
-; RV64ZBP-LABEL: rol_i64:
-; RV64ZBP:       # %bb.0:
-; RV64ZBP-NEXT:    rol a0, a0, a1
-; RV64ZBP-NEXT:    ret
-  %or = tail call i64 @llvm.fshl.i64(i64 %a, i64 %a, i64 %b)
-  ret i64 %or
-}
-
-declare i32 @llvm.fshr.i32(i32, i32, i32)
-
-define signext i32 @ror_i32(i32 signext %a, i32 signext %b) nounwind {
-; RV64I-LABEL: ror_i32:
-; RV64I:       # %bb.0:
-; RV64I-NEXT:    srlw a2, a0, a1
-; RV64I-NEXT:    negw a1, a1
-; RV64I-NEXT:    sllw a0, a0, a1
-; RV64I-NEXT:    or a0, a2, a0
-; RV64I-NEXT:    ret
-;
-; RV64B-LABEL: ror_i32:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    rorw a0, a0, a1
-; RV64B-NEXT:    ret
-;
-; RV64ZBB-LABEL: ror_i32:
-; RV64ZBB:       # %bb.0:
-; RV64ZBB-NEXT:    rorw a0, a0, a1
-; RV64ZBB-NEXT:    ret
-;
-; RV64ZBP-LABEL: ror_i32:
-; RV64ZBP:       # %bb.0:
-; RV64ZBP-NEXT:    rorw a0, a0, a1
-; RV64ZBP-NEXT:    ret
-  %1 = tail call i32 @llvm.fshr.i32(i32 %a, i32 %a, i32 %b)
-  ret i32 %1
-}
-
-; Similar to ror_i32, but doesn't sign extend the result.
-define void @ror_i32_nosext(i32 signext %a, i32 signext %b, i32* %x) nounwind {
-; RV64I-LABEL: ror_i32_nosext:
-; RV64I:       # %bb.0:
-; RV64I-NEXT:    srlw a3, a0, a1
-; RV64I-NEXT:    negw a1, a1
-; RV64I-NEXT:    sllw a0, a0, a1
-; RV64I-NEXT:    or a0, a3, a0
-; RV64I-NEXT:    sw a0, 0(a2)
-; RV64I-NEXT:    ret
-;
-; RV64B-LABEL: ror_i32_nosext:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    rorw a0, a0, a1
-; RV64B-NEXT:    sw a0, 0(a2)
-; RV64B-NEXT:    ret
-;
-; RV64ZBB-LABEL: ror_i32_nosext:
-; RV64ZBB:       # %bb.0:
-; RV64ZBB-NEXT:    rorw a0, a0, a1
-; RV64ZBB-NEXT:    sw a0, 0(a2)
-; RV64ZBB-NEXT:    ret
-;
-; RV64ZBP-LABEL: ror_i32_nosext:
-; RV64ZBP:       # %bb.0:
-; RV64ZBP-NEXT:    rorw a0, a0, a1
-; RV64ZBP-NEXT:    sw a0, 0(a2)
-; RV64ZBP-NEXT:    ret
-  %1 = tail call i32 @llvm.fshr.i32(i32 %a, i32 %a, i32 %b)
-  store i32 %1, i32* %x
-  ret void
-}
-
-define signext i32 @ror_i32_neg_constant_rhs(i32 signext %a) nounwind {
-; RV64I-LABEL: ror_i32_neg_constant_rhs:
-; RV64I:       # %bb.0:
-; RV64I-NEXT:    addi a1, zero, -2
-; RV64I-NEXT:    srlw a2, a1, a0
-; RV64I-NEXT:    negw a0, a0
-; RV64I-NEXT:    sllw a0, a1, a0
-; RV64I-NEXT:    or a0, a2, a0
-; RV64I-NEXT:    ret
-;
-; RV64B-LABEL: ror_i32_neg_constant_rhs:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    addi a1, zero, -2
-; RV64B-NEXT:    rorw a0, a1, a0
-; RV64B-NEXT:    ret
-;
-; RV64ZBB-LABEL: ror_i32_neg_constant_rhs:
-; RV64ZBB:       # %bb.0:
-; RV64ZBB-NEXT:    addi a1, zero, -2
-; RV64ZBB-NEXT:    rorw a0, a1, a0
-; RV64ZBB-NEXT:    ret
-;
-; RV64ZBP-LABEL: ror_i32_neg_constant_rhs:
-; RV64ZBP:       # %bb.0:
-; RV64ZBP-NEXT:    addi a1, zero, -2
-; RV64ZBP-NEXT:    rorw a0, a1, a0
-; RV64ZBP-NEXT:    ret
-  %1 = tail call i32 @llvm.fshr.i32(i32 -2, i32 -2, i32 %a)
-  ret i32 %1
-}
-
-declare i64 @llvm.fshr.i64(i64, i64, i64)
-
-define i64 @ror_i64(i64 %a, i64 %b) nounwind {
-; RV64I-LABEL: ror_i64:
-; RV64I:       # %bb.0:
-; RV64I-NEXT:    srl a2, a0, a1
-; RV64I-NEXT:    neg a1, a1
-; RV64I-NEXT:    sll a0, a0, a1
-; RV64I-NEXT:    or a0, a2, a0
-; RV64I-NEXT:    ret
-;
-; RV64B-LABEL: ror_i64:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    ror a0, a0, a1
-; RV64B-NEXT:    ret
-;
-; RV64ZBB-LABEL: ror_i64:
-; RV64ZBB:       # %bb.0:
-; RV64ZBB-NEXT:    ror a0, a0, a1
-; RV64ZBB-NEXT:    ret
-;
-; RV64ZBP-LABEL: ror_i64:
-; RV64ZBP:       # %bb.0:
-; RV64ZBP-NEXT:    ror a0, a0, a1
-; RV64ZBP-NEXT:    ret
-  %or = tail call i64 @llvm.fshr.i64(i64 %a, i64 %a, i64 %b)
-  ret i64 %or
-}
-
-define signext i32 @rori_i32_fshl(i32 signext %a) nounwind {
-; RV64I-LABEL: rori_i32_fshl:
-; RV64I:       # %bb.0:
-; RV64I-NEXT:    srliw a1, a0, 1
-; RV64I-NEXT:    slliw a0, a0, 31
-; RV64I-NEXT:    or a0, a0, a1
-; RV64I-NEXT:    ret
-;
-; RV64B-LABEL: rori_i32_fshl:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    roriw a0, a0, 1
-; RV64B-NEXT:    ret
-;
-; RV64ZBB-LABEL: rori_i32_fshl:
-; RV64ZBB:       # %bb.0:
-; RV64ZBB-NEXT:    roriw a0, a0, 1
-; RV64ZBB-NEXT:    ret
-;
-; RV64ZBP-LABEL: rori_i32_fshl:
-; RV64ZBP:       # %bb.0:
-; RV64ZBP-NEXT:    roriw a0, a0, 1
-; RV64ZBP-NEXT:    ret
-  %1 = tail call i32 @llvm.fshl.i32(i32 %a, i32 %a, i32 31)
-  ret i32 %1
-}
-
-; Similar to rori_i32_fshl, but doesn't sign extend the result.
-define void @rori_i32_fshl_nosext(i32 signext %a, i32* %x) nounwind {
-; RV64I-LABEL: rori_i32_fshl_nosext:
-; RV64I:       # %bb.0:
-; RV64I-NEXT:    srliw a2, a0, 1
-; RV64I-NEXT:    slli a0, a0, 31
-; RV64I-NEXT:    or a0, a0, a2
-; RV64I-NEXT:    sw a0, 0(a1)
-; RV64I-NEXT:    ret
-;
-; RV64B-LABEL: rori_i32_fshl_nosext:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    roriw a0, a0, 1
-; RV64B-NEXT:    sw a0, 0(a1)
-; RV64B-NEXT:    ret
-;
-; RV64ZBB-LABEL: rori_i32_fshl_nosext:
-; RV64ZBB:       # %bb.0:
-; RV64ZBB-NEXT:    roriw a0, a0, 1
-; RV64ZBB-NEXT:    sw a0, 0(a1)
-; RV64ZBB-NEXT:    ret
-;
-; RV64ZBP-LABEL: rori_i32_fshl_nosext:
-; RV64ZBP:       # %bb.0:
-; RV64ZBP-NEXT:    roriw a0, a0, 1
-; RV64ZBP-NEXT:    sw a0, 0(a1)
-; RV64ZBP-NEXT:    ret
-  %1 = tail call i32 @llvm.fshl.i32(i32 %a, i32 %a, i32 31)
-  store i32 %1, i32* %x
-  ret void
-}
-
-define signext i32 @rori_i32_fshr(i32 signext %a) nounwind {
-; RV64I-LABEL: rori_i32_fshr:
-; RV64I:       # %bb.0:
-; RV64I-NEXT:    slliw a1, a0, 1
-; RV64I-NEXT:    srliw a0, a0, 31
-; RV64I-NEXT:    or a0, a0, a1
-; RV64I-NEXT:    ret
-;
-; RV64B-LABEL: rori_i32_fshr:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    roriw a0, a0, 31
-; RV64B-NEXT:    ret
-;
-; RV64ZBB-LABEL: rori_i32_fshr:
-; RV64ZBB:       # %bb.0:
-; RV64ZBB-NEXT:    roriw a0, a0, 31
-; RV64ZBB-NEXT:    ret
-;
-; RV64ZBP-LABEL: rori_i32_fshr:
-; RV64ZBP:       # %bb.0:
-; RV64ZBP-NEXT:    roriw a0, a0, 31
-; RV64ZBP-NEXT:    ret
-  %1 = tail call i32 @llvm.fshr.i32(i32 %a, i32 %a, i32 31)
-  ret i32 %1
-}
-
-; Similar to rori_i32_fshr, but doesn't sign extend the result.
-define void @rori_i32_fshr_nosext(i32 signext %a, i32* %x) nounwind {
-; RV64I-LABEL: rori_i32_fshr_nosext:
-; RV64I:       # %bb.0:
-; RV64I-NEXT:    slli a2, a0, 1
-; RV64I-NEXT:    srliw a0, a0, 31
-; RV64I-NEXT:    or a0, a0, a2
-; RV64I-NEXT:    sw a0, 0(a1)
-; RV64I-NEXT:    ret
-;
-; RV64B-LABEL: rori_i32_fshr_nosext:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    roriw a0, a0, 31
-; RV64B-NEXT:    sw a0, 0(a1)
-; RV64B-NEXT:    ret
-;
-; RV64ZBB-LABEL: rori_i32_fshr_nosext:
-; RV64ZBB:       # %bb.0:
-; RV64ZBB-NEXT:    roriw a0, a0, 31
-; RV64ZBB-NEXT:    sw a0, 0(a1)
-; RV64ZBB-NEXT:    ret
-;
-; RV64ZBP-LABEL: rori_i32_fshr_nosext:
-; RV64ZBP:       # %bb.0:
-; RV64ZBP-NEXT:    roriw a0, a0, 31
-; RV64ZBP-NEXT:    sw a0, 0(a1)
-; RV64ZBP-NEXT:    ret
-  %1 = tail call i32 @llvm.fshr.i32(i32 %a, i32 %a, i32 31)
-  store i32 %1, i32* %x
-  ret void
-}
-
-; This test is similar to the type legalized version of the fshl/fshr tests, but
-; instead of having the same input to both shifts it has different inputs. Make
-; sure we don't match it as a roriw.
-define signext i32 @not_rori_i32(i32 signext %x, i32 signext %y) nounwind {
-; RV64I-LABEL: not_rori_i32:
-; RV64I:       # %bb.0:
-; RV64I-NEXT:    slliw a0, a0, 31
-; RV64I-NEXT:    srliw a1, a1, 1
-; RV64I-NEXT:    or a0, a0, a1
-; RV64I-NEXT:    ret
-;
-; RV64B-LABEL: not_rori_i32:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    slliw a0, a0, 31
-; RV64B-NEXT:    srliw a1, a1, 1
-; RV64B-NEXT:    or a0, a0, a1
-; RV64B-NEXT:    ret
-;
-; RV64ZBB-LABEL: not_rori_i32:
-; RV64ZBB:       # %bb.0:
-; RV64ZBB-NEXT:    slliw a0, a0, 31
-; RV64ZBB-NEXT:    srliw a1, a1, 1
-; RV64ZBB-NEXT:    or a0, a0, a1
-; RV64ZBB-NEXT:    ret
-;
-; RV64ZBP-LABEL: not_rori_i32:
-; RV64ZBP:       # %bb.0:
-; RV64ZBP-NEXT:    slliw a0, a0, 31
-; RV64ZBP-NEXT:    srliw a1, a1, 1
-; RV64ZBP-NEXT:    or a0, a0, a1
-; RV64ZBP-NEXT:    ret
-  %a = shl i32 %x, 31
-  %b = lshr i32 %y, 1
-  %c = or i32 %a, %b
-  ret i32 %c
-}
-
-; This is similar to the type legalized roriw pattern, but the and mask is more
-; than 32 bits so the lshr doesn't shift zeroes into the lower 32 bits. Make
-; sure we don't match it to roriw.
-define i64 @roriw_bug(i64 %x) nounwind {
-; RV64I-LABEL: roriw_bug:
-; RV64I:       # %bb.0:
-; RV64I-NEXT:    slli a1, a0, 31
-; RV64I-NEXT:    andi a0, a0, -2
-; RV64I-NEXT:    srli a2, a0, 1
-; RV64I-NEXT:    or a1, a1, a2
-; RV64I-NEXT:    sext.w a1, a1
-; RV64I-NEXT:    xor a0, a0, a1
-; RV64I-NEXT:    ret
-;
-; RV64B-LABEL: roriw_bug:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    slli a1, a0, 31
-; RV64B-NEXT:    andi a0, a0, -2
-; RV64B-NEXT:    srli a2, a0, 1
-; RV64B-NEXT:    or a1, a1, a2
-; RV64B-NEXT:    sext.w a1, a1
-; RV64B-NEXT:    xor a0, a0, a1
-; RV64B-NEXT:    ret
-;
-; RV64ZBB-LABEL: roriw_bug:
-; RV64ZBB:       # %bb.0:
-; RV64ZBB-NEXT:    slli a1, a0, 31
-; RV64ZBB-NEXT:    andi a0, a0, -2
-; RV64ZBB-NEXT:    srli a2, a0, 1
-; RV64ZBB-NEXT:    or a1, a1, a2
-; RV64ZBB-NEXT:    sext.w a1, a1
-; RV64ZBB-NEXT:    xor a0, a0, a1
-; RV64ZBB-NEXT:    ret
-;
-; RV64ZBP-LABEL: roriw_bug:
-; RV64ZBP:       # %bb.0:
-; RV64ZBP-NEXT:    slli a1, a0, 31
-; RV64ZBP-NEXT:    andi a0, a0, -2
-; RV64ZBP-NEXT:    srli a2, a0, 1
-; RV64ZBP-NEXT:    or a1, a1, a2
-; RV64ZBP-NEXT:    sext.w a1, a1
-; RV64ZBP-NEXT:    xor a0, a0, a1
-; RV64ZBP-NEXT:    ret
-  %a = shl i64 %x, 31
-  %b = and i64 %x, 18446744073709551614
-  %c = lshr i64 %b, 1
-  %d = or i64 %a, %c
-  %e = shl i64 %d, 32
-  %f = ashr i64 %e, 32
-  %g = xor i64 %b, %f ; to increase the use count on %b to disable SimplifyDemandedBits.
-  ret i64 %g
-}
-
-define i64 @rori_i64_fshl(i64 %a) nounwind {
-; RV64I-LABEL: rori_i64_fshl:
-; RV64I:       # %bb.0:
-; RV64I-NEXT:    srli a1, a0, 1
-; RV64I-NEXT:    slli a0, a0, 63
-; RV64I-NEXT:    or a0, a0, a1
-; RV64I-NEXT:    ret
-;
-; RV64B-LABEL: rori_i64_fshl:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    rori a0, a0, 1
-; RV64B-NEXT:    ret
-;
-; RV64ZBB-LABEL: rori_i64_fshl:
-; RV64ZBB:       # %bb.0:
-; RV64ZBB-NEXT:    rori a0, a0, 1
-; RV64ZBB-NEXT:    ret
-;
-; RV64ZBP-LABEL: rori_i64_fshl:
-; RV64ZBP:       # %bb.0:
-; RV64ZBP-NEXT:    rori a0, a0, 1
-; RV64ZBP-NEXT:    ret
-  %1 = tail call i64 @llvm.fshl.i64(i64 %a, i64 %a, i64 63)
-  ret i64 %1
-}
-
-define i64 @rori_i64_fshr(i64 %a) nounwind {
-; RV64I-LABEL: rori_i64_fshr:
-; RV64I:       # %bb.0:
-; RV64I-NEXT:    slli a1, a0, 1
-; RV64I-NEXT:    srli a0, a0, 63
-; RV64I-NEXT:    or a0, a0, a1
-; RV64I-NEXT:    ret
-;
-; RV64B-LABEL: rori_i64_fshr:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    rori a0, a0, 63
-; RV64B-NEXT:    ret
-;
-; RV64ZBB-LABEL: rori_i64_fshr:
-; RV64ZBB:       # %bb.0:
-; RV64ZBB-NEXT:    rori a0, a0, 63
-; RV64ZBB-NEXT:    ret
-;
-; RV64ZBP-LABEL: rori_i64_fshr:
-; RV64ZBP:       # %bb.0:
-; RV64ZBP-NEXT:    rori a0, a0, 63
-; RV64ZBP-NEXT:    ret
-  %1 = tail call i64 @llvm.fshr.i64(i64 %a, i64 %a, i64 63)
-  ret i64 %1
-}
-
-define i8 @srli_i8(i8 %a) nounwind {
-; RV64I-LABEL: srli_i8:
-; RV64I:       # %bb.0:
-; RV64I-NEXT:    andi a0, a0, 192
-; RV64I-NEXT:    srli a0, a0, 6
-; RV64I-NEXT:    ret
-;
-; RV64B-LABEL: srli_i8:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    andi a0, a0, 192
-; RV64B-NEXT:    srli a0, a0, 6
-; RV64B-NEXT:    ret
-;
-; RV64ZBB-LABEL: srli_i8:
-; RV64ZBB:       # %bb.0:
-; RV64ZBB-NEXT:    andi a0, a0, 192
-; RV64ZBB-NEXT:    srli a0, a0, 6
-; RV64ZBB-NEXT:    ret
-;
-; RV64ZBP-LABEL: srli_i8:
-; RV64ZBP:       # %bb.0:
-; RV64ZBP-NEXT:    andi a0, a0, 192
-; RV64ZBP-NEXT:    srli a0, a0, 6
-; RV64ZBP-NEXT:    ret
-  %1 = lshr i8 %a, 6
-  ret i8 %1
-}
-
-define i8 @srai_i8(i8 %a) nounwind {
-; RV64I-LABEL: srai_i8:
-; RV64I:       # %bb.0:
-; RV64I-NEXT:    slli a0, a0, 56
-; RV64I-NEXT:    srai a0, a0, 61
-; RV64I-NEXT:    ret
-;
-; RV64B-LABEL: srai_i8:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    sext.b a0, a0
-; RV64B-NEXT:    srai a0, a0, 5
-; RV64B-NEXT:    ret
-;
-; RV64ZBB-LABEL: srai_i8:
-; RV64ZBB:       # %bb.0:
-; RV64ZBB-NEXT:    sext.b a0, a0
-; RV64ZBB-NEXT:    srai a0, a0, 5
-; RV64ZBB-NEXT:    ret
-;
-; RV64ZBP-LABEL: srai_i8:
-; RV64ZBP:       # %bb.0:
-; RV64ZBP-NEXT:    slli a0, a0, 56
-; RV64ZBP-NEXT:    srai a0, a0, 61
-; RV64ZBP-NEXT:    ret
-  %1 = ashr i8 %a, 5
-  ret i8 %1
-}
-
-define i16 @srli_i16(i16 %a) nounwind {
-; RV64I-LABEL: srli_i16:
-; RV64I:       # %bb.0:
-; RV64I-NEXT:    slli a0, a0, 48
-; RV64I-NEXT:    srli a0, a0, 54
-; RV64I-NEXT:    ret
-;
-; RV64B-LABEL: srli_i16:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    zext.h a0, a0
-; RV64B-NEXT:    srli a0, a0, 6
-; RV64B-NEXT:    ret
-;
-; RV64ZBB-LABEL: srli_i16:
-; RV64ZBB:       # %bb.0:
-; RV64ZBB-NEXT:    zext.h a0, a0
-; RV64ZBB-NEXT:    srli a0, a0, 6
-; RV64ZBB-NEXT:    ret
-;
-; RV64ZBP-LABEL: srli_i16:
-; RV64ZBP:       # %bb.0:
-; RV64ZBP-NEXT:    zext.h a0, a0
-; RV64ZBP-NEXT:    srli a0, a0, 6
-; RV64ZBP-NEXT:    ret
-  %1 = lshr i16 %a, 6
-  ret i16 %1
-}
-
-define i16 @srai_i16(i16 %a) nounwind {
-; RV64I-LABEL: srai_i16:
-; RV64I:       # %bb.0:
-; RV64I-NEXT:    slli a0, a0, 48
-; RV64I-NEXT:    srai a0, a0, 57
-; RV64I-NEXT:    ret
-;
-; RV64B-LABEL: srai_i16:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    sext.h a0, a0
-; RV64B-NEXT:    srai a0, a0, 9
-; RV64B-NEXT:    ret
-;
-; RV64ZBB-LABEL: srai_i16:
-; RV64ZBB:       # %bb.0:
-; RV64ZBB-NEXT:    sext.h a0, a0
-; RV64ZBB-NEXT:    srai a0, a0, 9
-; RV64ZBB-NEXT:    ret
-;
-; RV64ZBP-LABEL: srai_i16:
-; RV64ZBP:       # %bb.0:
-; RV64ZBP-NEXT:    slli a0, a0, 48
-; RV64ZBP-NEXT:    srai a0, a0, 57
-; RV64ZBP-NEXT:    ret
-  %1 = ashr i16 %a, 9
-  ret i16 %1
-}
diff --git a/llvm/test/CodeGen/RISCV/rv64zbe-intrinsic.ll b/llvm/test/CodeGen/RISCV/rv64zbe-intrinsic.ll
deleted file mode 100644
index 2c6b98fe8abd..000000000000
--- a/llvm/test/CodeGen/RISCV/rv64zbe-intrinsic.ll
+++ /dev/null
@@ -1,109 +0,0 @@
-; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
-; RUN: llc -mtriple=riscv64 -mattr=+experimental-b -verify-machineinstrs < %s \
-; RUN:   | FileCheck %s -check-prefix=RV64B
-; RUN: llc -mtriple=riscv64 -mattr=+experimental-zbe -verify-machineinstrs < %s \
-; RUN:   | FileCheck %s -check-prefix=RV64ZBE
-
-declare i32 @llvm.riscv.bcompress.i32(i32 %a, i32 %b)
-
-define signext i32 @bcompress32(i32 signext %a, i32 signext %b) nounwind {
-; RV64B-LABEL: bcompress32:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    bcompressw a0, a0, a1
-; RV64B-NEXT:    ret
-;
-; RV64ZBE-LABEL: bcompress32:
-; RV64ZBE:       # %bb.0:
-; RV64ZBE-NEXT:    bcompressw a0, a0, a1
-; RV64ZBE-NEXT:    ret
-  %tmp = call i32 @llvm.riscv.bcompress.i32(i32 %a, i32 %b)
-  ret i32 %tmp
-}
-
-define signext i32 @bcompress32_demandedbits(i32 signext %a, i32 signext %b, i32 signext %c, i32 signext %d) nounwind {
-; RV64B-LABEL: bcompress32_demandedbits:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    add a0, a0, a1
-; RV64B-NEXT:    add a1, a2, a3
-; RV64B-NEXT:    bcompressw a0, a0, a1
-; RV64B-NEXT:    ret
-;
-; RV64ZBE-LABEL: bcompress32_demandedbits:
-; RV64ZBE:       # %bb.0:
-; RV64ZBE-NEXT:    add a0, a0, a1
-; RV64ZBE-NEXT:    add a1, a2, a3
-; RV64ZBE-NEXT:    bcompressw a0, a0, a1
-; RV64ZBE-NEXT:    ret
-  %e = add i32 %a, %b
-  %f = add i32 %c, %d
-  %tmp = call i32 @llvm.riscv.bcompress.i32(i32 %e, i32 %f)
-  ret i32 %tmp
-}
-
-declare i32 @llvm.riscv.bdecompress.i32(i32 %a, i32 %b)
-
-define signext i32 @bdecompress32(i32 signext %a, i32 signext %b) nounwind {
-; RV64B-LABEL: bdecompress32:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    bdecompressw a0, a0, a1
-; RV64B-NEXT:    ret
-;
-; RV64ZBE-LABEL: bdecompress32:
-; RV64ZBE:       # %bb.0:
-; RV64ZBE-NEXT:    bdecompressw a0, a0, a1
-; RV64ZBE-NEXT:    ret
-  %tmp = call i32 @llvm.riscv.bdecompress.i32(i32 %a, i32 %b)
-  ret i32 %tmp
-}
-
-define signext i32 @bdecompress32_demandedbits(i32 signext %a, i32 signext %b, i32 signext %c, i32 signext %d) nounwind {
-; RV64B-LABEL: bdecompress32_demandedbits:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    add a0, a0, a1
-; RV64B-NEXT:    add a1, a2, a3
-; RV64B-NEXT:    bdecompressw a0, a0, a1
-; RV64B-NEXT:    ret
-;
-; RV64ZBE-LABEL: bdecompress32_demandedbits:
-; RV64ZBE:       # %bb.0:
-; RV64ZBE-NEXT:    add a0, a0, a1
-; RV64ZBE-NEXT:    add a1, a2, a3
-; RV64ZBE-NEXT:    bdecompressw a0, a0, a1
-; RV64ZBE-NEXT:    ret
-  %e = add i32 %a, %b
-  %f = add i32 %c, %d
-  %tmp = call i32 @llvm.riscv.bdecompress.i32(i32 %e, i32 %f)
-  ret i32 %tmp
-}
-
-declare i64 @llvm.riscv.bcompress.i64(i64 %a, i64 %b)
-
-define i64 @bcompress64(i64 %a, i64 %b) nounwind {
-; RV64B-LABEL: bcompress64:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    bcompress a0, a0, a1
-; RV64B-NEXT:    ret
-;
-; RV64ZBE-LABEL: bcompress64:
-; RV64ZBE:       # %bb.0:
-; RV64ZBE-NEXT:    bcompress a0, a0, a1
-; RV64ZBE-NEXT:    ret
-  %tmp = call i64 @llvm.riscv.bcompress.i64(i64 %a, i64 %b)
-  ret i64 %tmp
-}
-
-declare i64 @llvm.riscv.bdecompress.i64(i64 %a, i64 %b)
-
-define i64 @bdecompress64(i64 %a, i64 %b) nounwind {
-; RV64B-LABEL: bdecompress64:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    bdecompress a0, a0, a1
-; RV64B-NEXT:    ret
-;
-; RV64ZBE-LABEL: bdecompress64:
-; RV64ZBE:       # %bb.0:
-; RV64ZBE-NEXT:    bdecompress a0, a0, a1
-; RV64ZBE-NEXT:    ret
-  %tmp = call i64 @llvm.riscv.bdecompress.i64(i64 %a, i64 %b)
-  ret i64 %tmp
-}
diff --git a/llvm/test/CodeGen/RISCV/rv64zbp-intrinsic.ll b/llvm/test/CodeGen/RISCV/rv64zbp-intrinsic.ll
deleted file mode 100644
index c9175b696302..000000000000
--- a/llvm/test/CodeGen/RISCV/rv64zbp-intrinsic.ll
+++ /dev/null
@@ -1,445 +0,0 @@
-; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
-; RUN: llc -mtriple=riscv64 -mattr=+experimental-b -verify-machineinstrs < %s \
-; RUN:   | FileCheck %s -check-prefix=RV64B
-; RUN: llc -mtriple=riscv64 -mattr=+experimental-zbp -verify-machineinstrs < %s \
-; RUN:   | FileCheck %s -check-prefix=RV64ZBP
-
-declare i32 @llvm.riscv.grev.i32(i32 %a, i32 %b)
-
-define signext i32 @grev32(i32 signext %a, i32 signext %b) nounwind {
-; RV64B-LABEL: grev32:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    grevw a0, a0, a1
-; RV64B-NEXT:    ret
-;
-; RV64ZBP-LABEL: grev32:
-; RV64ZBP:       # %bb.0:
-; RV64ZBP-NEXT:    grevw a0, a0, a1
-; RV64ZBP-NEXT:    ret
-  %tmp = call i32 @llvm.riscv.grev.i32(i32 %a, i32 %b)
- ret i32 %tmp
-}
-
-define signext i32 @grev32_demandedbits(i32 signext %a, i32 signext %b, i32 signext %c) nounwind {
-; RV64B-LABEL: grev32_demandedbits:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    add a0, a0, a1
-; RV64B-NEXT:    grevw a0, a0, a2
-; RV64B-NEXT:    ret
-;
-; RV64ZBP-LABEL: grev32_demandedbits:
-; RV64ZBP:       # %bb.0:
-; RV64ZBP-NEXT:    add a0, a0, a1
-; RV64ZBP-NEXT:    grevw a0, a0, a2
-; RV64ZBP-NEXT:    ret
-  %d = add i32 %a, %b
-  %e = and i32 %c, 31
-  %tmp = call i32 @llvm.riscv.grev.i32(i32 %d, i32 %e)
-  ret i32 %tmp
-}
-
-declare i32 @llvm.riscv.grevi.i32(i32 %a)
-
-define signext i32 @grevi32(i32 signext %a) nounwind {
-; RV64B-LABEL: grevi32:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    greviw a0, a0, 13
-; RV64B-NEXT:    ret
-;
-; RV64ZBP-LABEL: grevi32:
-; RV64ZBP:       # %bb.0:
-; RV64ZBP-NEXT:    greviw a0, a0, 13
-; RV64ZBP-NEXT:    ret
-  %tmp = call i32 @llvm.riscv.grev.i32(i32 %a, i32 13)
- ret i32 %tmp
-}
-
-declare i32 @llvm.riscv.gorc.i32(i32 %a, i32 %b)
-
-define signext i32 @gorc32(i32 signext %a, i32 signext %b) nounwind {
-; RV64B-LABEL: gorc32:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    gorcw a0, a0, a1
-; RV64B-NEXT:    ret
-;
-; RV64ZBP-LABEL: gorc32:
-; RV64ZBP:       # %bb.0:
-; RV64ZBP-NEXT:    gorcw a0, a0, a1
-; RV64ZBP-NEXT:    ret
-  %tmp = call i32 @llvm.riscv.gorc.i32(i32 %a, i32 %b)
- ret i32 %tmp
-}
-
-define signext i32 @gorc32_demandedbits(i32 signext %a, i32 signext %b, i32 signext %c) nounwind {
-; RV64B-LABEL: gorc32_demandedbits:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    add a0, a0, a1
-; RV64B-NEXT:    gorcw a0, a0, a2
-; RV64B-NEXT:    ret
-;
-; RV64ZBP-LABEL: gorc32_demandedbits:
-; RV64ZBP:       # %bb.0:
-; RV64ZBP-NEXT:    add a0, a0, a1
-; RV64ZBP-NEXT:    gorcw a0, a0, a2
-; RV64ZBP-NEXT:    ret
-  %d = add i32 %a, %b
-  %e = and i32 %c, 31
-  %tmp = call i32 @llvm.riscv.gorc.i32(i32 %d, i32 %e)
-  ret i32 %tmp
-}
-
-define signext i32 @gorci32(i32 signext %a) nounwind {
-; RV64B-LABEL: gorci32:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    gorciw a0, a0, 13
-; RV64B-NEXT:    ret
-;
-; RV64ZBP-LABEL: gorci32:
-; RV64ZBP:       # %bb.0:
-; RV64ZBP-NEXT:    gorciw a0, a0, 13
-; RV64ZBP-NEXT:    ret
-  %tmp = call i32 @llvm.riscv.gorc.i32(i32 %a, i32 13)
- ret i32 %tmp
-}
-
-declare i32 @llvm.riscv.shfl.i32(i32 %a, i32 %b)
-
-define signext i32 @shfl32(i32 signext %a, i32 signext %b) nounwind {
-; RV64B-LABEL: shfl32:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    shflw a0, a0, a1
-; RV64B-NEXT:    ret
-;
-; RV64ZBP-LABEL: shfl32:
-; RV64ZBP:       # %bb.0:
-; RV64ZBP-NEXT:    shflw a0, a0, a1
-; RV64ZBP-NEXT:    ret
-  %tmp = call i32 @llvm.riscv.shfl.i32(i32 %a, i32 %b)
- ret i32 %tmp
-}
-
-define signext i32 @shfl32_demandedbits(i32 signext %a, i32 signext %b, i32 signext %c) nounwind {
-; RV64B-LABEL: shfl32_demandedbits:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    add a0, a0, a1
-; RV64B-NEXT:    shflw a0, a0, a2
-; RV64B-NEXT:    ret
-;
-; RV64ZBP-LABEL: shfl32_demandedbits:
-; RV64ZBP:       # %bb.0:
-; RV64ZBP-NEXT:    add a0, a0, a1
-; RV64ZBP-NEXT:    shflw a0, a0, a2
-; RV64ZBP-NEXT:    ret
-  %d = add i32 %a, %b
-  %e = and i32 %c, 15
-  %tmp = call i32 @llvm.riscv.shfl.i32(i32 %d, i32 %e)
-  ret i32 %tmp
-}
-
-define signext i32 @shfli32(i32 signext %a) nounwind {
-; RV64B-LABEL: shfli32:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    shfli a0, a0, 13
-; RV64B-NEXT:    ret
-;
-; RV64ZBP-LABEL: shfli32:
-; RV64ZBP:       # %bb.0:
-; RV64ZBP-NEXT:    shfli a0, a0, 13
-; RV64ZBP-NEXT:    ret
-  %tmp = call i32 @llvm.riscv.shfl.i32(i32 %a, i32 13)
- ret i32 %tmp
-}
-
-declare i32 @llvm.riscv.unshfl.i32(i32 %a, i32 %b)
-
-define signext i32 @unshfl32(i32 signext %a, i32 signext %b) nounwind {
-; RV64B-LABEL: unshfl32:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    unshflw a0, a0, a1
-; RV64B-NEXT:    ret
-;
-; RV64ZBP-LABEL: unshfl32:
-; RV64ZBP:       # %bb.0:
-; RV64ZBP-NEXT:    unshflw a0, a0, a1
-; RV64ZBP-NEXT:    ret
-  %tmp = call i32 @llvm.riscv.unshfl.i32(i32 %a, i32 %b)
- ret i32 %tmp
-}
-
-define signext i32 @unshfl32_demandedbits(i32 signext %a, i32 signext %b, i32 signext %c) nounwind {
-; RV64B-LABEL: unshfl32_demandedbits:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    add a0, a0, a1
-; RV64B-NEXT:    unshflw a0, a0, a2
-; RV64B-NEXT:    ret
-;
-; RV64ZBP-LABEL: unshfl32_demandedbits:
-; RV64ZBP:       # %bb.0:
-; RV64ZBP-NEXT:    add a0, a0, a1
-; RV64ZBP-NEXT:    unshflw a0, a0, a2
-; RV64ZBP-NEXT:    ret
-  %d = add i32 %a, %b
-  %e = and i32 %c, 15
-  %tmp = call i32 @llvm.riscv.unshfl.i32(i32 %d, i32 %e)
-  ret i32 %tmp
-}
-
-define signext i32 @unshfli32(i32 signext %a) nounwind {
-; RV64B-LABEL: unshfli32:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    unshfli a0, a0, 13
-; RV64B-NEXT:    ret
-;
-; RV64ZBP-LABEL: unshfli32:
-; RV64ZBP:       # %bb.0:
-; RV64ZBP-NEXT:    unshfli a0, a0, 13
-; RV64ZBP-NEXT:    ret
-  %tmp = call i32 @llvm.riscv.unshfl.i32(i32 %a, i32 13)
- ret i32 %tmp
-}
-
-declare i64 @llvm.riscv.grev.i64(i64 %a, i64 %b)
-
-define i64 @grev64(i64 %a, i64 %b) nounwind {
-; RV64B-LABEL: grev64:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    grev a0, a0, a1
-; RV64B-NEXT:    ret
-;
-; RV64ZBP-LABEL: grev64:
-; RV64ZBP:       # %bb.0:
-; RV64ZBP-NEXT:    grev a0, a0, a1
-; RV64ZBP-NEXT:    ret
-  %tmp = call i64 @llvm.riscv.grev.i64(i64 %a, i64 %b)
- ret i64 %tmp
-}
-
-define i64 @grev64_demandedbits(i64 %a, i64 %b) nounwind {
-; RV64B-LABEL: grev64_demandedbits:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    grev a0, a0, a1
-; RV64B-NEXT:    ret
-;
-; RV64ZBP-LABEL: grev64_demandedbits:
-; RV64ZBP:       # %bb.0:
-; RV64ZBP-NEXT:    grev a0, a0, a1
-; RV64ZBP-NEXT:    ret
-  %c = and i64 %b, 63
-  %tmp = call i64 @llvm.riscv.grev.i64(i64 %a, i64 %c)
-  ret i64 %tmp
-}
-
-define i64 @grevi64(i64 %a) nounwind {
-; RV64B-LABEL: grevi64:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    grevi a0, a0, 13
-; RV64B-NEXT:    ret
-;
-; RV64ZBP-LABEL: grevi64:
-; RV64ZBP:       # %bb.0:
-; RV64ZBP-NEXT:    grevi a0, a0, 13
-; RV64ZBP-NEXT:    ret
-  %tmp = call i64 @llvm.riscv.grev.i64(i64 %a, i64 13)
- ret i64 %tmp
-}
-
-declare i64 @llvm.riscv.gorc.i64(i64 %a, i64 %b)
-
-define i64 @gorc64(i64 %a, i64 %b) nounwind {
-; RV64B-LABEL: gorc64:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    gorc a0, a0, a1
-; RV64B-NEXT:    ret
-;
-; RV64ZBP-LABEL: gorc64:
-; RV64ZBP:       # %bb.0:
-; RV64ZBP-NEXT:    gorc a0, a0, a1
-; RV64ZBP-NEXT:    ret
-  %tmp = call i64 @llvm.riscv.gorc.i64(i64 %a, i64 %b)
- ret i64 %tmp
-}
-
-define i64 @gorc64_demandedbits(i64 %a, i64 %b) nounwind {
-; RV64B-LABEL: gorc64_demandedbits:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    gorc a0, a0, a1
-; RV64B-NEXT:    ret
-;
-; RV64ZBP-LABEL: gorc64_demandedbits:
-; RV64ZBP:       # %bb.0:
-; RV64ZBP-NEXT:    gorc a0, a0, a1
-; RV64ZBP-NEXT:    ret
-  %c = and i64 %b, 63
-  %tmp = call i64 @llvm.riscv.gorc.i64(i64 %a, i64 %c)
-  ret i64 %tmp
-}
-
-declare i64 @llvm.riscv.gorci.i64(i64 %a)
-
-define i64 @gorci64(i64 %a) nounwind {
-; RV64B-LABEL: gorci64:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    gorci a0, a0, 13
-; RV64B-NEXT:    ret
-;
-; RV64ZBP-LABEL: gorci64:
-; RV64ZBP:       # %bb.0:
-; RV64ZBP-NEXT:    gorci a0, a0, 13
-; RV64ZBP-NEXT:    ret
-  %tmp = call i64 @llvm.riscv.gorc.i64(i64 %a, i64 13)
- ret i64 %tmp
-}
-
-declare i64 @llvm.riscv.shfl.i64(i64 %a, i64 %b)
-
-define i64 @shfl64(i64 %a, i64 %b) nounwind {
-; RV64B-LABEL: shfl64:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    shfl a0, a0, a1
-; RV64B-NEXT:    ret
-;
-; RV64ZBP-LABEL: shfl64:
-; RV64ZBP:       # %bb.0:
-; RV64ZBP-NEXT:    shfl a0, a0, a1
-; RV64ZBP-NEXT:    ret
-  %tmp = call i64 @llvm.riscv.shfl.i64(i64 %a, i64 %b)
- ret i64 %tmp
-}
-
-define i64 @shfl64_demandedbits(i64 %a, i64 %b) nounwind {
-; RV64B-LABEL: shfl64_demandedbits:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    shfl a0, a0, a1
-; RV64B-NEXT:    ret
-;
-; RV64ZBP-LABEL: shfl64_demandedbits:
-; RV64ZBP:       # %bb.0:
-; RV64ZBP-NEXT:    shfl a0, a0, a1
-; RV64ZBP-NEXT:    ret
-  %c = and i64 %b, 31
-  %tmp = call i64 @llvm.riscv.shfl.i64(i64 %a, i64 %c)
-  ret i64 %tmp
-}
-
-define i64 @shfli64(i64 %a) nounwind {
-; RV64B-LABEL: shfli64:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    shfli a0, a0, 13
-; RV64B-NEXT:    ret
-;
-; RV64ZBP-LABEL: shfli64:
-; RV64ZBP:       # %bb.0:
-; RV64ZBP-NEXT:    shfli a0, a0, 13
-; RV64ZBP-NEXT:    ret
-  %tmp = call i64 @llvm.riscv.shfl.i64(i64 %a, i64 13)
- ret i64 %tmp
-}
-
-declare i64 @llvm.riscv.unshfl.i64(i64 %a, i64 %b)
-
-define i64 @unshfl64(i64 %a, i64 %b) nounwind {
-; RV64B-LABEL: unshfl64:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    unshfl a0, a0, a1
-; RV64B-NEXT:    ret
-;
-; RV64ZBP-LABEL: unshfl64:
-; RV64ZBP:       # %bb.0:
-; RV64ZBP-NEXT:    unshfl a0, a0, a1
-; RV64ZBP-NEXT:    ret
-  %tmp = call i64 @llvm.riscv.unshfl.i64(i64 %a, i64 %b)
- ret i64 %tmp
-}
-
-define i64 @unshfl64_demandedbits(i64 %a, i64 %b) nounwind {
-; RV64B-LABEL: unshfl64_demandedbits:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    unshfl a0, a0, a1
-; RV64B-NEXT:    ret
-;
-; RV64ZBP-LABEL: unshfl64_demandedbits:
-; RV64ZBP:       # %bb.0:
-; RV64ZBP-NEXT:    unshfl a0, a0, a1
-; RV64ZBP-NEXT:    ret
-  %c = and i64 %b, 31
-  %tmp = call i64 @llvm.riscv.unshfl.i64(i64 %a, i64 %c)
-  ret i64 %tmp
-}
-
-define i64 @unshfli64(i64 %a) nounwind {
-; RV64B-LABEL: unshfli64:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    unshfli a0, a0, 13
-; RV64B-NEXT:    ret
-;
-; RV64ZBP-LABEL: unshfli64:
-; RV64ZBP:       # %bb.0:
-; RV64ZBP-NEXT:    unshfli a0, a0, 13
-; RV64ZBP-NEXT:    ret
-  %tmp = call i64 @llvm.riscv.unshfl.i64(i64 %a, i64 13)
- ret i64 %tmp
-}
-
-declare i64 @llvm.riscv.xperm.n.i64(i64 %a, i64 %b)
-
-define i64 @xpermn64(i64 %a, i64 %b) nounwind {
-; RV64B-LABEL: xpermn64:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    xperm.n a0, a0, a1
-; RV64B-NEXT:    ret
-;
-; RV64ZBP-LABEL: xpermn64:
-; RV64ZBP:       # %bb.0:
-; RV64ZBP-NEXT:    xperm.n a0, a0, a1
-; RV64ZBP-NEXT:    ret
-  %tmp = call i64 @llvm.riscv.xperm.n.i64(i64 %a, i64 %b)
- ret i64 %tmp
-}
-
-declare i64 @llvm.riscv.xperm.b.i64(i64 %a, i64 %b)
-
-define i64 @xpermb64(i64 %a, i64 %b) nounwind {
-; RV64B-LABEL: xpermb64:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    xperm.b a0, a0, a1
-; RV64B-NEXT:    ret
-;
-; RV64ZBP-LABEL: xpermb64:
-; RV64ZBP:       # %bb.0:
-; RV64ZBP-NEXT:    xperm.b a0, a0, a1
-; RV64ZBP-NEXT:    ret
-  %tmp = call i64 @llvm.riscv.xperm.b.i64(i64 %a, i64 %b)
- ret i64 %tmp
-}
-
-declare i64 @llvm.riscv.xperm.h.i64(i64 %a, i64 %b)
-
-define i64 @xpermh64(i64 %a, i64 %b) nounwind {
-; RV64B-LABEL: xpermh64:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    xperm.h a0, a0, a1
-; RV64B-NEXT:    ret
-;
-; RV64ZBP-LABEL: xpermh64:
-; RV64ZBP:       # %bb.0:
-; RV64ZBP-NEXT:    xperm.h a0, a0, a1
-; RV64ZBP-NEXT:    ret
-  %tmp = call i64 @llvm.riscv.xperm.h.i64(i64 %a, i64 %b)
- ret i64 %tmp
-}
-
-declare i64 @llvm.riscv.xperm.w.i64(i64 %a, i64 %b)
-
-define i64 @xpermw64(i64 %a, i64 %b) nounwind {
-; RV64B-LABEL: xpermw64:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    xperm.w a0, a0, a1
-; RV64B-NEXT:    ret
-;
-; RV64ZBP-LABEL: xpermw64:
-; RV64ZBP:       # %bb.0:
-; RV64ZBP-NEXT:    xperm.w a0, a0, a1
-; RV64ZBP-NEXT:    ret
-  %tmp = call i64 @llvm.riscv.xperm.w.i64(i64 %a, i64 %b)
- ret i64 %tmp
-}
diff --git a/llvm/test/CodeGen/RISCV/rv64zbp.ll b/llvm/test/CodeGen/RISCV/rv64zbp.ll
deleted file mode 100644
index 96121858ff53..000000000000
--- a/llvm/test/CodeGen/RISCV/rv64zbp.ll
+++ /dev/null
@@ -1,3858 +0,0 @@
-; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
-; RUN: llc -mtriple=riscv64 -verify-machineinstrs < %s \
-; RUN:   | FileCheck %s -check-prefix=RV64I
-; RUN: llc -mtriple=riscv64 -mattr=+experimental-b -verify-machineinstrs < %s \
-; RUN:   | FileCheck %s -check-prefix=RV64B
-; RUN: llc -mtriple=riscv64 -mattr=+experimental-zbp -verify-machineinstrs < %s \
-; RUN:   | FileCheck %s -check-prefix=RV64ZBP
-
-define signext i32 @gorc1_i32(i32 signext %a) nounwind {
-; RV64I-LABEL: gorc1_i32:
-; RV64I:       # %bb.0:
-; RV64I-NEXT:    slliw a1, a0, 1
-; RV64I-NEXT:    lui a2, 699051
-; RV64I-NEXT:    addiw a2, a2, -1366
-; RV64I-NEXT:    and a1, a1, a2
-; RV64I-NEXT:    srli a2, a0, 1
-; RV64I-NEXT:    lui a3, 349525
-; RV64I-NEXT:    addiw a3, a3, 1365
-; RV64I-NEXT:    and a2, a2, a3
-; RV64I-NEXT:    or a0, a2, a0
-; RV64I-NEXT:    or a0, a0, a1
-; RV64I-NEXT:    ret
-;
-; RV64B-LABEL: gorc1_i32:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    gorciw a0, a0, 1
-; RV64B-NEXT:    ret
-;
-; RV64ZBP-LABEL: gorc1_i32:
-; RV64ZBP:       # %bb.0:
-; RV64ZBP-NEXT:    gorciw a0, a0, 1
-; RV64ZBP-NEXT:    ret
-  %and = shl i32 %a, 1
-  %shl = and i32 %and, -1431655766
-  %and1 = lshr i32 %a, 1
-  %shr = and i32 %and1, 1431655765
-  %or = or i32 %shr, %a
-  %or2 = or i32 %or, %shl
-  ret i32 %or2
-}
-
-define i64 @gorc1_i64(i64 %a) nounwind {
-; RV64I-LABEL: gorc1_i64:
-; RV64I:       # %bb.0:
-; RV64I-NEXT:    slli a1, a0, 1
-; RV64I-NEXT:    lui a2, 1026731
-; RV64I-NEXT:    addiw a2, a2, -1365
-; RV64I-NEXT:    slli a2, a2, 12
-; RV64I-NEXT:    addi a2, a2, -1365
-; RV64I-NEXT:    slli a2, a2, 12
-; RV64I-NEXT:    addi a2, a2, -1365
-; RV64I-NEXT:    slli a2, a2, 12
-; RV64I-NEXT:    addi a2, a2, -1366
-; RV64I-NEXT:    and a1, a1, a2
-; RV64I-NEXT:    srli a2, a0, 1
-; RV64I-NEXT:    lui a3, 21845
-; RV64I-NEXT:    addiw a3, a3, 1365
-; RV64I-NEXT:    slli a3, a3, 12
-; RV64I-NEXT:    addi a3, a3, 1365
-; RV64I-NEXT:    slli a3, a3, 12
-; RV64I-NEXT:    addi a3, a3, 1365
-; RV64I-NEXT:    slli a3, a3, 12
-; RV64I-NEXT:    addi a3, a3, 1365
-; RV64I-NEXT:    and a2, a2, a3
-; RV64I-NEXT:    or a0, a2, a0
-; RV64I-NEXT:    or a0, a0, a1
-; RV64I-NEXT:    ret
-;
-; RV64B-LABEL: gorc1_i64:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    orc.p a0, a0
-; RV64B-NEXT:    ret
-;
-; RV64ZBP-LABEL: gorc1_i64:
-; RV64ZBP:       # %bb.0:
-; RV64ZBP-NEXT:    orc.p a0, a0
-; RV64ZBP-NEXT:    ret
-  %and = shl i64 %a, 1
-  %shl = and i64 %and, -6148914691236517206
-  %and1 = lshr i64 %a, 1
-  %shr = and i64 %and1, 6148914691236517205
-  %or = or i64 %shr, %a
-  %or2 = or i64 %or, %shl
-  ret i64 %or2
-}
-
-define signext i32 @gorc2_i32(i32 signext %a) nounwind {
-; RV64I-LABEL: gorc2_i32:
-; RV64I:       # %bb.0:
-; RV64I-NEXT:    slliw a1, a0, 2
-; RV64I-NEXT:    lui a2, 838861
-; RV64I-NEXT:    addiw a2, a2, -820
-; RV64I-NEXT:    and a1, a1, a2
-; RV64I-NEXT:    srli a2, a0, 2
-; RV64I-NEXT:    lui a3, 209715
-; RV64I-NEXT:    addiw a3, a3, 819
-; RV64I-NEXT:    and a2, a2, a3
-; RV64I-NEXT:    or a0, a2, a0
-; RV64I-NEXT:    or a0, a0, a1
-; RV64I-NEXT:    ret
-;
-; RV64B-LABEL: gorc2_i32:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    gorciw a0, a0, 2
-; RV64B-NEXT:    ret
-;
-; RV64ZBP-LABEL: gorc2_i32:
-; RV64ZBP:       # %bb.0:
-; RV64ZBP-NEXT:    gorciw a0, a0, 2
-; RV64ZBP-NEXT:    ret
-  %and = shl i32 %a, 2
-  %shl = and i32 %and, -858993460
-  %and1 = lshr i32 %a, 2
-  %shr = and i32 %and1, 858993459
-  %or = or i32 %shr, %a
-  %or2 = or i32 %or, %shl
-  ret i32 %or2
-}
-
-define i64 @gorc2_i64(i64 %a) nounwind {
-; RV64I-LABEL: gorc2_i64:
-; RV64I:       # %bb.0:
-; RV64I-NEXT:    slli a1, a0, 2
-; RV64I-NEXT:    lui a2, 1035469
-; RV64I-NEXT:    addiw a2, a2, -819
-; RV64I-NEXT:    slli a2, a2, 12
-; RV64I-NEXT:    addi a2, a2, -819
-; RV64I-NEXT:    slli a2, a2, 12
-; RV64I-NEXT:    addi a2, a2, -819
-; RV64I-NEXT:    slli a2, a2, 12
-; RV64I-NEXT:    addi a2, a2, -820
-; RV64I-NEXT:    and a1, a1, a2
-; RV64I-NEXT:    srli a2, a0, 2
-; RV64I-NEXT:    lui a3, 13107
-; RV64I-NEXT:    addiw a3, a3, 819
-; RV64I-NEXT:    slli a3, a3, 12
-; RV64I-NEXT:    addi a3, a3, 819
-; RV64I-NEXT:    slli a3, a3, 12
-; RV64I-NEXT:    addi a3, a3, 819
-; RV64I-NEXT:    slli a3, a3, 12
-; RV64I-NEXT:    addi a3, a3, 819
-; RV64I-NEXT:    and a2, a2, a3
-; RV64I-NEXT:    or a0, a2, a0
-; RV64I-NEXT:    or a0, a0, a1
-; RV64I-NEXT:    ret
-;
-; RV64B-LABEL: gorc2_i64:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    orc2.n a0, a0
-; RV64B-NEXT:    ret
-;
-; RV64ZBP-LABEL: gorc2_i64:
-; RV64ZBP:       # %bb.0:
-; RV64ZBP-NEXT:    orc2.n a0, a0
-; RV64ZBP-NEXT:    ret
-  %and = shl i64 %a, 2
-  %shl = and i64 %and, -3689348814741910324
-  %and1 = lshr i64 %a, 2
-  %shr = and i64 %and1, 3689348814741910323
-  %or = or i64 %shr, %a
-  %or2 = or i64 %or, %shl
-  ret i64 %or2
-}
-
-define signext i32 @gorc3_i32(i32 signext %a) nounwind {
-; RV64I-LABEL: gorc3_i32:
-; RV64I:       # %bb.0:
-; RV64I-NEXT:    slliw a1, a0, 1
-; RV64I-NEXT:    lui a2, 699051
-; RV64I-NEXT:    addiw a2, a2, -1366
-; RV64I-NEXT:    and a1, a1, a2
-; RV64I-NEXT:    srli a2, a0, 1
-; RV64I-NEXT:    lui a3, 349525
-; RV64I-NEXT:    addiw a3, a3, 1365
-; RV64I-NEXT:    and a2, a2, a3
-; RV64I-NEXT:    or a0, a2, a0
-; RV64I-NEXT:    or a0, a0, a1
-; RV64I-NEXT:    slliw a1, a0, 2
-; RV64I-NEXT:    lui a2, 838861
-; RV64I-NEXT:    addiw a2, a2, -820
-; RV64I-NEXT:    and a1, a1, a2
-; RV64I-NEXT:    srli a2, a0, 2
-; RV64I-NEXT:    lui a3, 209715
-; RV64I-NEXT:    addiw a3, a3, 819
-; RV64I-NEXT:    and a2, a2, a3
-; RV64I-NEXT:    or a0, a2, a0
-; RV64I-NEXT:    or a0, a0, a1
-; RV64I-NEXT:    ret
-;
-; RV64B-LABEL: gorc3_i32:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    gorciw a0, a0, 3
-; RV64B-NEXT:    ret
-;
-; RV64ZBP-LABEL: gorc3_i32:
-; RV64ZBP:       # %bb.0:
-; RV64ZBP-NEXT:    gorciw a0, a0, 3
-; RV64ZBP-NEXT:    ret
-  %and1 = shl i32 %a, 1
-  %shl1 = and i32 %and1, -1431655766
-  %and1b = lshr i32 %a, 1
-  %shr1 = and i32 %and1b, 1431655765
-  %or1 = or i32 %shr1, %a
-  %or1b = or i32 %or1, %shl1
-  %and2 = shl i32 %or1b, 2
-  %shl2 = and i32 %and2, -858993460
-  %and2b = lshr i32 %or1b, 2
-  %shr2 = and i32 %and2b, 858993459
-  %or2 = or i32 %shr2, %or1b
-  %or2b = or i32 %or2, %shl2
-  ret i32 %or2b
-}
-
-define i64 @gorc3_i64(i64 %a) nounwind {
-; RV64I-LABEL: gorc3_i64:
-; RV64I:       # %bb.0:
-; RV64I-NEXT:    slli a1, a0, 1
-; RV64I-NEXT:    lui a2, 1026731
-; RV64I-NEXT:    addiw a2, a2, -1365
-; RV64I-NEXT:    slli a2, a2, 12
-; RV64I-NEXT:    addi a2, a2, -1365
-; RV64I-NEXT:    slli a2, a2, 12
-; RV64I-NEXT:    addi a2, a2, -1365
-; RV64I-NEXT:    slli a2, a2, 12
-; RV64I-NEXT:    addi a2, a2, -1366
-; RV64I-NEXT:    and a1, a1, a2
-; RV64I-NEXT:    srli a2, a0, 1
-; RV64I-NEXT:    lui a3, 21845
-; RV64I-NEXT:    addiw a3, a3, 1365
-; RV64I-NEXT:    slli a3, a3, 12
-; RV64I-NEXT:    addi a3, a3, 1365
-; RV64I-NEXT:    slli a3, a3, 12
-; RV64I-NEXT:    addi a3, a3, 1365
-; RV64I-NEXT:    slli a3, a3, 12
-; RV64I-NEXT:    addi a3, a3, 1365
-; RV64I-NEXT:    and a2, a2, a3
-; RV64I-NEXT:    or a0, a2, a0
-; RV64I-NEXT:    or a0, a0, a1
-; RV64I-NEXT:    slli a1, a0, 2
-; RV64I-NEXT:    lui a2, 1035469
-; RV64I-NEXT:    addiw a2, a2, -819
-; RV64I-NEXT:    slli a2, a2, 12
-; RV64I-NEXT:    addi a2, a2, -819
-; RV64I-NEXT:    slli a2, a2, 12
-; RV64I-NEXT:    addi a2, a2, -819
-; RV64I-NEXT:    slli a2, a2, 12
-; RV64I-NEXT:    addi a2, a2, -820
-; RV64I-NEXT:    and a1, a1, a2
-; RV64I-NEXT:    srli a2, a0, 2
-; RV64I-NEXT:    lui a3, 13107
-; RV64I-NEXT:    addiw a3, a3, 819
-; RV64I-NEXT:    slli a3, a3, 12
-; RV64I-NEXT:    addi a3, a3, 819
-; RV64I-NEXT:    slli a3, a3, 12
-; RV64I-NEXT:    addi a3, a3, 819
-; RV64I-NEXT:    slli a3, a3, 12
-; RV64I-NEXT:    addi a3, a3, 819
-; RV64I-NEXT:    and a2, a2, a3
-; RV64I-NEXT:    or a0, a2, a0
-; RV64I-NEXT:    or a0, a0, a1
-; RV64I-NEXT:    ret
-;
-; RV64B-LABEL: gorc3_i64:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    orc.n a0, a0
-; RV64B-NEXT:    ret
-;
-; RV64ZBP-LABEL: gorc3_i64:
-; RV64ZBP:       # %bb.0:
-; RV64ZBP-NEXT:    orc.n a0, a0
-; RV64ZBP-NEXT:    ret
-  %and1 = shl i64 %a, 1
-  %shl1 = and i64 %and1, -6148914691236517206
-  %and1b = lshr i64 %a, 1
-  %shr1 = and i64 %and1b, 6148914691236517205
-  %or1 = or i64 %shr1, %a
-  %or1b = or i64 %or1, %shl1
-  %and2 = shl i64 %or1b, 2
-  %shl2 = and i64 %and2, -3689348814741910324
-  %and2b = lshr i64 %or1b, 2
-  %shr2 = and i64 %and2b, 3689348814741910323
-  %or2 = or i64 %shr2, %or1b
-  %or2b = or i64 %or2, %shl2
-  ret i64 %or2b
-}
-
-define signext i32 @gorc4_i32(i32 signext %a) nounwind {
-; RV64I-LABEL: gorc4_i32:
-; RV64I:       # %bb.0:
-; RV64I-NEXT:    slliw a1, a0, 4
-; RV64I-NEXT:    lui a2, 986895
-; RV64I-NEXT:    addiw a2, a2, 240
-; RV64I-NEXT:    and a1, a1, a2
-; RV64I-NEXT:    srli a2, a0, 4
-; RV64I-NEXT:    lui a3, 61681
-; RV64I-NEXT:    addiw a3, a3, -241
-; RV64I-NEXT:    and a2, a2, a3
-; RV64I-NEXT:    or a0, a2, a0
-; RV64I-NEXT:    or a0, a0, a1
-; RV64I-NEXT:    ret
-;
-; RV64B-LABEL: gorc4_i32:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    gorciw a0, a0, 4
-; RV64B-NEXT:    ret
-;
-; RV64ZBP-LABEL: gorc4_i32:
-; RV64ZBP:       # %bb.0:
-; RV64ZBP-NEXT:    gorciw a0, a0, 4
-; RV64ZBP-NEXT:    ret
-  %and = shl i32 %a, 4
-  %shl = and i32 %and, -252645136
-  %and1 = lshr i32 %a, 4
-  %shr = and i32 %and1, 252645135
-  %or = or i32 %shr, %a
-  %or2 = or i32 %or, %shl
-  ret i32 %or2
-}
-
-define i64 @gorc4_i64(i64 %a) nounwind {
-; RV64I-LABEL: gorc4_i64:
-; RV64I:       # %bb.0:
-; RV64I-NEXT:    slli a1, a0, 4
-; RV64I-NEXT:    lui a2, 1044721
-; RV64I-NEXT:    addiw a2, a2, -241
-; RV64I-NEXT:    slli a2, a2, 12
-; RV64I-NEXT:    addi a2, a2, 241
-; RV64I-NEXT:    slli a2, a2, 12
-; RV64I-NEXT:    addi a2, a2, -241
-; RV64I-NEXT:    slli a2, a2, 12
-; RV64I-NEXT:    addi a2, a2, 240
-; RV64I-NEXT:    and a1, a1, a2
-; RV64I-NEXT:    srli a2, a0, 4
-; RV64I-NEXT:    lui a3, 3855
-; RV64I-NEXT:    addiw a3, a3, 241
-; RV64I-NEXT:    slli a3, a3, 12
-; RV64I-NEXT:    addi a3, a3, -241
-; RV64I-NEXT:    slli a3, a3, 12
-; RV64I-NEXT:    addi a3, a3, 241
-; RV64I-NEXT:    slli a3, a3, 12
-; RV64I-NEXT:    addi a3, a3, -241
-; RV64I-NEXT:    and a2, a2, a3
-; RV64I-NEXT:    or a0, a2, a0
-; RV64I-NEXT:    or a0, a0, a1
-; RV64I-NEXT:    ret
-;
-; RV64B-LABEL: gorc4_i64:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    orc4.b a0, a0
-; RV64B-NEXT:    ret
-;
-; RV64ZBP-LABEL: gorc4_i64:
-; RV64ZBP:       # %bb.0:
-; RV64ZBP-NEXT:    orc4.b a0, a0
-; RV64ZBP-NEXT:    ret
-  %and = shl i64 %a, 4
-  %shl = and i64 %and, -1085102592571150096
-  %and1 = lshr i64 %a, 4
-  %shr = and i64 %and1, 1085102592571150095
-  %or = or i64 %shr, %a
-  %or2 = or i64 %or, %shl
-  ret i64 %or2
-}
-
-define signext i32 @gorc5_i32(i32 signext %a) nounwind {
-; RV64I-LABEL: gorc5_i32:
-; RV64I:       # %bb.0:
-; RV64I-NEXT:    slliw a1, a0, 1
-; RV64I-NEXT:    lui a2, 699051
-; RV64I-NEXT:    addiw a2, a2, -1366
-; RV64I-NEXT:    and a1, a1, a2
-; RV64I-NEXT:    srli a2, a0, 1
-; RV64I-NEXT:    lui a3, 349525
-; RV64I-NEXT:    addiw a3, a3, 1365
-; RV64I-NEXT:    and a2, a2, a3
-; RV64I-NEXT:    or a0, a2, a0
-; RV64I-NEXT:    or a0, a0, a1
-; RV64I-NEXT:    slliw a1, a0, 4
-; RV64I-NEXT:    lui a2, 986895
-; RV64I-NEXT:    addiw a2, a2, 240
-; RV64I-NEXT:    and a1, a1, a2
-; RV64I-NEXT:    srli a2, a0, 4
-; RV64I-NEXT:    lui a3, 61681
-; RV64I-NEXT:    addiw a3, a3, -241
-; RV64I-NEXT:    and a2, a2, a3
-; RV64I-NEXT:    or a0, a2, a0
-; RV64I-NEXT:    or a0, a0, a1
-; RV64I-NEXT:    ret
-;
-; RV64B-LABEL: gorc5_i32:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    gorciw a0, a0, 5
-; RV64B-NEXT:    ret
-;
-; RV64ZBP-LABEL: gorc5_i32:
-; RV64ZBP:       # %bb.0:
-; RV64ZBP-NEXT:    gorciw a0, a0, 5
-; RV64ZBP-NEXT:    ret
-  %and1 = shl i32 %a, 1
-  %shl1 = and i32 %and1, -1431655766
-  %and1b = lshr i32 %a, 1
-  %shr1 = and i32 %and1b, 1431655765
-  %or1 = or i32 %shr1, %a
-  %or1b = or i32 %or1, %shl1
-  %and2 = shl i32 %or1b, 4
-  %shl2 = and i32 %and2, -252645136
-  %and2b = lshr i32 %or1b, 4
-  %shr2 = and i32 %and2b, 252645135
-  %or2 = or i32 %shr2, %or1b
-  %or2b = or i32 %or2, %shl2
-  ret i32 %or2b
-}
-
-define i64 @gorc5_i64(i64 %a) nounwind {
-; RV64I-LABEL: gorc5_i64:
-; RV64I:       # %bb.0:
-; RV64I-NEXT:    slli a1, a0, 1
-; RV64I-NEXT:    lui a2, 1026731
-; RV64I-NEXT:    addiw a2, a2, -1365
-; RV64I-NEXT:    slli a2, a2, 12
-; RV64I-NEXT:    addi a2, a2, -1365
-; RV64I-NEXT:    slli a2, a2, 12
-; RV64I-NEXT:    addi a2, a2, -1365
-; RV64I-NEXT:    slli a2, a2, 12
-; RV64I-NEXT:    addi a2, a2, -1366
-; RV64I-NEXT:    and a1, a1, a2
-; RV64I-NEXT:    srli a2, a0, 1
-; RV64I-NEXT:    lui a3, 21845
-; RV64I-NEXT:    addiw a3, a3, 1365
-; RV64I-NEXT:    slli a3, a3, 12
-; RV64I-NEXT:    addi a3, a3, 1365
-; RV64I-NEXT:    slli a3, a3, 12
-; RV64I-NEXT:    addi a3, a3, 1365
-; RV64I-NEXT:    slli a3, a3, 12
-; RV64I-NEXT:    addi a3, a3, 1365
-; RV64I-NEXT:    and a2, a2, a3
-; RV64I-NEXT:    or a0, a2, a0
-; RV64I-NEXT:    or a0, a0, a1
-; RV64I-NEXT:    slli a1, a0, 4
-; RV64I-NEXT:    lui a2, 1044721
-; RV64I-NEXT:    addiw a2, a2, -241
-; RV64I-NEXT:    slli a2, a2, 12
-; RV64I-NEXT:    addi a2, a2, 241
-; RV64I-NEXT:    slli a2, a2, 12
-; RV64I-NEXT:    addi a2, a2, -241
-; RV64I-NEXT:    slli a2, a2, 12
-; RV64I-NEXT:    addi a2, a2, 240
-; RV64I-NEXT:    and a1, a1, a2
-; RV64I-NEXT:    srli a2, a0, 4
-; RV64I-NEXT:    lui a3, 3855
-; RV64I-NEXT:    addiw a3, a3, 241
-; RV64I-NEXT:    slli a3, a3, 12
-; RV64I-NEXT:    addi a3, a3, -241
-; RV64I-NEXT:    slli a3, a3, 12
-; RV64I-NEXT:    addi a3, a3, 241
-; RV64I-NEXT:    slli a3, a3, 12
-; RV64I-NEXT:    addi a3, a3, -241
-; RV64I-NEXT:    and a2, a2, a3
-; RV64I-NEXT:    or a0, a2, a0
-; RV64I-NEXT:    or a0, a0, a1
-; RV64I-NEXT:    ret
-;
-; RV64B-LABEL: gorc5_i64:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    gorci a0, a0, 5
-; RV64B-NEXT:    ret
-;
-; RV64ZBP-LABEL: gorc5_i64:
-; RV64ZBP:       # %bb.0:
-; RV64ZBP-NEXT:    gorci a0, a0, 5
-; RV64ZBP-NEXT:    ret
-  %and1 = shl i64 %a, 1
-  %shl1 = and i64 %and1, -6148914691236517206
-  %and1b = lshr i64 %a, 1
-  %shr1 = and i64 %and1b, 6148914691236517205
-  %or1 = or i64 %shr1, %a
-  %or1b = or i64 %or1, %shl1
-  %and2 = shl i64 %or1b, 4
-  %shl2 = and i64 %and2, -1085102592571150096
-  %and2b = lshr i64 %or1b, 4
-  %shr2 = and i64 %and2b, 1085102592571150095
-  %or2 = or i64 %shr2, %or1b
-  %or2b = or i64 %or2, %shl2
-  ret i64 %or2b
-}
-
-define signext i32 @gorc6_i32(i32 signext %a) nounwind {
-; RV64I-LABEL: gorc6_i32:
-; RV64I:       # %bb.0:
-; RV64I-NEXT:    slliw a1, a0, 2
-; RV64I-NEXT:    lui a2, 838861
-; RV64I-NEXT:    addiw a2, a2, -820
-; RV64I-NEXT:    and a1, a1, a2
-; RV64I-NEXT:    srli a2, a0, 2
-; RV64I-NEXT:    lui a3, 209715
-; RV64I-NEXT:    addiw a3, a3, 819
-; RV64I-NEXT:    and a2, a2, a3
-; RV64I-NEXT:    or a0, a2, a0
-; RV64I-NEXT:    or a0, a0, a1
-; RV64I-NEXT:    slliw a1, a0, 4
-; RV64I-NEXT:    lui a2, 986895
-; RV64I-NEXT:    addiw a2, a2, 240
-; RV64I-NEXT:    and a1, a1, a2
-; RV64I-NEXT:    srli a2, a0, 4
-; RV64I-NEXT:    lui a3, 61681
-; RV64I-NEXT:    addiw a3, a3, -241
-; RV64I-NEXT:    and a2, a2, a3
-; RV64I-NEXT:    or a0, a2, a0
-; RV64I-NEXT:    or a0, a0, a1
-; RV64I-NEXT:    ret
-;
-; RV64B-LABEL: gorc6_i32:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    gorciw a0, a0, 6
-; RV64B-NEXT:    ret
-;
-; RV64ZBP-LABEL: gorc6_i32:
-; RV64ZBP:       # %bb.0:
-; RV64ZBP-NEXT:    gorciw a0, a0, 6
-; RV64ZBP-NEXT:    ret
-  %and1 = shl i32 %a, 2
-  %shl1 = and i32 %and1, -858993460
-  %and1b = lshr i32 %a, 2
-  %shr1 = and i32 %and1b, 858993459
-  %or1 = or i32 %shr1, %a
-  %or1b = or i32 %or1, %shl1
-  %and2 = shl i32 %or1b, 4
-  %shl2 = and i32 %and2, -252645136
-  %and2b = lshr i32 %or1b, 4
-  %shr2 = and i32 %and2b, 252645135
-  %or2 = or i32 %shr2, %or1b
-  %or2b = or i32 %or2, %shl2
-  ret i32 %or2b
-}
-
-define i64 @gorc6_i64(i64 %a) nounwind {
-; RV64I-LABEL: gorc6_i64:
-; RV64I:       # %bb.0:
-; RV64I-NEXT:    slli a1, a0, 2
-; RV64I-NEXT:    lui a2, 1035469
-; RV64I-NEXT:    addiw a2, a2, -819
-; RV64I-NEXT:    slli a2, a2, 12
-; RV64I-NEXT:    addi a2, a2, -819
-; RV64I-NEXT:    slli a2, a2, 12
-; RV64I-NEXT:    addi a2, a2, -819
-; RV64I-NEXT:    slli a2, a2, 12
-; RV64I-NEXT:    addi a2, a2, -820
-; RV64I-NEXT:    and a1, a1, a2
-; RV64I-NEXT:    srli a2, a0, 2
-; RV64I-NEXT:    lui a3, 13107
-; RV64I-NEXT:    addiw a3, a3, 819
-; RV64I-NEXT:    slli a3, a3, 12
-; RV64I-NEXT:    addi a3, a3, 819
-; RV64I-NEXT:    slli a3, a3, 12
-; RV64I-NEXT:    addi a3, a3, 819
-; RV64I-NEXT:    slli a3, a3, 12
-; RV64I-NEXT:    addi a3, a3, 819
-; RV64I-NEXT:    and a2, a2, a3
-; RV64I-NEXT:    or a0, a2, a0
-; RV64I-NEXT:    or a0, a0, a1
-; RV64I-NEXT:    slli a1, a0, 4
-; RV64I-NEXT:    lui a2, 1044721
-; RV64I-NEXT:    addiw a2, a2, -241
-; RV64I-NEXT:    slli a2, a2, 12
-; RV64I-NEXT:    addi a2, a2, 241
-; RV64I-NEXT:    slli a2, a2, 12
-; RV64I-NEXT:    addi a2, a2, -241
-; RV64I-NEXT:    slli a2, a2, 12
-; RV64I-NEXT:    addi a2, a2, 240
-; RV64I-NEXT:    and a1, a1, a2
-; RV64I-NEXT:    srli a2, a0, 4
-; RV64I-NEXT:    lui a3, 3855
-; RV64I-NEXT:    addiw a3, a3, 241
-; RV64I-NEXT:    slli a3, a3, 12
-; RV64I-NEXT:    addi a3, a3, -241
-; RV64I-NEXT:    slli a3, a3, 12
-; RV64I-NEXT:    addi a3, a3, 241
-; RV64I-NEXT:    slli a3, a3, 12
-; RV64I-NEXT:    addi a3, a3, -241
-; RV64I-NEXT:    and a2, a2, a3
-; RV64I-NEXT:    or a0, a2, a0
-; RV64I-NEXT:    or a0, a0, a1
-; RV64I-NEXT:    ret
-;
-; RV64B-LABEL: gorc6_i64:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    orc2.b a0, a0
-; RV64B-NEXT:    ret
-;
-; RV64ZBP-LABEL: gorc6_i64:
-; RV64ZBP:       # %bb.0:
-; RV64ZBP-NEXT:    orc2.b a0, a0
-; RV64ZBP-NEXT:    ret
-  %and1 = shl i64 %a, 2
-  %shl1 = and i64 %and1, -3689348814741910324
-  %and1b = lshr i64 %a, 2
-  %shr1 = and i64 %and1b, 3689348814741910323
-  %or1 = or i64 %shr1, %a
-  %or1b = or i64 %or1, %shl1
-  %and2 = shl i64 %or1b, 4
-  %shl2 = and i64 %and2, -1085102592571150096
-  %and2b = lshr i64 %or1b, 4
-  %shr2 = and i64 %and2b, 1085102592571150095
-  %or2 = or i64 %shr2, %or1b
-  %or2b = or i64 %or2, %shl2
-  ret i64 %or2b
-}
-
-define signext i32 @gorc7_i32(i32 signext %a) nounwind {
-; RV64I-LABEL: gorc7_i32:
-; RV64I:       # %bb.0:
-; RV64I-NEXT:    slliw a1, a0, 1
-; RV64I-NEXT:    lui a2, 699051
-; RV64I-NEXT:    addiw a2, a2, -1366
-; RV64I-NEXT:    and a1, a1, a2
-; RV64I-NEXT:    srli a2, a0, 1
-; RV64I-NEXT:    lui a3, 349525
-; RV64I-NEXT:    addiw a3, a3, 1365
-; RV64I-NEXT:    and a2, a2, a3
-; RV64I-NEXT:    or a0, a2, a0
-; RV64I-NEXT:    or a0, a0, a1
-; RV64I-NEXT:    slliw a1, a0, 2
-; RV64I-NEXT:    lui a2, 838861
-; RV64I-NEXT:    addiw a2, a2, -820
-; RV64I-NEXT:    and a1, a1, a2
-; RV64I-NEXT:    srli a2, a0, 2
-; RV64I-NEXT:    lui a3, 209715
-; RV64I-NEXT:    addiw a3, a3, 819
-; RV64I-NEXT:    and a2, a2, a3
-; RV64I-NEXT:    or a0, a2, a0
-; RV64I-NEXT:    or a0, a0, a1
-; RV64I-NEXT:    slli a1, a0, 4
-; RV64I-NEXT:    lui a2, 986895
-; RV64I-NEXT:    addiw a2, a2, 240
-; RV64I-NEXT:    and a1, a1, a2
-; RV64I-NEXT:    srli a2, a0, 4
-; RV64I-NEXT:    lui a3, 61681
-; RV64I-NEXT:    addiw a3, a3, -241
-; RV64I-NEXT:    and a2, a2, a3
-; RV64I-NEXT:    or a0, a2, a0
-; RV64I-NEXT:    or a0, a0, a1
-; RV64I-NEXT:    sext.w a0, a0
-; RV64I-NEXT:    ret
-;
-; RV64B-LABEL: gorc7_i32:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    gorciw a0, a0, 7
-; RV64B-NEXT:    ret
-;
-; RV64ZBP-LABEL: gorc7_i32:
-; RV64ZBP:       # %bb.0:
-; RV64ZBP-NEXT:    gorciw a0, a0, 7
-; RV64ZBP-NEXT:    ret
-  %and1 = shl i32 %a, 1
-  %shl1 = and i32 %and1, -1431655766
-  %and1b = lshr i32 %a, 1
-  %shr1 = and i32 %and1b, 1431655765
-  %or1 = or i32 %shr1, %a
-  %or1b = or i32 %or1, %shl1
-  %and2 = shl i32 %or1b, 2
-  %shl2 = and i32 %and2, -858993460
-  %and2b = lshr i32 %or1b, 2
-  %shr2 = and i32 %and2b, 858993459
-  %or2 = or i32 %shr2, %or1b
-  %or2b = or i32 %or2, %shl2
-  %and3 = shl i32 %or2b, 4
-  %shl3 = and i32 %and3, -252645136
-  %and3b = lshr i32 %or2b, 4
-  %shr3 = and i32 %and3b, 252645135
-  %or3 = or i32 %shr3, %or2b
-  %or3b = or i32 %or3, %shl3
-  ret i32 %or3b
-}
-
-define i64 @gorc7_i64(i64 %a) nounwind {
-; RV64I-LABEL: gorc7_i64:
-; RV64I:       # %bb.0:
-; RV64I-NEXT:    slli a1, a0, 1
-; RV64I-NEXT:    lui a2, 1026731
-; RV64I-NEXT:    addiw a2, a2, -1365
-; RV64I-NEXT:    slli a2, a2, 12
-; RV64I-NEXT:    addi a2, a2, -1365
-; RV64I-NEXT:    slli a2, a2, 12
-; RV64I-NEXT:    addi a2, a2, -1365
-; RV64I-NEXT:    slli a2, a2, 12
-; RV64I-NEXT:    addi a2, a2, -1366
-; RV64I-NEXT:    and a1, a1, a2
-; RV64I-NEXT:    srli a2, a0, 1
-; RV64I-NEXT:    lui a3, 21845
-; RV64I-NEXT:    addiw a3, a3, 1365
-; RV64I-NEXT:    slli a3, a3, 12
-; RV64I-NEXT:    addi a3, a3, 1365
-; RV64I-NEXT:    slli a3, a3, 12
-; RV64I-NEXT:    addi a3, a3, 1365
-; RV64I-NEXT:    slli a3, a3, 12
-; RV64I-NEXT:    addi a3, a3, 1365
-; RV64I-NEXT:    and a2, a2, a3
-; RV64I-NEXT:    or a0, a2, a0
-; RV64I-NEXT:    or a0, a0, a1
-; RV64I-NEXT:    slli a1, a0, 2
-; RV64I-NEXT:    lui a2, 1035469
-; RV64I-NEXT:    addiw a2, a2, -819
-; RV64I-NEXT:    slli a2, a2, 12
-; RV64I-NEXT:    addi a2, a2, -819
-; RV64I-NEXT:    slli a2, a2, 12
-; RV64I-NEXT:    addi a2, a2, -819
-; RV64I-NEXT:    slli a2, a2, 12
-; RV64I-NEXT:    addi a2, a2, -820
-; RV64I-NEXT:    and a1, a1, a2
-; RV64I-NEXT:    srli a2, a0, 2
-; RV64I-NEXT:    lui a3, 13107
-; RV64I-NEXT:    addiw a3, a3, 819
-; RV64I-NEXT:    slli a3, a3, 12
-; RV64I-NEXT:    addi a3, a3, 819
-; RV64I-NEXT:    slli a3, a3, 12
-; RV64I-NEXT:    addi a3, a3, 819
-; RV64I-NEXT:    slli a3, a3, 12
-; RV64I-NEXT:    addi a3, a3, 819
-; RV64I-NEXT:    and a2, a2, a3
-; RV64I-NEXT:    or a0, a2, a0
-; RV64I-NEXT:    or a0, a0, a1
-; RV64I-NEXT:    slli a1, a0, 4
-; RV64I-NEXT:    lui a2, 1044721
-; RV64I-NEXT:    addiw a2, a2, -241
-; RV64I-NEXT:    slli a2, a2, 12
-; RV64I-NEXT:    addi a2, a2, 241
-; RV64I-NEXT:    slli a2, a2, 12
-; RV64I-NEXT:    addi a2, a2, -241
-; RV64I-NEXT:    slli a2, a2, 12
-; RV64I-NEXT:    addi a2, a2, 240
-; RV64I-NEXT:    and a1, a1, a2
-; RV64I-NEXT:    srli a2, a0, 4
-; RV64I-NEXT:    lui a3, 3855
-; RV64I-NEXT:    addiw a3, a3, 241
-; RV64I-NEXT:    slli a3, a3, 12
-; RV64I-NEXT:    addi a3, a3, -241
-; RV64I-NEXT:    slli a3, a3, 12
-; RV64I-NEXT:    addi a3, a3, 241
-; RV64I-NEXT:    slli a3, a3, 12
-; RV64I-NEXT:    addi a3, a3, -241
-; RV64I-NEXT:    and a2, a2, a3
-; RV64I-NEXT:    or a0, a2, a0
-; RV64I-NEXT:    or a0, a0, a1
-; RV64I-NEXT:    ret
-;
-; RV64B-LABEL: gorc7_i64:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    orc.b a0, a0
-; RV64B-NEXT:    ret
-;
-; RV64ZBP-LABEL: gorc7_i64:
-; RV64ZBP:       # %bb.0:
-; RV64ZBP-NEXT:    orc.b a0, a0
-; RV64ZBP-NEXT:    ret
-  %and1 = shl i64 %a, 1
-  %shl1 = and i64 %and1, -6148914691236517206
-  %and1b = lshr i64 %a, 1
-  %shr1 = and i64 %and1b, 6148914691236517205
-  %or1 = or i64 %shr1, %a
-  %or1b = or i64 %or1, %shl1
-  %and2 = shl i64 %or1b, 2
-  %shl2 = and i64 %and2, -3689348814741910324
-  %and2b = lshr i64 %or1b, 2
-  %shr2 = and i64 %and2b, 3689348814741910323
-  %or2 = or i64 %shr2, %or1b
-  %or2b = or i64 %or2, %shl2
-  %and3 = shl i64 %or2b, 4
-  %shl3 = and i64 %and3, -1085102592571150096
-  %and3b = lshr i64 %or2b, 4
-  %shr3 = and i64 %and3b, 1085102592571150095
-  %or3 = or i64 %shr3, %or2b
-  %or3b = or i64 %or3, %shl3
-  ret i64 %or3b
-}
-
-define signext i32 @gorc8_i32(i32 signext %a) nounwind {
-; RV64I-LABEL: gorc8_i32:
-; RV64I:       # %bb.0:
-; RV64I-NEXT:    slliw a1, a0, 8
-; RV64I-NEXT:    lui a2, 1044496
-; RV64I-NEXT:    addiw a2, a2, -256
-; RV64I-NEXT:    and a1, a1, a2
-; RV64I-NEXT:    srli a2, a0, 8
-; RV64I-NEXT:    lui a3, 4080
-; RV64I-NEXT:    addiw a3, a3, 255
-; RV64I-NEXT:    and a2, a2, a3
-; RV64I-NEXT:    or a0, a2, a0
-; RV64I-NEXT:    or a0, a0, a1
-; RV64I-NEXT:    ret
-;
-; RV64B-LABEL: gorc8_i32:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    gorciw a0, a0, 8
-; RV64B-NEXT:    ret
-;
-; RV64ZBP-LABEL: gorc8_i32:
-; RV64ZBP:       # %bb.0:
-; RV64ZBP-NEXT:    gorciw a0, a0, 8
-; RV64ZBP-NEXT:    ret
-  %and = shl i32 %a, 8
-  %shl = and i32 %and, -16711936
-  %and1 = lshr i32 %a, 8
-  %shr = and i32 %and1, 16711935
-  %or = or i32 %shr, %a
-  %or2 = or i32 %or, %shl
-  ret i32 %or2
-}
-
-define i64 @gorc8_i64(i64 %a) nounwind {
-; RV64I-LABEL: gorc8_i64:
-; RV64I:       # %bb.0:
-; RV64I-NEXT:    slli a1, a0, 8
-; RV64I-NEXT:    lui a2, 1044496
-; RV64I-NEXT:    addiw a2, a2, -255
-; RV64I-NEXT:    slli a2, a2, 16
-; RV64I-NEXT:    addi a2, a2, -255
-; RV64I-NEXT:    slli a2, a2, 16
-; RV64I-NEXT:    addi a2, a2, -256
-; RV64I-NEXT:    and a1, a1, a2
-; RV64I-NEXT:    srli a2, a0, 8
-; RV64I-NEXT:    lui a3, 4080
-; RV64I-NEXT:    addiw a3, a3, 255
-; RV64I-NEXT:    slli a3, a3, 16
-; RV64I-NEXT:    addi a3, a3, 255
-; RV64I-NEXT:    slli a3, a3, 16
-; RV64I-NEXT:    addi a3, a3, 255
-; RV64I-NEXT:    and a2, a2, a3
-; RV64I-NEXT:    or a0, a2, a0
-; RV64I-NEXT:    or a0, a0, a1
-; RV64I-NEXT:    ret
-;
-; RV64B-LABEL: gorc8_i64:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    orc8.h a0, a0
-; RV64B-NEXT:    ret
-;
-; RV64ZBP-LABEL: gorc8_i64:
-; RV64ZBP:       # %bb.0:
-; RV64ZBP-NEXT:    orc8.h a0, a0
-; RV64ZBP-NEXT:    ret
-  %and = shl i64 %a, 8
-  %shl = and i64 %and, -71777214294589696
-  %and1 = lshr i64 %a, 8
-  %shr = and i64 %and1, 71777214294589695
-  %or = or i64 %shr, %a
-  %or2 = or i64 %or, %shl
-  ret i64 %or2
-}
-
-define signext i32 @gorc16_i32(i32 signext %a) nounwind {
-; RV64I-LABEL: gorc16_i32:
-; RV64I:       # %bb.0:
-; RV64I-NEXT:    slliw a1, a0, 16
-; RV64I-NEXT:    srliw a2, a0, 16
-; RV64I-NEXT:    or a0, a2, a0
-; RV64I-NEXT:    or a0, a0, a1
-; RV64I-NEXT:    ret
-;
-; RV64B-LABEL: gorc16_i32:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    gorciw a0, a0, 16
-; RV64B-NEXT:    ret
-;
-; RV64ZBP-LABEL: gorc16_i32:
-; RV64ZBP:       # %bb.0:
-; RV64ZBP-NEXT:    gorciw a0, a0, 16
-; RV64ZBP-NEXT:    ret
-  %shl = shl i32 %a, 16
-  %shr = lshr i32 %a, 16
-  %or = or i32 %shr, %a
-  %or2 = or i32 %or, %shl
-  ret i32 %or2
-}
-
-define i32 @gorc16_rotl_i32(i32 %a) nounwind {
-; RV64I-LABEL: gorc16_rotl_i32:
-; RV64I:       # %bb.0:
-; RV64I-NEXT:    srliw a1, a0, 16
-; RV64I-NEXT:    slliw a2, a0, 16
-; RV64I-NEXT:    or a1, a2, a1
-; RV64I-NEXT:    or a0, a1, a0
-; RV64I-NEXT:    ret
-;
-; RV64B-LABEL: gorc16_rotl_i32:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    gorciw a0, a0, 16
-; RV64B-NEXT:    ret
-;
-; RV64ZBP-LABEL: gorc16_rotl_i32:
-; RV64ZBP:       # %bb.0:
-; RV64ZBP-NEXT:    gorciw a0, a0, 16
-; RV64ZBP-NEXT:    ret
-  %rot = tail call i32 @llvm.fshl.i32(i32 %a, i32 %a, i32 16)
-  %or = or i32 %rot, %a
-  ret i32 %or
-}
-
-define i32 @gorc16_rotr_i32(i32 %a) nounwind {
-; RV64I-LABEL: gorc16_rotr_i32:
-; RV64I:       # %bb.0:
-; RV64I-NEXT:    slliw a1, a0, 16
-; RV64I-NEXT:    srliw a2, a0, 16
-; RV64I-NEXT:    or a1, a2, a1
-; RV64I-NEXT:    or a0, a1, a0
-; RV64I-NEXT:    ret
-;
-; RV64B-LABEL: gorc16_rotr_i32:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    gorciw a0, a0, 16
-; RV64B-NEXT:    ret
-;
-; RV64ZBP-LABEL: gorc16_rotr_i32:
-; RV64ZBP:       # %bb.0:
-; RV64ZBP-NEXT:    gorciw a0, a0, 16
-; RV64ZBP-NEXT:    ret
-  %rot = tail call i32 @llvm.fshr.i32(i32 %a, i32 %a, i32 16)
-  %or = or i32 %rot, %a
-  ret i32 %or
-}
-
-define i64 @gorc16_i64(i64 %a) nounwind {
-; RV64I-LABEL: gorc16_i64:
-; RV64I:       # %bb.0:
-; RV64I-NEXT:    slli a1, a0, 16
-; RV64I-NEXT:    lui a2, 983041
-; RV64I-NEXT:    slli a3, a2, 4
-; RV64I-NEXT:    addi a3, a3, -1
-; RV64I-NEXT:    slli a3, a3, 16
-; RV64I-NEXT:    and a1, a1, a3
-; RV64I-NEXT:    srli a3, a0, 16
-; RV64I-NEXT:    slli a2, a2, 20
-; RV64I-NEXT:    addi a2, a2, -1
-; RV64I-NEXT:    srli a2, a2, 16
-; RV64I-NEXT:    and a2, a3, a2
-; RV64I-NEXT:    or a0, a2, a0
-; RV64I-NEXT:    or a0, a0, a1
-; RV64I-NEXT:    ret
-;
-; RV64B-LABEL: gorc16_i64:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    orc16.w a0, a0
-; RV64B-NEXT:    ret
-;
-; RV64ZBP-LABEL: gorc16_i64:
-; RV64ZBP:       # %bb.0:
-; RV64ZBP-NEXT:    orc16.w a0, a0
-; RV64ZBP-NEXT:    ret
-  %and = shl i64 %a, 16
-  %shl = and i64 %and, -281470681808896
-  %and1 = lshr i64 %a, 16
-  %shr = and i64 %and1, 281470681808895
-  %or = or i64 %shr, %a
-  %or2 = or i64 %or, %shl
-  ret i64 %or2
-}
-
-define i64 @gorc32(i64 %a) nounwind {
-; RV64I-LABEL: gorc32:
-; RV64I:       # %bb.0:
-; RV64I-NEXT:    slli a1, a0, 32
-; RV64I-NEXT:    srli a2, a0, 32
-; RV64I-NEXT:    or a0, a2, a0
-; RV64I-NEXT:    or a0, a0, a1
-; RV64I-NEXT:    ret
-;
-; RV64B-LABEL: gorc32:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    orc32 a0, a0
-; RV64B-NEXT:    ret
-;
-; RV64ZBP-LABEL: gorc32:
-; RV64ZBP:       # %bb.0:
-; RV64ZBP-NEXT:    orc32 a0, a0
-; RV64ZBP-NEXT:    ret
-  %shl = shl i64 %a, 32
-  %shr = lshr i64 %a, 32
-  %or = or i64 %shr, %a
-  %or2 = or i64 %or, %shl
-  ret i64 %or2
-}
-
-; gorc2, gorc2 -> gorc2
-define signext i32 @gorc2b_i32(i32 signext %a) nounwind {
-; RV64I-LABEL: gorc2b_i32:
-; RV64I:       # %bb.0:
-; RV64I-NEXT:    slliw a1, a0, 2
-; RV64I-NEXT:    lui a2, 838861
-; RV64I-NEXT:    addiw a2, a2, -820
-; RV64I-NEXT:    and a1, a1, a2
-; RV64I-NEXT:    srli a3, a0, 2
-; RV64I-NEXT:    lui a4, 209715
-; RV64I-NEXT:    addiw a4, a4, 819
-; RV64I-NEXT:    and a3, a3, a4
-; RV64I-NEXT:    or a0, a3, a0
-; RV64I-NEXT:    or a0, a0, a1
-; RV64I-NEXT:    slliw a1, a0, 2
-; RV64I-NEXT:    and a1, a1, a2
-; RV64I-NEXT:    srli a2, a0, 2
-; RV64I-NEXT:    and a2, a2, a4
-; RV64I-NEXT:    or a0, a2, a0
-; RV64I-NEXT:    or a0, a0, a1
-; RV64I-NEXT:    ret
-;
-; RV64B-LABEL: gorc2b_i32:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    gorciw a0, a0, 2
-; RV64B-NEXT:    ret
-;
-; RV64ZBP-LABEL: gorc2b_i32:
-; RV64ZBP:       # %bb.0:
-; RV64ZBP-NEXT:    gorciw a0, a0, 2
-; RV64ZBP-NEXT:    ret
-  %and1 = shl i32 %a, 2
-  %shl1 = and i32 %and1, -858993460
-  %and1b = lshr i32 %a, 2
-  %shr1 = and i32 %and1b, 858993459
-  %or1 = or i32 %shr1, %a
-  %or1b = or i32 %or1, %shl1
-  %and2 = shl i32 %or1b, 2
-  %shl2 = and i32 %and2, -858993460
-  %and2b = lshr i32 %or1b, 2
-  %shr2 = and i32 %and2b, 858993459
-  %or2 = or i32 %shr2, %or1b
-  %or2b = or i32 %or2, %shl2
-  ret i32 %or2b
-}
-
-; gorc2, gorc2 -> gorc2
-define i64 @gorc2b_i64(i64 %a) nounwind {
-; RV64I-LABEL: gorc2b_i64:
-; RV64I:       # %bb.0:
-; RV64I-NEXT:    slli a1, a0, 2
-; RV64I-NEXT:    lui a2, 1035469
-; RV64I-NEXT:    addiw a2, a2, -819
-; RV64I-NEXT:    slli a2, a2, 12
-; RV64I-NEXT:    addi a2, a2, -819
-; RV64I-NEXT:    slli a2, a2, 12
-; RV64I-NEXT:    addi a2, a2, -819
-; RV64I-NEXT:    slli a2, a2, 12
-; RV64I-NEXT:    addi a2, a2, -820
-; RV64I-NEXT:    and a1, a1, a2
-; RV64I-NEXT:    srli a3, a0, 2
-; RV64I-NEXT:    lui a4, 13107
-; RV64I-NEXT:    addiw a4, a4, 819
-; RV64I-NEXT:    slli a4, a4, 12
-; RV64I-NEXT:    addi a4, a4, 819
-; RV64I-NEXT:    slli a4, a4, 12
-; RV64I-NEXT:    addi a4, a4, 819
-; RV64I-NEXT:    slli a4, a4, 12
-; RV64I-NEXT:    addi a4, a4, 819
-; RV64I-NEXT:    and a3, a3, a4
-; RV64I-NEXT:    or a0, a3, a0
-; RV64I-NEXT:    or a0, a0, a1
-; RV64I-NEXT:    slli a1, a0, 2
-; RV64I-NEXT:    and a1, a1, a2
-; RV64I-NEXT:    srli a2, a0, 2
-; RV64I-NEXT:    and a2, a2, a4
-; RV64I-NEXT:    or a0, a2, a0
-; RV64I-NEXT:    or a0, a0, a1
-; RV64I-NEXT:    ret
-;
-; RV64B-LABEL: gorc2b_i64:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    orc2.n a0, a0
-; RV64B-NEXT:    ret
-;
-; RV64ZBP-LABEL: gorc2b_i64:
-; RV64ZBP:       # %bb.0:
-; RV64ZBP-NEXT:    orc2.n a0, a0
-; RV64ZBP-NEXT:    ret
-  %and1 = shl i64 %a, 2
-  %shl1 = and i64 %and1, -3689348814741910324
-  %and1b = lshr i64 %a, 2
-  %shr1 = and i64 %and1b, 3689348814741910323
-  %or1 = or i64 %shr1, %a
-  %or1b = or i64 %or1, %shl1
-  %and2 = shl i64 %or1b, 2
-  %shl2 = and i64 %and2, -3689348814741910324
-  %and2b = lshr i64 %or1b, 2
-  %shr2 = and i64 %and2b, 3689348814741910323
-  %or2 = or i64 %shr2, %or1b
-  %or2b = or i64 %or2, %shl2
-  ret i64 %or2b
-}
-
-; gorc1, gorc2, gorc1 -> gorc2
-define signext i32 @gorc3b_i32(i32 signext %a) nounwind {
-; RV64I-LABEL: gorc3b_i32:
-; RV64I:       # %bb.0:
-; RV64I-NEXT:    slliw a1, a0, 1
-; RV64I-NEXT:    lui a2, 699051
-; RV64I-NEXT:    addiw a2, a2, -1366
-; RV64I-NEXT:    and a1, a1, a2
-; RV64I-NEXT:    srli a3, a0, 1
-; RV64I-NEXT:    lui a4, 349525
-; RV64I-NEXT:    addiw a4, a4, 1365
-; RV64I-NEXT:    and a3, a3, a4
-; RV64I-NEXT:    or a0, a3, a0
-; RV64I-NEXT:    or a0, a0, a1
-; RV64I-NEXT:    slliw a1, a0, 2
-; RV64I-NEXT:    lui a3, 838861
-; RV64I-NEXT:    addiw a3, a3, -820
-; RV64I-NEXT:    and a1, a1, a3
-; RV64I-NEXT:    srli a3, a0, 2
-; RV64I-NEXT:    lui a5, 209715
-; RV64I-NEXT:    addiw a5, a5, 819
-; RV64I-NEXT:    and a3, a3, a5
-; RV64I-NEXT:    or a0, a3, a0
-; RV64I-NEXT:    or a0, a0, a1
-; RV64I-NEXT:    slli a1, a0, 1
-; RV64I-NEXT:    and a1, a1, a2
-; RV64I-NEXT:    srli a2, a0, 1
-; RV64I-NEXT:    and a2, a2, a4
-; RV64I-NEXT:    or a0, a2, a0
-; RV64I-NEXT:    or a0, a0, a1
-; RV64I-NEXT:    sext.w a0, a0
-; RV64I-NEXT:    ret
-;
-; RV64B-LABEL: gorc3b_i32:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    gorciw a0, a0, 3
-; RV64B-NEXT:    ret
-;
-; RV64ZBP-LABEL: gorc3b_i32:
-; RV64ZBP:       # %bb.0:
-; RV64ZBP-NEXT:    gorciw a0, a0, 3
-; RV64ZBP-NEXT:    ret
-  %and1 = shl i32 %a, 1
-  %shl1 = and i32 %and1, -1431655766
-  %and1b = lshr i32 %a, 1
-  %shr1 = and i32 %and1b, 1431655765
-  %or1 = or i32 %shr1, %a
-  %or1b = or i32 %or1, %shl1
-  %and2 = shl i32 %or1b, 2
-  %shl2 = and i32 %and2, -858993460
-  %and2b = lshr i32 %or1b, 2
-  %shr2 = and i32 %and2b, 858993459
-  %or2 = or i32 %shr2, %or1b
-  %or2b = or i32 %or2, %shl2
-  %and3 = shl i32 %or2b, 1
-  %shl3 = and i32 %and3, -1431655766
-  %and3b = lshr i32 %or2b, 1
-  %shr3 = and i32 %and3b, 1431655765
-  %or3 = or i32 %shr3, %or2b
-  %or3b = or i32 %or3, %shl3
-  ret i32 %or3b
-}
-
-; gorc1, gorc2, gorc1 -> gorc2
-define i64 @gorc3b_i64(i64 %a) nounwind {
-; RV64I-LABEL: gorc3b_i64:
-; RV64I:       # %bb.0:
-; RV64I-NEXT:    slli a1, a0, 1
-; RV64I-NEXT:    lui a2, 1026731
-; RV64I-NEXT:    addiw a2, a2, -1365
-; RV64I-NEXT:    slli a2, a2, 12
-; RV64I-NEXT:    addi a2, a2, -1365
-; RV64I-NEXT:    slli a2, a2, 12
-; RV64I-NEXT:    addi a2, a2, -1365
-; RV64I-NEXT:    slli a2, a2, 12
-; RV64I-NEXT:    addi a2, a2, -1366
-; RV64I-NEXT:    and a1, a1, a2
-; RV64I-NEXT:    srli a3, a0, 1
-; RV64I-NEXT:    lui a4, 21845
-; RV64I-NEXT:    addiw a4, a4, 1365
-; RV64I-NEXT:    slli a4, a4, 12
-; RV64I-NEXT:    addi a4, a4, 1365
-; RV64I-NEXT:    slli a4, a4, 12
-; RV64I-NEXT:    addi a4, a4, 1365
-; RV64I-NEXT:    slli a4, a4, 12
-; RV64I-NEXT:    addi a4, a4, 1365
-; RV64I-NEXT:    and a3, a3, a4
-; RV64I-NEXT:    or a0, a3, a0
-; RV64I-NEXT:    or a0, a0, a1
-; RV64I-NEXT:    slli a1, a0, 2
-; RV64I-NEXT:    lui a3, 1035469
-; RV64I-NEXT:    addiw a3, a3, -819
-; RV64I-NEXT:    slli a3, a3, 12
-; RV64I-NEXT:    addi a3, a3, -819
-; RV64I-NEXT:    slli a3, a3, 12
-; RV64I-NEXT:    addi a3, a3, -819
-; RV64I-NEXT:    slli a3, a3, 12
-; RV64I-NEXT:    addi a3, a3, -820
-; RV64I-NEXT:    and a1, a1, a3
-; RV64I-NEXT:    srli a3, a0, 2
-; RV64I-NEXT:    lui a5, 13107
-; RV64I-NEXT:    addiw a5, a5, 819
-; RV64I-NEXT:    slli a5, a5, 12
-; RV64I-NEXT:    addi a5, a5, 819
-; RV64I-NEXT:    slli a5, a5, 12
-; RV64I-NEXT:    addi a5, a5, 819
-; RV64I-NEXT:    slli a5, a5, 12
-; RV64I-NEXT:    addi a5, a5, 819
-; RV64I-NEXT:    and a3, a3, a5
-; RV64I-NEXT:    or a0, a3, a0
-; RV64I-NEXT:    or a0, a0, a1
-; RV64I-NEXT:    slli a1, a0, 1
-; RV64I-NEXT:    and a1, a1, a2
-; RV64I-NEXT:    srli a2, a0, 1
-; RV64I-NEXT:    and a2, a2, a4
-; RV64I-NEXT:    or a0, a2, a0
-; RV64I-NEXT:    or a0, a0, a1
-; RV64I-NEXT:    ret
-;
-; RV64B-LABEL: gorc3b_i64:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    orc.n a0, a0
-; RV64B-NEXT:    ret
-;
-; RV64ZBP-LABEL: gorc3b_i64:
-; RV64ZBP:       # %bb.0:
-; RV64ZBP-NEXT:    orc.n a0, a0
-; RV64ZBP-NEXT:    ret
-  %and1 = shl i64 %a, 1
-  %shl1 = and i64 %and1, -6148914691236517206
-  %and1b = lshr i64 %a, 1
-  %shr1 = and i64 %and1b, 6148914691236517205
-  %or1 = or i64 %shr1, %a
-  %or1b = or i64 %or1, %shl1
-  %and2 = shl i64 %or1b, 2
-  %shl2 = and i64 %and2, -3689348814741910324
-  %and2b = lshr i64 %or1b, 2
-  %shr2 = and i64 %and2b, 3689348814741910323
-  %or2 = or i64 %shr2, %or1b
-  %or2b = or i64 %or2, %shl2
-  %and3 = shl i64 %or2b, 1
-  %shl3 = and i64 %and3, -6148914691236517206
-  %and3b = lshr i64 %or2b, 1
-  %shr3 = and i64 %and3b, 6148914691236517205
-  %or3 = or i64 %shr3, %or2b
-  %or3b = or i64 %or3, %shl3
-  ret i64 %or3b
-}
-
-define i64 @gorc32_rotl(i64 %a) nounwind {
-; RV64I-LABEL: gorc32_rotl:
-; RV64I:       # %bb.0:
-; RV64I-NEXT:    srli a1, a0, 32
-; RV64I-NEXT:    slli a2, a0, 32
-; RV64I-NEXT:    or a1, a2, a1
-; RV64I-NEXT:    or a0, a1, a0
-; RV64I-NEXT:    ret
-;
-; RV64B-LABEL: gorc32_rotl:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    orc32 a0, a0
-; RV64B-NEXT:    ret
-;
-; RV64ZBP-LABEL: gorc32_rotl:
-; RV64ZBP:       # %bb.0:
-; RV64ZBP-NEXT:    orc32 a0, a0
-; RV64ZBP-NEXT:    ret
-  %rot = tail call i64 @llvm.fshl.i64(i64 %a, i64 %a, i64 32)
-  %or = or i64 %rot, %a
-  ret i64 %or
-}
-
-define i64 @gorc32_rotr(i64 %a) nounwind {
-; RV64I-LABEL: gorc32_rotr:
-; RV64I:       # %bb.0:
-; RV64I-NEXT:    slli a1, a0, 32
-; RV64I-NEXT:    srli a2, a0, 32
-; RV64I-NEXT:    or a1, a2, a1
-; RV64I-NEXT:    or a0, a1, a0
-; RV64I-NEXT:    ret
-;
-; RV64B-LABEL: gorc32_rotr:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    orc32 a0, a0
-; RV64B-NEXT:    ret
-;
-; RV64ZBP-LABEL: gorc32_rotr:
-; RV64ZBP:       # %bb.0:
-; RV64ZBP-NEXT:    orc32 a0, a0
-; RV64ZBP-NEXT:    ret
-  %rot = tail call i64 @llvm.fshr.i64(i64 %a, i64 %a, i64 32)
-  %or = or i64 %rot, %a
-  ret i64 %or
-}
-
-define signext i32 @grev1_i32(i32 signext %a) nounwind {
-; RV64I-LABEL: grev1_i32:
-; RV64I:       # %bb.0:
-; RV64I-NEXT:    slliw a1, a0, 1
-; RV64I-NEXT:    lui a2, 699051
-; RV64I-NEXT:    addiw a2, a2, -1366
-; RV64I-NEXT:    and a1, a1, a2
-; RV64I-NEXT:    srli a0, a0, 1
-; RV64I-NEXT:    lui a2, 349525
-; RV64I-NEXT:    addiw a2, a2, 1365
-; RV64I-NEXT:    and a0, a0, a2
-; RV64I-NEXT:    or a0, a1, a0
-; RV64I-NEXT:    ret
-;
-; RV64B-LABEL: grev1_i32:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    greviw a0, a0, 1
-; RV64B-NEXT:    ret
-;
-; RV64ZBP-LABEL: grev1_i32:
-; RV64ZBP:       # %bb.0:
-; RV64ZBP-NEXT:    greviw a0, a0, 1
-; RV64ZBP-NEXT:    ret
-  %and = shl i32 %a, 1
-  %shl = and i32 %and, -1431655766
-  %and1 = lshr i32 %a, 1
-  %shr = and i32 %and1, 1431655765
-  %or = or i32 %shl, %shr
-  ret i32 %or
-}
-
-define i64 @grev1_i64(i64 %a) nounwind {
-; RV64I-LABEL: grev1_i64:
-; RV64I:       # %bb.0:
-; RV64I-NEXT:    slli a1, a0, 1
-; RV64I-NEXT:    lui a2, 1026731
-; RV64I-NEXT:    addiw a2, a2, -1365
-; RV64I-NEXT:    slli a2, a2, 12
-; RV64I-NEXT:    addi a2, a2, -1365
-; RV64I-NEXT:    slli a2, a2, 12
-; RV64I-NEXT:    addi a2, a2, -1365
-; RV64I-NEXT:    slli a2, a2, 12
-; RV64I-NEXT:    addi a2, a2, -1366
-; RV64I-NEXT:    and a1, a1, a2
-; RV64I-NEXT:    srli a0, a0, 1
-; RV64I-NEXT:    lui a2, 21845
-; RV64I-NEXT:    addiw a2, a2, 1365
-; RV64I-NEXT:    slli a2, a2, 12
-; RV64I-NEXT:    addi a2, a2, 1365
-; RV64I-NEXT:    slli a2, a2, 12
-; RV64I-NEXT:    addi a2, a2, 1365
-; RV64I-NEXT:    slli a2, a2, 12
-; RV64I-NEXT:    addi a2, a2, 1365
-; RV64I-NEXT:    and a0, a0, a2
-; RV64I-NEXT:    or a0, a1, a0
-; RV64I-NEXT:    ret
-;
-; RV64B-LABEL: grev1_i64:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    rev.p a0, a0
-; RV64B-NEXT:    ret
-;
-; RV64ZBP-LABEL: grev1_i64:
-; RV64ZBP:       # %bb.0:
-; RV64ZBP-NEXT:    rev.p a0, a0
-; RV64ZBP-NEXT:    ret
-  %and = shl i64 %a, 1
-  %shl = and i64 %and, -6148914691236517206
-  %and1 = lshr i64 %a, 1
-  %shr = and i64 %and1, 6148914691236517205
-  %or = or i64 %shl, %shr
-  ret i64 %or
-}
-
-define signext i32 @grev2_i32(i32 signext %a) nounwind {
-; RV64I-LABEL: grev2_i32:
-; RV64I:       # %bb.0:
-; RV64I-NEXT:    slliw a1, a0, 2
-; RV64I-NEXT:    lui a2, 838861
-; RV64I-NEXT:    addiw a2, a2, -820
-; RV64I-NEXT:    and a1, a1, a2
-; RV64I-NEXT:    srli a0, a0, 2
-; RV64I-NEXT:    lui a2, 209715
-; RV64I-NEXT:    addiw a2, a2, 819
-; RV64I-NEXT:    and a0, a0, a2
-; RV64I-NEXT:    or a0, a1, a0
-; RV64I-NEXT:    ret
-;
-; RV64B-LABEL: grev2_i32:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    greviw a0, a0, 2
-; RV64B-NEXT:    ret
-;
-; RV64ZBP-LABEL: grev2_i32:
-; RV64ZBP:       # %bb.0:
-; RV64ZBP-NEXT:    greviw a0, a0, 2
-; RV64ZBP-NEXT:    ret
-  %and = shl i32 %a, 2
-  %shl = and i32 %and, -858993460
-  %and1 = lshr i32 %a, 2
-  %shr = and i32 %and1, 858993459
-  %or = or i32 %shl, %shr
-  ret i32 %or
-}
-
-define i64 @grev2_i64(i64 %a) nounwind {
-; RV64I-LABEL: grev2_i64:
-; RV64I:       # %bb.0:
-; RV64I-NEXT:    slli a1, a0, 2
-; RV64I-NEXT:    lui a2, 1035469
-; RV64I-NEXT:    addiw a2, a2, -819
-; RV64I-NEXT:    slli a2, a2, 12
-; RV64I-NEXT:    addi a2, a2, -819
-; RV64I-NEXT:    slli a2, a2, 12
-; RV64I-NEXT:    addi a2, a2, -819
-; RV64I-NEXT:    slli a2, a2, 12
-; RV64I-NEXT:    addi a2, a2, -820
-; RV64I-NEXT:    and a1, a1, a2
-; RV64I-NEXT:    srli a0, a0, 2
-; RV64I-NEXT:    lui a2, 13107
-; RV64I-NEXT:    addiw a2, a2, 819
-; RV64I-NEXT:    slli a2, a2, 12
-; RV64I-NEXT:    addi a2, a2, 819
-; RV64I-NEXT:    slli a2, a2, 12
-; RV64I-NEXT:    addi a2, a2, 819
-; RV64I-NEXT:    slli a2, a2, 12
-; RV64I-NEXT:    addi a2, a2, 819
-; RV64I-NEXT:    and a0, a0, a2
-; RV64I-NEXT:    or a0, a1, a0
-; RV64I-NEXT:    ret
-;
-; RV64B-LABEL: grev2_i64:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    rev2.n a0, a0
-; RV64B-NEXT:    ret
-;
-; RV64ZBP-LABEL: grev2_i64:
-; RV64ZBP:       # %bb.0:
-; RV64ZBP-NEXT:    rev2.n a0, a0
-; RV64ZBP-NEXT:    ret
-  %and = shl i64 %a, 2
-  %shl = and i64 %and, -3689348814741910324
-  %and1 = lshr i64 %a, 2
-  %shr = and i64 %and1, 3689348814741910323
-  %or = or i64 %shl, %shr
-  ret i64 %or
-}
-
-define signext i32 @grev3_i32(i32 signext %a) nounwind {
-; RV64I-LABEL: grev3_i32:
-; RV64I:       # %bb.0:
-; RV64I-NEXT:    slliw a1, a0, 1
-; RV64I-NEXT:    lui a2, 699051
-; RV64I-NEXT:    addiw a2, a2, -1366
-; RV64I-NEXT:    and a1, a1, a2
-; RV64I-NEXT:    srli a0, a0, 1
-; RV64I-NEXT:    lui a2, 349525
-; RV64I-NEXT:    addiw a2, a2, 1365
-; RV64I-NEXT:    and a0, a0, a2
-; RV64I-NEXT:    or a0, a1, a0
-; RV64I-NEXT:    slliw a1, a0, 2
-; RV64I-NEXT:    lui a2, 838861
-; RV64I-NEXT:    addiw a2, a2, -820
-; RV64I-NEXT:    and a1, a1, a2
-; RV64I-NEXT:    srli a0, a0, 2
-; RV64I-NEXT:    lui a2, 209715
-; RV64I-NEXT:    addiw a2, a2, 819
-; RV64I-NEXT:    and a0, a0, a2
-; RV64I-NEXT:    or a0, a1, a0
-; RV64I-NEXT:    ret
-;
-; RV64B-LABEL: grev3_i32:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    greviw a0, a0, 3
-; RV64B-NEXT:    ret
-;
-; RV64ZBP-LABEL: grev3_i32:
-; RV64ZBP:       # %bb.0:
-; RV64ZBP-NEXT:    greviw a0, a0, 3
-; RV64ZBP-NEXT:    ret
-  %and1 = shl i32 %a, 1
-  %shl1 = and i32 %and1, -1431655766
-  %and1b = lshr i32 %a, 1
-  %shr1 = and i32 %and1b, 1431655765
-  %or1 = or i32 %shl1, %shr1
-  %and2 = shl i32 %or1, 2
-  %shl2 = and i32 %and2, -858993460
-  %and2b = lshr i32 %or1, 2
-  %shr2 = and i32 %and2b, 858993459
-  %or2 = or i32 %shl2, %shr2
-  ret i32 %or2
-}
-
-define i64 @grev3_i64(i64 %a) nounwind {
-; RV64I-LABEL: grev3_i64:
-; RV64I:       # %bb.0:
-; RV64I-NEXT:    slli a1, a0, 1
-; RV64I-NEXT:    lui a2, 1026731
-; RV64I-NEXT:    addiw a2, a2, -1365
-; RV64I-NEXT:    slli a2, a2, 12
-; RV64I-NEXT:    addi a2, a2, -1365
-; RV64I-NEXT:    slli a2, a2, 12
-; RV64I-NEXT:    addi a2, a2, -1365
-; RV64I-NEXT:    slli a2, a2, 12
-; RV64I-NEXT:    addi a2, a2, -1366
-; RV64I-NEXT:    and a1, a1, a2
-; RV64I-NEXT:    srli a0, a0, 1
-; RV64I-NEXT:    lui a2, 21845
-; RV64I-NEXT:    addiw a2, a2, 1365
-; RV64I-NEXT:    slli a2, a2, 12
-; RV64I-NEXT:    addi a2, a2, 1365
-; RV64I-NEXT:    slli a2, a2, 12
-; RV64I-NEXT:    addi a2, a2, 1365
-; RV64I-NEXT:    slli a2, a2, 12
-; RV64I-NEXT:    addi a2, a2, 1365
-; RV64I-NEXT:    and a0, a0, a2
-; RV64I-NEXT:    or a0, a1, a0
-; RV64I-NEXT:    slli a1, a0, 2
-; RV64I-NEXT:    lui a2, 1035469
-; RV64I-NEXT:    addiw a2, a2, -819
-; RV64I-NEXT:    slli a2, a2, 12
-; RV64I-NEXT:    addi a2, a2, -819
-; RV64I-NEXT:    slli a2, a2, 12
-; RV64I-NEXT:    addi a2, a2, -819
-; RV64I-NEXT:    slli a2, a2, 12
-; RV64I-NEXT:    addi a2, a2, -820
-; RV64I-NEXT:    and a1, a1, a2
-; RV64I-NEXT:    srli a0, a0, 2
-; RV64I-NEXT:    lui a2, 13107
-; RV64I-NEXT:    addiw a2, a2, 819
-; RV64I-NEXT:    slli a2, a2, 12
-; RV64I-NEXT:    addi a2, a2, 819
-; RV64I-NEXT:    slli a2, a2, 12
-; RV64I-NEXT:    addi a2, a2, 819
-; RV64I-NEXT:    slli a2, a2, 12
-; RV64I-NEXT:    addi a2, a2, 819
-; RV64I-NEXT:    and a0, a0, a2
-; RV64I-NEXT:    or a0, a1, a0
-; RV64I-NEXT:    ret
-;
-; RV64B-LABEL: grev3_i64:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    rev.n a0, a0
-; RV64B-NEXT:    ret
-;
-; RV64ZBP-LABEL: grev3_i64:
-; RV64ZBP:       # %bb.0:
-; RV64ZBP-NEXT:    rev.n a0, a0
-; RV64ZBP-NEXT:    ret
-  %and1 = shl i64 %a, 1
-  %shl1 = and i64 %and1, -6148914691236517206
-  %and1b = lshr i64 %a, 1
-  %shr1 = and i64 %and1b, 6148914691236517205
-  %or1 = or i64 %shl1, %shr1
-  %and2 = shl i64 %or1, 2
-  %shl2 = and i64 %and2, -3689348814741910324
-  %and2b = lshr i64 %or1, 2
-  %shr2 = and i64 %and2b, 3689348814741910323
-  %or2 = or i64 %shl2, %shr2
-  ret i64 %or2
-}
-
-define signext i32 @grev4_i32(i32 signext %a) nounwind {
-; RV64I-LABEL: grev4_i32:
-; RV64I:       # %bb.0:
-; RV64I-NEXT:    slliw a1, a0, 4
-; RV64I-NEXT:    lui a2, 986895
-; RV64I-NEXT:    addiw a2, a2, 240
-; RV64I-NEXT:    and a1, a1, a2
-; RV64I-NEXT:    srli a0, a0, 4
-; RV64I-NEXT:    lui a2, 61681
-; RV64I-NEXT:    addiw a2, a2, -241
-; RV64I-NEXT:    and a0, a0, a2
-; RV64I-NEXT:    or a0, a1, a0
-; RV64I-NEXT:    ret
-;
-; RV64B-LABEL: grev4_i32:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    greviw a0, a0, 4
-; RV64B-NEXT:    ret
-;
-; RV64ZBP-LABEL: grev4_i32:
-; RV64ZBP:       # %bb.0:
-; RV64ZBP-NEXT:    greviw a0, a0, 4
-; RV64ZBP-NEXT:    ret
-  %and = shl i32 %a, 4
-  %shl = and i32 %and, -252645136
-  %and1 = lshr i32 %a, 4
-  %shr = and i32 %and1, 252645135
-  %or = or i32 %shl, %shr
-  ret i32 %or
-}
-
-define i64 @grev4_i64(i64 %a) nounwind {
-; RV64I-LABEL: grev4_i64:
-; RV64I:       # %bb.0:
-; RV64I-NEXT:    slli a1, a0, 4
-; RV64I-NEXT:    lui a2, 1044721
-; RV64I-NEXT:    addiw a2, a2, -241
-; RV64I-NEXT:    slli a2, a2, 12
-; RV64I-NEXT:    addi a2, a2, 241
-; RV64I-NEXT:    slli a2, a2, 12
-; RV64I-NEXT:    addi a2, a2, -241
-; RV64I-NEXT:    slli a2, a2, 12
-; RV64I-NEXT:    addi a2, a2, 240
-; RV64I-NEXT:    and a1, a1, a2
-; RV64I-NEXT:    srli a0, a0, 4
-; RV64I-NEXT:    lui a2, 3855
-; RV64I-NEXT:    addiw a2, a2, 241
-; RV64I-NEXT:    slli a2, a2, 12
-; RV64I-NEXT:    addi a2, a2, -241
-; RV64I-NEXT:    slli a2, a2, 12
-; RV64I-NEXT:    addi a2, a2, 241
-; RV64I-NEXT:    slli a2, a2, 12
-; RV64I-NEXT:    addi a2, a2, -241
-; RV64I-NEXT:    and a0, a0, a2
-; RV64I-NEXT:    or a0, a1, a0
-; RV64I-NEXT:    ret
-;
-; RV64B-LABEL: grev4_i64:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    rev4.b a0, a0
-; RV64B-NEXT:    ret
-;
-; RV64ZBP-LABEL: grev4_i64:
-; RV64ZBP:       # %bb.0:
-; RV64ZBP-NEXT:    rev4.b a0, a0
-; RV64ZBP-NEXT:    ret
-  %and = shl i64 %a, 4
-  %shl = and i64 %and, -1085102592571150096
-  %and1 = lshr i64 %a, 4
-  %shr = and i64 %and1, 1085102592571150095
-  %or = or i64 %shl, %shr
-  ret i64 %or
-}
-
-define signext i32 @grev5_i32(i32 signext %a) nounwind {
-; RV64I-LABEL: grev5_i32:
-; RV64I:       # %bb.0:
-; RV64I-NEXT:    slliw a1, a0, 1
-; RV64I-NEXT:    lui a2, 699051
-; RV64I-NEXT:    addiw a2, a2, -1366
-; RV64I-NEXT:    and a1, a1, a2
-; RV64I-NEXT:    srli a0, a0, 1
-; RV64I-NEXT:    lui a2, 349525
-; RV64I-NEXT:    addiw a2, a2, 1365
-; RV64I-NEXT:    and a0, a0, a2
-; RV64I-NEXT:    or a0, a1, a0
-; RV64I-NEXT:    slliw a1, a0, 4
-; RV64I-NEXT:    lui a2, 986895
-; RV64I-NEXT:    addiw a2, a2, 240
-; RV64I-NEXT:    and a1, a1, a2
-; RV64I-NEXT:    srli a0, a0, 4
-; RV64I-NEXT:    lui a2, 61681
-; RV64I-NEXT:    addiw a2, a2, -241
-; RV64I-NEXT:    and a0, a0, a2
-; RV64I-NEXT:    or a0, a1, a0
-; RV64I-NEXT:    ret
-;
-; RV64B-LABEL: grev5_i32:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    greviw a0, a0, 5
-; RV64B-NEXT:    ret
-;
-; RV64ZBP-LABEL: grev5_i32:
-; RV64ZBP:       # %bb.0:
-; RV64ZBP-NEXT:    greviw a0, a0, 5
-; RV64ZBP-NEXT:    ret
-  %and1 = shl i32 %a, 1
-  %shl1 = and i32 %and1, -1431655766
-  %and1b = lshr i32 %a, 1
-  %shr1 = and i32 %and1b, 1431655765
-  %or1 = or i32 %shl1, %shr1
-  %and2 = shl i32 %or1, 4
-  %shl2 = and i32 %and2, -252645136
-  %and2b = lshr i32 %or1, 4
-  %shr2 = and i32 %and2b, 252645135
-  %or2 = or i32 %shl2, %shr2
-  ret i32 %or2
-}
-
-define i64 @grev5_i64(i64 %a) nounwind {
-; RV64I-LABEL: grev5_i64:
-; RV64I:       # %bb.0:
-; RV64I-NEXT:    slli a1, a0, 1
-; RV64I-NEXT:    lui a2, 1026731
-; RV64I-NEXT:    addiw a2, a2, -1365
-; RV64I-NEXT:    slli a2, a2, 12
-; RV64I-NEXT:    addi a2, a2, -1365
-; RV64I-NEXT:    slli a2, a2, 12
-; RV64I-NEXT:    addi a2, a2, -1365
-; RV64I-NEXT:    slli a2, a2, 12
-; RV64I-NEXT:    addi a2, a2, -1366
-; RV64I-NEXT:    and a1, a1, a2
-; RV64I-NEXT:    srli a0, a0, 1
-; RV64I-NEXT:    lui a2, 21845
-; RV64I-NEXT:    addiw a2, a2, 1365
-; RV64I-NEXT:    slli a2, a2, 12
-; RV64I-NEXT:    addi a2, a2, 1365
-; RV64I-NEXT:    slli a2, a2, 12
-; RV64I-NEXT:    addi a2, a2, 1365
-; RV64I-NEXT:    slli a2, a2, 12
-; RV64I-NEXT:    addi a2, a2, 1365
-; RV64I-NEXT:    and a0, a0, a2
-; RV64I-NEXT:    or a0, a1, a0
-; RV64I-NEXT:    slli a1, a0, 4
-; RV64I-NEXT:    lui a2, 1044721
-; RV64I-NEXT:    addiw a2, a2, -241
-; RV64I-NEXT:    slli a2, a2, 12
-; RV64I-NEXT:    addi a2, a2, 241
-; RV64I-NEXT:    slli a2, a2, 12
-; RV64I-NEXT:    addi a2, a2, -241
-; RV64I-NEXT:    slli a2, a2, 12
-; RV64I-NEXT:    addi a2, a2, 240
-; RV64I-NEXT:    and a1, a1, a2
-; RV64I-NEXT:    srli a0, a0, 4
-; RV64I-NEXT:    lui a2, 3855
-; RV64I-NEXT:    addiw a2, a2, 241
-; RV64I-NEXT:    slli a2, a2, 12
-; RV64I-NEXT:    addi a2, a2, -241
-; RV64I-NEXT:    slli a2, a2, 12
-; RV64I-NEXT:    addi a2, a2, 241
-; RV64I-NEXT:    slli a2, a2, 12
-; RV64I-NEXT:    addi a2, a2, -241
-; RV64I-NEXT:    and a0, a0, a2
-; RV64I-NEXT:    or a0, a1, a0
-; RV64I-NEXT:    ret
-;
-; RV64B-LABEL: grev5_i64:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    grevi a0, a0, 5
-; RV64B-NEXT:    ret
-;
-; RV64ZBP-LABEL: grev5_i64:
-; RV64ZBP:       # %bb.0:
-; RV64ZBP-NEXT:    grevi a0, a0, 5
-; RV64ZBP-NEXT:    ret
-  %and1 = shl i64 %a, 1
-  %shl1 = and i64 %and1, -6148914691236517206
-  %and1b = lshr i64 %a, 1
-  %shr1 = and i64 %and1b, 6148914691236517205
-  %or1 = or i64 %shl1, %shr1
-
-  %and2 = shl i64 %or1, 4
-  %shl2 = and i64 %and2, -1085102592571150096
-  %and2b = lshr i64 %or1, 4
-  %shr2 = and i64 %and2b, 1085102592571150095
-  %or2 = or i64 %shl2, %shr2
-  ret i64 %or2
-}
-
-define signext i32 @grev6_i32(i32 signext %a) nounwind {
-; RV64I-LABEL: grev6_i32:
-; RV64I:       # %bb.0:
-; RV64I-NEXT:    slliw a1, a0, 2
-; RV64I-NEXT:    lui a2, 838861
-; RV64I-NEXT:    addiw a2, a2, -820
-; RV64I-NEXT:    and a1, a1, a2
-; RV64I-NEXT:    srli a0, a0, 2
-; RV64I-NEXT:    lui a2, 209715
-; RV64I-NEXT:    addiw a2, a2, 819
-; RV64I-NEXT:    and a0, a0, a2
-; RV64I-NEXT:    or a0, a1, a0
-; RV64I-NEXT:    slliw a1, a0, 4
-; RV64I-NEXT:    lui a2, 986895
-; RV64I-NEXT:    addiw a2, a2, 240
-; RV64I-NEXT:    and a1, a1, a2
-; RV64I-NEXT:    srli a0, a0, 4
-; RV64I-NEXT:    lui a2, 61681
-; RV64I-NEXT:    addiw a2, a2, -241
-; RV64I-NEXT:    and a0, a0, a2
-; RV64I-NEXT:    or a0, a1, a0
-; RV64I-NEXT:    ret
-;
-; RV64B-LABEL: grev6_i32:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    greviw a0, a0, 6
-; RV64B-NEXT:    ret
-;
-; RV64ZBP-LABEL: grev6_i32:
-; RV64ZBP:       # %bb.0:
-; RV64ZBP-NEXT:    greviw a0, a0, 6
-; RV64ZBP-NEXT:    ret
-  %and1 = shl i32 %a, 2
-  %shl1 = and i32 %and1, -858993460
-  %and1b = lshr i32 %a, 2
-  %shr1 = and i32 %and1b, 858993459
-  %or1 = or i32 %shl1, %shr1
-  %and2 = shl i32 %or1, 4
-  %shl2 = and i32 %and2, -252645136
-  %and2b = lshr i32 %or1, 4
-  %shr2 = and i32 %and2b, 252645135
-  %or2 = or i32 %shl2, %shr2
-  ret i32 %or2
-}
-
-define i64 @grev6_i64(i64 %a) nounwind {
-; RV64I-LABEL: grev6_i64:
-; RV64I:       # %bb.0:
-; RV64I-NEXT:    slli a1, a0, 2
-; RV64I-NEXT:    lui a2, 1035469
-; RV64I-NEXT:    addiw a2, a2, -819
-; RV64I-NEXT:    slli a2, a2, 12
-; RV64I-NEXT:    addi a2, a2, -819
-; RV64I-NEXT:    slli a2, a2, 12
-; RV64I-NEXT:    addi a2, a2, -819
-; RV64I-NEXT:    slli a2, a2, 12
-; RV64I-NEXT:    addi a2, a2, -820
-; RV64I-NEXT:    and a1, a1, a2
-; RV64I-NEXT:    srli a0, a0, 2
-; RV64I-NEXT:    lui a2, 13107
-; RV64I-NEXT:    addiw a2, a2, 819
-; RV64I-NEXT:    slli a2, a2, 12
-; RV64I-NEXT:    addi a2, a2, 819
-; RV64I-NEXT:    slli a2, a2, 12
-; RV64I-NEXT:    addi a2, a2, 819
-; RV64I-NEXT:    slli a2, a2, 12
-; RV64I-NEXT:    addi a2, a2, 819
-; RV64I-NEXT:    and a0, a0, a2
-; RV64I-NEXT:    or a0, a1, a0
-; RV64I-NEXT:    slli a1, a0, 4
-; RV64I-NEXT:    lui a2, 1044721
-; RV64I-NEXT:    addiw a2, a2, -241
-; RV64I-NEXT:    slli a2, a2, 12
-; RV64I-NEXT:    addi a2, a2, 241
-; RV64I-NEXT:    slli a2, a2, 12
-; RV64I-NEXT:    addi a2, a2, -241
-; RV64I-NEXT:    slli a2, a2, 12
-; RV64I-NEXT:    addi a2, a2, 240
-; RV64I-NEXT:    and a1, a1, a2
-; RV64I-NEXT:    srli a0, a0, 4
-; RV64I-NEXT:    lui a2, 3855
-; RV64I-NEXT:    addiw a2, a2, 241
-; RV64I-NEXT:    slli a2, a2, 12
-; RV64I-NEXT:    addi a2, a2, -241
-; RV64I-NEXT:    slli a2, a2, 12
-; RV64I-NEXT:    addi a2, a2, 241
-; RV64I-NEXT:    slli a2, a2, 12
-; RV64I-NEXT:    addi a2, a2, -241
-; RV64I-NEXT:    and a0, a0, a2
-; RV64I-NEXT:    or a0, a1, a0
-; RV64I-NEXT:    ret
-;
-; RV64B-LABEL: grev6_i64:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    rev2.b a0, a0
-; RV64B-NEXT:    ret
-;
-; RV64ZBP-LABEL: grev6_i64:
-; RV64ZBP:       # %bb.0:
-; RV64ZBP-NEXT:    rev2.b a0, a0
-; RV64ZBP-NEXT:    ret
-  %and1 = shl i64 %a, 2
-  %shl1 = and i64 %and1, -3689348814741910324
-  %and1b = lshr i64 %a, 2
-  %shr1 = and i64 %and1b, 3689348814741910323
-  %or1 = or i64 %shl1, %shr1
-  %and2 = shl i64 %or1, 4
-  %shl2 = and i64 %and2, -1085102592571150096
-  %and2b = lshr i64 %or1, 4
-  %shr2 = and i64 %and2b, 1085102592571150095
-  %or2 = or i64 %shl2, %shr2
-  ret i64 %or2
-}
-
-define signext i32 @grev7_i32(i32 signext %a) nounwind {
-; RV64I-LABEL: grev7_i32:
-; RV64I:       # %bb.0:
-; RV64I-NEXT:    slliw a1, a0, 1
-; RV64I-NEXT:    lui a2, 699051
-; RV64I-NEXT:    addiw a2, a2, -1366
-; RV64I-NEXT:    and a1, a1, a2
-; RV64I-NEXT:    srli a0, a0, 1
-; RV64I-NEXT:    lui a2, 349525
-; RV64I-NEXT:    addiw a2, a2, 1365
-; RV64I-NEXT:    and a0, a0, a2
-; RV64I-NEXT:    or a0, a1, a0
-; RV64I-NEXT:    slliw a1, a0, 2
-; RV64I-NEXT:    lui a2, 838861
-; RV64I-NEXT:    addiw a2, a2, -820
-; RV64I-NEXT:    and a1, a1, a2
-; RV64I-NEXT:    srli a0, a0, 2
-; RV64I-NEXT:    lui a2, 209715
-; RV64I-NEXT:    addiw a2, a2, 819
-; RV64I-NEXT:    and a0, a0, a2
-; RV64I-NEXT:    or a0, a1, a0
-; RV64I-NEXT:    slliw a1, a0, 4
-; RV64I-NEXT:    lui a2, 986895
-; RV64I-NEXT:    addiw a2, a2, 240
-; RV64I-NEXT:    and a1, a1, a2
-; RV64I-NEXT:    srli a0, a0, 4
-; RV64I-NEXT:    lui a2, 61681
-; RV64I-NEXT:    addiw a2, a2, -241
-; RV64I-NEXT:    and a0, a0, a2
-; RV64I-NEXT:    or a0, a1, a0
-; RV64I-NEXT:    ret
-;
-; RV64B-LABEL: grev7_i32:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    greviw a0, a0, 7
-; RV64B-NEXT:    ret
-;
-; RV64ZBP-LABEL: grev7_i32:
-; RV64ZBP:       # %bb.0:
-; RV64ZBP-NEXT:    greviw a0, a0, 7
-; RV64ZBP-NEXT:    ret
-  %and1 = shl i32 %a, 1
-  %shl1 = and i32 %and1, -1431655766
-  %and1b = lshr i32 %a, 1
-  %shr1 = and i32 %and1b, 1431655765
-  %or1 = or i32 %shl1, %shr1
-  %and2 = shl i32 %or1, 2
-  %shl2 = and i32 %and2, -858993460
-  %and2b = lshr i32 %or1, 2
-  %shr2 = and i32 %and2b, 858993459
-  %or2 = or i32 %shl2, %shr2
-  %and3 = shl i32 %or2, 4
-  %shl3 = and i32 %and3, -252645136
-  %and3b = lshr i32 %or2, 4
-  %shr3 = and i32 %and3b, 252645135
-  %or3 = or i32 %shl3, %shr3
-  ret i32 %or3
-}
-
-define i64 @grev7_i64(i64 %a) nounwind {
-; RV64I-LABEL: grev7_i64:
-; RV64I:       # %bb.0:
-; RV64I-NEXT:    slli a1, a0, 1
-; RV64I-NEXT:    lui a2, 1026731
-; RV64I-NEXT:    addiw a2, a2, -1365
-; RV64I-NEXT:    slli a2, a2, 12
-; RV64I-NEXT:    addi a2, a2, -1365
-; RV64I-NEXT:    slli a2, a2, 12
-; RV64I-NEXT:    addi a2, a2, -1365
-; RV64I-NEXT:    slli a2, a2, 12
-; RV64I-NEXT:    addi a2, a2, -1366
-; RV64I-NEXT:    and a1, a1, a2
-; RV64I-NEXT:    srli a0, a0, 1
-; RV64I-NEXT:    lui a2, 21845
-; RV64I-NEXT:    addiw a2, a2, 1365
-; RV64I-NEXT:    slli a2, a2, 12
-; RV64I-NEXT:    addi a2, a2, 1365
-; RV64I-NEXT:    slli a2, a2, 12
-; RV64I-NEXT:    addi a2, a2, 1365
-; RV64I-NEXT:    slli a2, a2, 12
-; RV64I-NEXT:    addi a2, a2, 1365
-; RV64I-NEXT:    and a0, a0, a2
-; RV64I-NEXT:    or a0, a1, a0
-; RV64I-NEXT:    slli a1, a0, 2
-; RV64I-NEXT:    lui a2, 1035469
-; RV64I-NEXT:    addiw a2, a2, -819
-; RV64I-NEXT:    slli a2, a2, 12
-; RV64I-NEXT:    addi a2, a2, -819
-; RV64I-NEXT:    slli a2, a2, 12
-; RV64I-NEXT:    addi a2, a2, -819
-; RV64I-NEXT:    slli a2, a2, 12
-; RV64I-NEXT:    addi a2, a2, -820
-; RV64I-NEXT:    and a1, a1, a2
-; RV64I-NEXT:    srli a0, a0, 2
-; RV64I-NEXT:    lui a2, 13107
-; RV64I-NEXT:    addiw a2, a2, 819
-; RV64I-NEXT:    slli a2, a2, 12
-; RV64I-NEXT:    addi a2, a2, 819
-; RV64I-NEXT:    slli a2, a2, 12
-; RV64I-NEXT:    addi a2, a2, 819
-; RV64I-NEXT:    slli a2, a2, 12
-; RV64I-NEXT:    addi a2, a2, 819
-; RV64I-NEXT:    and a0, a0, a2
-; RV64I-NEXT:    or a0, a1, a0
-; RV64I-NEXT:    slli a1, a0, 4
-; RV64I-NEXT:    lui a2, 1044721
-; RV64I-NEXT:    addiw a2, a2, -241
-; RV64I-NEXT:    slli a2, a2, 12
-; RV64I-NEXT:    addi a2, a2, 241
-; RV64I-NEXT:    slli a2, a2, 12
-; RV64I-NEXT:    addi a2, a2, -241
-; RV64I-NEXT:    slli a2, a2, 12
-; RV64I-NEXT:    addi a2, a2, 240
-; RV64I-NEXT:    and a1, a1, a2
-; RV64I-NEXT:    srli a0, a0, 4
-; RV64I-NEXT:    lui a2, 3855
-; RV64I-NEXT:    addiw a2, a2, 241
-; RV64I-NEXT:    slli a2, a2, 12
-; RV64I-NEXT:    addi a2, a2, -241
-; RV64I-NEXT:    slli a2, a2, 12
-; RV64I-NEXT:    addi a2, a2, 241
-; RV64I-NEXT:    slli a2, a2, 12
-; RV64I-NEXT:    addi a2, a2, -241
-; RV64I-NEXT:    and a0, a0, a2
-; RV64I-NEXT:    or a0, a1, a0
-; RV64I-NEXT:    ret
-;
-; RV64B-LABEL: grev7_i64:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    rev.b a0, a0
-; RV64B-NEXT:    ret
-;
-; RV64ZBP-LABEL: grev7_i64:
-; RV64ZBP:       # %bb.0:
-; RV64ZBP-NEXT:    rev.b a0, a0
-; RV64ZBP-NEXT:    ret
-  %and1 = shl i64 %a, 1
-  %shl1 = and i64 %and1, -6148914691236517206
-  %and1b = lshr i64 %a, 1
-  %shr1 = and i64 %and1b, 6148914691236517205
-  %or1 = or i64 %shl1, %shr1
-  %and2 = shl i64 %or1, 2
-  %shl2 = and i64 %and2, -3689348814741910324
-  %and2b = lshr i64 %or1, 2
-  %shr2 = and i64 %and2b, 3689348814741910323
-  %or2 = or i64 %shl2, %shr2
-  %and3 = shl i64 %or2, 4
-  %shl3 = and i64 %and3, -1085102592571150096
-  %and3b = lshr i64 %or2, 4
-  %shr3 = and i64 %and3b, 1085102592571150095
-  %or3 = or i64 %shl3, %shr3
-  ret i64 %or3
-}
-
-define signext i32 @grev8_i32(i32 signext %a) nounwind {
-; RV64I-LABEL: grev8_i32:
-; RV64I:       # %bb.0:
-; RV64I-NEXT:    slliw a1, a0, 8
-; RV64I-NEXT:    lui a2, 1044496
-; RV64I-NEXT:    addiw a2, a2, -256
-; RV64I-NEXT:    and a1, a1, a2
-; RV64I-NEXT:    srli a0, a0, 8
-; RV64I-NEXT:    lui a2, 4080
-; RV64I-NEXT:    addiw a2, a2, 255
-; RV64I-NEXT:    and a0, a0, a2
-; RV64I-NEXT:    or a0, a1, a0
-; RV64I-NEXT:    ret
-;
-; RV64B-LABEL: grev8_i32:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    greviw a0, a0, 8
-; RV64B-NEXT:    ret
-;
-; RV64ZBP-LABEL: grev8_i32:
-; RV64ZBP:       # %bb.0:
-; RV64ZBP-NEXT:    greviw a0, a0, 8
-; RV64ZBP-NEXT:    ret
-  %and = shl i32 %a, 8
-  %shl = and i32 %and, -16711936
-  %and1 = lshr i32 %a, 8
-  %shr = and i32 %and1, 16711935
-  %or = or i32 %shl, %shr
-  ret i32 %or
-}
-
-define i64 @grev8_i64(i64 %a) nounwind {
-; RV64I-LABEL: grev8_i64:
-; RV64I:       # %bb.0:
-; RV64I-NEXT:    slli a1, a0, 8
-; RV64I-NEXT:    lui a2, 1044496
-; RV64I-NEXT:    addiw a2, a2, -255
-; RV64I-NEXT:    slli a2, a2, 16
-; RV64I-NEXT:    addi a2, a2, -255
-; RV64I-NEXT:    slli a2, a2, 16
-; RV64I-NEXT:    addi a2, a2, -256
-; RV64I-NEXT:    and a1, a1, a2
-; RV64I-NEXT:    srli a0, a0, 8
-; RV64I-NEXT:    lui a2, 4080
-; RV64I-NEXT:    addiw a2, a2, 255
-; RV64I-NEXT:    slli a2, a2, 16
-; RV64I-NEXT:    addi a2, a2, 255
-; RV64I-NEXT:    slli a2, a2, 16
-; RV64I-NEXT:    addi a2, a2, 255
-; RV64I-NEXT:    and a0, a0, a2
-; RV64I-NEXT:    or a0, a1, a0
-; RV64I-NEXT:    ret
-;
-; RV64B-LABEL: grev8_i64:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    rev8.h a0, a0
-; RV64B-NEXT:    ret
-;
-; RV64ZBP-LABEL: grev8_i64:
-; RV64ZBP:       # %bb.0:
-; RV64ZBP-NEXT:    rev8.h a0, a0
-; RV64ZBP-NEXT:    ret
-  %and = shl i64 %a, 8
-  %shl = and i64 %and, -71777214294589696
-  %and1 = lshr i64 %a, 8
-  %shr = and i64 %and1, 71777214294589695
-  %or = or i64 %shl, %shr
-  ret i64 %or
-}
-
-define signext i32 @grev16_i32(i32 signext %a) nounwind {
-; RV64I-LABEL: grev16_i32:
-; RV64I:       # %bb.0:
-; RV64I-NEXT:    slliw a1, a0, 16
-; RV64I-NEXT:    srliw a0, a0, 16
-; RV64I-NEXT:    or a0, a1, a0
-; RV64I-NEXT:    ret
-;
-; RV64B-LABEL: grev16_i32:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    greviw a0, a0, 16
-; RV64B-NEXT:    ret
-;
-; RV64ZBP-LABEL: grev16_i32:
-; RV64ZBP:       # %bb.0:
-; RV64ZBP-NEXT:    greviw a0, a0, 16
-; RV64ZBP-NEXT:    ret
-  %shl = shl i32 %a, 16
-  %shr = lshr i32 %a, 16
-  %or = or i32 %shl, %shr
-  ret i32 %or
-}
-
-declare i32 @llvm.fshl.i32(i32, i32, i32)
-declare i32 @llvm.fshr.i32(i32, i32, i32)
-
-define signext i32 @grev16_i32_fshl(i32 signext %a) nounwind {
-; RV64I-LABEL: grev16_i32_fshl:
-; RV64I:       # %bb.0:
-; RV64I-NEXT:    srliw a1, a0, 16
-; RV64I-NEXT:    slliw a0, a0, 16
-; RV64I-NEXT:    or a0, a0, a1
-; RV64I-NEXT:    ret
-;
-; RV64B-LABEL: grev16_i32_fshl:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    roriw a0, a0, 16
-; RV64B-NEXT:    ret
-;
-; RV64ZBP-LABEL: grev16_i32_fshl:
-; RV64ZBP:       # %bb.0:
-; RV64ZBP-NEXT:    roriw a0, a0, 16
-; RV64ZBP-NEXT:    ret
-  %or = tail call i32 @llvm.fshl.i32(i32 %a, i32 %a, i32 16)
-  ret i32 %or
-}
-
-define signext i32 @grev16_i32_fshr(i32 signext %a) nounwind {
-; RV64I-LABEL: grev16_i32_fshr:
-; RV64I:       # %bb.0:
-; RV64I-NEXT:    slliw a1, a0, 16
-; RV64I-NEXT:    srliw a0, a0, 16
-; RV64I-NEXT:    or a0, a0, a1
-; RV64I-NEXT:    ret
-;
-; RV64B-LABEL: grev16_i32_fshr:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    roriw a0, a0, 16
-; RV64B-NEXT:    ret
-;
-; RV64ZBP-LABEL: grev16_i32_fshr:
-; RV64ZBP:       # %bb.0:
-; RV64ZBP-NEXT:    roriw a0, a0, 16
-; RV64ZBP-NEXT:    ret
-  %or = tail call i32 @llvm.fshr.i32(i32 %a, i32 %a, i32 16)
-  ret i32 %or
-}
-
-define i64 @grev16_i64(i64 %a) nounwind {
-; RV64I-LABEL: grev16_i64:
-; RV64I:       # %bb.0:
-; RV64I-NEXT:    slli a1, a0, 16
-; RV64I-NEXT:    lui a2, 983041
-; RV64I-NEXT:    slli a3, a2, 4
-; RV64I-NEXT:    addi a3, a3, -1
-; RV64I-NEXT:    slli a3, a3, 16
-; RV64I-NEXT:    and a1, a1, a3
-; RV64I-NEXT:    srli a0, a0, 16
-; RV64I-NEXT:    slli a2, a2, 20
-; RV64I-NEXT:    addi a2, a2, -1
-; RV64I-NEXT:    srli a2, a2, 16
-; RV64I-NEXT:    and a0, a0, a2
-; RV64I-NEXT:    or a0, a1, a0
-; RV64I-NEXT:    ret
-;
-; RV64B-LABEL: grev16_i64:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    rev16.w a0, a0
-; RV64B-NEXT:    ret
-;
-; RV64ZBP-LABEL: grev16_i64:
-; RV64ZBP:       # %bb.0:
-; RV64ZBP-NEXT:    rev16.w a0, a0
-; RV64ZBP-NEXT:    ret
-  %and = shl i64 %a, 16
-  %shl = and i64 %and, -281470681808896
-  %and1 = lshr i64 %a, 16
-  %shr = and i64 %and1, 281470681808895
-  %or = or i64 %shl, %shr
-  ret i64 %or
-}
-
-define i64 @grev32(i64 %a) nounwind {
-; RV64I-LABEL: grev32:
-; RV64I:       # %bb.0:
-; RV64I-NEXT:    slli a1, a0, 32
-; RV64I-NEXT:    srli a0, a0, 32
-; RV64I-NEXT:    or a0, a1, a0
-; RV64I-NEXT:    ret
-;
-; RV64B-LABEL: grev32:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    rori a0, a0, 32
-; RV64B-NEXT:    ret
-;
-; RV64ZBP-LABEL: grev32:
-; RV64ZBP:       # %bb.0:
-; RV64ZBP-NEXT:    rori a0, a0, 32
-; RV64ZBP-NEXT:    ret
-  %shl = shl i64 %a, 32
-  %shr = lshr i64 %a, 32
-  %or = or i64 %shl, %shr
-  ret i64 %or
-}
-
-define signext i32 @grev3b_i32(i32 signext %a) nounwind {
-; RV64I-LABEL: grev3b_i32:
-; RV64I:       # %bb.0:
-; RV64I-NEXT:    slliw a1, a0, 2
-; RV64I-NEXT:    lui a2, 838861
-; RV64I-NEXT:    addiw a2, a2, -820
-; RV64I-NEXT:    and a1, a1, a2
-; RV64I-NEXT:    srli a0, a0, 2
-; RV64I-NEXT:    lui a2, 209715
-; RV64I-NEXT:    addiw a2, a2, 819
-; RV64I-NEXT:    and a0, a0, a2
-; RV64I-NEXT:    or a0, a1, a0
-; RV64I-NEXT:    slliw a1, a0, 1
-; RV64I-NEXT:    lui a2, 699051
-; RV64I-NEXT:    addiw a2, a2, -1366
-; RV64I-NEXT:    and a1, a1, a2
-; RV64I-NEXT:    srli a0, a0, 1
-; RV64I-NEXT:    lui a2, 349525
-; RV64I-NEXT:    addiw a2, a2, 1365
-; RV64I-NEXT:    and a0, a0, a2
-; RV64I-NEXT:    or a0, a1, a0
-; RV64I-NEXT:    ret
-;
-; RV64B-LABEL: grev3b_i32:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    greviw a0, a0, 3
-; RV64B-NEXT:    ret
-;
-; RV64ZBP-LABEL: grev3b_i32:
-; RV64ZBP:       # %bb.0:
-; RV64ZBP-NEXT:    greviw a0, a0, 3
-; RV64ZBP-NEXT:    ret
-  %and2 = shl i32 %a, 2
-  %shl2 = and i32 %and2, -858993460
-  %and2b = lshr i32 %a, 2
-  %shr2 = and i32 %and2b, 858993459
-  %or2 = or i32 %shl2, %shr2
-  %and1 = shl i32 %or2, 1
-  %shl1 = and i32 %and1, -1431655766
-  %and1b = lshr i32 %or2, 1
-  %shr1 = and i32 %and1b, 1431655765
-  %or1 = or i32 %shl1, %shr1
-  ret i32 %or1
-}
-
-define i64 @grev3b_i64(i64 %a) nounwind {
-; RV64I-LABEL: grev3b_i64:
-; RV64I:       # %bb.0:
-; RV64I-NEXT:    slli a1, a0, 2
-; RV64I-NEXT:    lui a2, 1035469
-; RV64I-NEXT:    addiw a2, a2, -819
-; RV64I-NEXT:    slli a2, a2, 12
-; RV64I-NEXT:    addi a2, a2, -819
-; RV64I-NEXT:    slli a2, a2, 12
-; RV64I-NEXT:    addi a2, a2, -819
-; RV64I-NEXT:    slli a2, a2, 12
-; RV64I-NEXT:    addi a2, a2, -820
-; RV64I-NEXT:    and a1, a1, a2
-; RV64I-NEXT:    srli a0, a0, 2
-; RV64I-NEXT:    lui a2, 13107
-; RV64I-NEXT:    addiw a2, a2, 819
-; RV64I-NEXT:    slli a2, a2, 12
-; RV64I-NEXT:    addi a2, a2, 819
-; RV64I-NEXT:    slli a2, a2, 12
-; RV64I-NEXT:    addi a2, a2, 819
-; RV64I-NEXT:    slli a2, a2, 12
-; RV64I-NEXT:    addi a2, a2, 819
-; RV64I-NEXT:    and a0, a0, a2
-; RV64I-NEXT:    or a0, a1, a0
-; RV64I-NEXT:    slli a1, a0, 1
-; RV64I-NEXT:    lui a2, 1026731
-; RV64I-NEXT:    addiw a2, a2, -1365
-; RV64I-NEXT:    slli a2, a2, 12
-; RV64I-NEXT:    addi a2, a2, -1365
-; RV64I-NEXT:    slli a2, a2, 12
-; RV64I-NEXT:    addi a2, a2, -1365
-; RV64I-NEXT:    slli a2, a2, 12
-; RV64I-NEXT:    addi a2, a2, -1366
-; RV64I-NEXT:    and a1, a1, a2
-; RV64I-NEXT:    srli a0, a0, 1
-; RV64I-NEXT:    lui a2, 21845
-; RV64I-NEXT:    addiw a2, a2, 1365
-; RV64I-NEXT:    slli a2, a2, 12
-; RV64I-NEXT:    addi a2, a2, 1365
-; RV64I-NEXT:    slli a2, a2, 12
-; RV64I-NEXT:    addi a2, a2, 1365
-; RV64I-NEXT:    slli a2, a2, 12
-; RV64I-NEXT:    addi a2, a2, 1365
-; RV64I-NEXT:    and a0, a0, a2
-; RV64I-NEXT:    or a0, a1, a0
-; RV64I-NEXT:    ret
-;
-; RV64B-LABEL: grev3b_i64:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    rev.n a0, a0
-; RV64B-NEXT:    ret
-;
-; RV64ZBP-LABEL: grev3b_i64:
-; RV64ZBP:       # %bb.0:
-; RV64ZBP-NEXT:    rev.n a0, a0
-; RV64ZBP-NEXT:    ret
-  %and2 = shl i64 %a, 2
-  %shl2 = and i64 %and2, -3689348814741910324
-  %and2b = lshr i64 %a, 2
-  %shr2 = and i64 %and2b, 3689348814741910323
-  %or2 = or i64 %shl2, %shr2
-  %and1 = shl i64 %or2, 1
-  %shl1 = and i64 %and1, -6148914691236517206
-  %and1b = lshr i64 %or2, 1
-  %shr1 = and i64 %and1b, 6148914691236517205
-  %or1 = or i64 %shl1, %shr1
-  ret i64 %or1
-}
-
-; grev1, grev2, grev1 -> grev2
-define signext i32 @grev2b_i32(i32 signext %a) nounwind {
-; RV64I-LABEL: grev2b_i32:
-; RV64I:       # %bb.0:
-; RV64I-NEXT:    slliw a1, a0, 1
-; RV64I-NEXT:    lui a2, 699051
-; RV64I-NEXT:    addiw a2, a2, -1366
-; RV64I-NEXT:    and a1, a1, a2
-; RV64I-NEXT:    srli a0, a0, 1
-; RV64I-NEXT:    lui a3, 349525
-; RV64I-NEXT:    addiw a3, a3, 1365
-; RV64I-NEXT:    and a0, a0, a3
-; RV64I-NEXT:    or a0, a1, a0
-; RV64I-NEXT:    slliw a1, a0, 2
-; RV64I-NEXT:    lui a4, 838861
-; RV64I-NEXT:    addiw a4, a4, -820
-; RV64I-NEXT:    and a1, a1, a4
-; RV64I-NEXT:    srli a0, a0, 2
-; RV64I-NEXT:    lui a4, 209715
-; RV64I-NEXT:    addiw a4, a4, 819
-; RV64I-NEXT:    and a0, a0, a4
-; RV64I-NEXT:    or a0, a1, a0
-; RV64I-NEXT:    slliw a1, a0, 1
-; RV64I-NEXT:    and a1, a1, a2
-; RV64I-NEXT:    srli a0, a0, 1
-; RV64I-NEXT:    and a0, a0, a3
-; RV64I-NEXT:    or a0, a1, a0
-; RV64I-NEXT:    ret
-;
-; RV64B-LABEL: grev2b_i32:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    greviw a0, a0, 2
-; RV64B-NEXT:    ret
-;
-; RV64ZBP-LABEL: grev2b_i32:
-; RV64ZBP:       # %bb.0:
-; RV64ZBP-NEXT:    greviw a0, a0, 2
-; RV64ZBP-NEXT:    ret
-  %and1 = shl i32 %a, 1
-  %shl1 = and i32 %and1, -1431655766
-  %and1b = lshr i32 %a, 1
-  %shr1 = and i32 %and1b, 1431655765
-  %or1 = or i32 %shl1, %shr1
-  %and2 = shl i32 %or1, 2
-  %shl2 = and i32 %and2, -858993460
-  %and2b = lshr i32 %or1, 2
-  %shr2 = and i32 %and2b, 858993459
-  %or2 = or i32 %shl2, %shr2
-  %and3 = shl i32 %or2, 1
-  %shl3 = and i32 %and3, -1431655766
-  %and3b = lshr i32 %or2, 1
-  %shr3 = and i32 %and3b, 1431655765
-  %or3 = or i32 %shl3, %shr3
-  ret i32 %or3
-}
-
-; grev1, grev2, grev1 -> grev2
-define i64 @grev2b_i64(i64 %a) nounwind {
-; RV64I-LABEL: grev2b_i64:
-; RV64I:       # %bb.0:
-; RV64I-NEXT:    slli a1, a0, 1
-; RV64I-NEXT:    lui a2, 1026731
-; RV64I-NEXT:    addiw a2, a2, -1365
-; RV64I-NEXT:    slli a2, a2, 12
-; RV64I-NEXT:    addi a2, a2, -1365
-; RV64I-NEXT:    slli a2, a2, 12
-; RV64I-NEXT:    addi a2, a2, -1365
-; RV64I-NEXT:    slli a2, a2, 12
-; RV64I-NEXT:    addi a2, a2, -1366
-; RV64I-NEXT:    and a1, a1, a2
-; RV64I-NEXT:    srli a0, a0, 1
-; RV64I-NEXT:    lui a3, 21845
-; RV64I-NEXT:    addiw a3, a3, 1365
-; RV64I-NEXT:    slli a3, a3, 12
-; RV64I-NEXT:    addi a3, a3, 1365
-; RV64I-NEXT:    slli a3, a3, 12
-; RV64I-NEXT:    addi a3, a3, 1365
-; RV64I-NEXT:    slli a3, a3, 12
-; RV64I-NEXT:    addi a3, a3, 1365
-; RV64I-NEXT:    and a0, a0, a3
-; RV64I-NEXT:    or a0, a1, a0
-; RV64I-NEXT:    slli a1, a0, 2
-; RV64I-NEXT:    lui a4, 1035469
-; RV64I-NEXT:    addiw a4, a4, -819
-; RV64I-NEXT:    slli a4, a4, 12
-; RV64I-NEXT:    addi a4, a4, -819
-; RV64I-NEXT:    slli a4, a4, 12
-; RV64I-NEXT:    addi a4, a4, -819
-; RV64I-NEXT:    slli a4, a4, 12
-; RV64I-NEXT:    addi a4, a4, -820
-; RV64I-NEXT:    and a1, a1, a4
-; RV64I-NEXT:    srli a0, a0, 2
-; RV64I-NEXT:    lui a4, 13107
-; RV64I-NEXT:    addiw a4, a4, 819
-; RV64I-NEXT:    slli a4, a4, 12
-; RV64I-NEXT:    addi a4, a4, 819
-; RV64I-NEXT:    slli a4, a4, 12
-; RV64I-NEXT:    addi a4, a4, 819
-; RV64I-NEXT:    slli a4, a4, 12
-; RV64I-NEXT:    addi a4, a4, 819
-; RV64I-NEXT:    and a0, a0, a4
-; RV64I-NEXT:    or a0, a1, a0
-; RV64I-NEXT:    slli a1, a0, 1
-; RV64I-NEXT:    and a1, a1, a2
-; RV64I-NEXT:    srli a0, a0, 1
-; RV64I-NEXT:    and a0, a0, a3
-; RV64I-NEXT:    or a0, a1, a0
-; RV64I-NEXT:    ret
-;
-; RV64B-LABEL: grev2b_i64:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    rev2.n a0, a0
-; RV64B-NEXT:    ret
-;
-; RV64ZBP-LABEL: grev2b_i64:
-; RV64ZBP:       # %bb.0:
-; RV64ZBP-NEXT:    rev2.n a0, a0
-; RV64ZBP-NEXT:    ret
-  %and1 = shl i64 %a, 1
-  %shl1 = and i64 %and1, -6148914691236517206
-  %and1b = lshr i64 %a, 1
-  %shr1 = and i64 %and1b, 6148914691236517205
-  %or1 = or i64 %shl1, %shr1
-  %and2 = shl i64 %or1, 2
-  %shl2 = and i64 %and2, -3689348814741910324
-  %and2b = lshr i64 %or1, 2
-  %shr2 = and i64 %and2b, 3689348814741910323
-  %or2 = or i64 %shl2, %shr2
-  %and3 = shl i64 %or2, 1
-  %shl3 = and i64 %and3, -6148914691236517206
-  %and3b = lshr i64 %or2, 1
-  %shr3 = and i64 %and3b, 6148914691236517205
-  %or3 = or i64 %shl3, %shr3
-  ret i64 %or3
-}
-
-; grev1, grev2, grev1, grev2 -> identity
-define signext i32 @grev0_i32(i32 signext %a) nounwind {
-; RV64I-LABEL: grev0_i32:
-; RV64I:       # %bb.0:
-; RV64I-NEXT:    slliw a1, a0, 1
-; RV64I-NEXT:    lui a2, 699051
-; RV64I-NEXT:    addiw a2, a2, -1366
-; RV64I-NEXT:    and a1, a1, a2
-; RV64I-NEXT:    srli a0, a0, 1
-; RV64I-NEXT:    lui a3, 349525
-; RV64I-NEXT:    addiw a3, a3, 1365
-; RV64I-NEXT:    and a0, a0, a3
-; RV64I-NEXT:    or a0, a1, a0
-; RV64I-NEXT:    slliw a1, a0, 2
-; RV64I-NEXT:    lui a4, 838861
-; RV64I-NEXT:    addiw a4, a4, -820
-; RV64I-NEXT:    and a1, a1, a4
-; RV64I-NEXT:    srli a0, a0, 2
-; RV64I-NEXT:    lui a5, 209715
-; RV64I-NEXT:    addiw a5, a5, 819
-; RV64I-NEXT:    and a0, a0, a5
-; RV64I-NEXT:    or a0, a1, a0
-; RV64I-NEXT:    slliw a1, a0, 1
-; RV64I-NEXT:    and a1, a1, a2
-; RV64I-NEXT:    srli a0, a0, 1
-; RV64I-NEXT:    and a0, a0, a3
-; RV64I-NEXT:    or a0, a1, a0
-; RV64I-NEXT:    slliw a1, a0, 2
-; RV64I-NEXT:    and a1, a1, a4
-; RV64I-NEXT:    srli a0, a0, 2
-; RV64I-NEXT:    and a0, a0, a5
-; RV64I-NEXT:    or a0, a1, a0
-; RV64I-NEXT:    ret
-;
-; RV64B-LABEL: grev0_i32:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    ret
-;
-; RV64ZBP-LABEL: grev0_i32:
-; RV64ZBP:       # %bb.0:
-; RV64ZBP-NEXT:    ret
-  %and1 = shl i32 %a, 1
-  %shl1 = and i32 %and1, -1431655766
-  %and1b = lshr i32 %a, 1
-  %shr1 = and i32 %and1b, 1431655765
-  %or1 = or i32 %shl1, %shr1
-  %and2 = shl i32 %or1, 2
-  %shl2 = and i32 %and2, -858993460
-  %and2b = lshr i32 %or1, 2
-  %shr2 = and i32 %and2b, 858993459
-  %or2 = or i32 %shl2, %shr2
-  %and3 = shl i32 %or2, 1
-  %shl3 = and i32 %and3, -1431655766
-  %and3b = lshr i32 %or2, 1
-  %shr3 = and i32 %and3b, 1431655765
-  %or3 = or i32 %shl3, %shr3
-  %and4 = shl i32 %or3, 2
-  %shl4 = and i32 %and4, -858993460
-  %and4b = lshr i32 %or3, 2
-  %shr4 = and i32 %and4b, 858993459
-  %or4 = or i32 %shl4, %shr4
-  ret i32 %or4
-}
-
-; grev1, grev2, grev1, grev2 -> identity
-define i64 @grev0_i64(i64 %a) nounwind {
-; RV64I-LABEL: grev0_i64:
-; RV64I:       # %bb.0:
-; RV64I-NEXT:    slli a1, a0, 1
-; RV64I-NEXT:    lui a2, 1026731
-; RV64I-NEXT:    addiw a2, a2, -1365
-; RV64I-NEXT:    slli a2, a2, 12
-; RV64I-NEXT:    addi a2, a2, -1365
-; RV64I-NEXT:    slli a2, a2, 12
-; RV64I-NEXT:    addi a2, a2, -1365
-; RV64I-NEXT:    slli a2, a2, 12
-; RV64I-NEXT:    addi a2, a2, -1366
-; RV64I-NEXT:    and a1, a1, a2
-; RV64I-NEXT:    srli a0, a0, 1
-; RV64I-NEXT:    lui a3, 21845
-; RV64I-NEXT:    addiw a3, a3, 1365
-; RV64I-NEXT:    slli a3, a3, 12
-; RV64I-NEXT:    addi a3, a3, 1365
-; RV64I-NEXT:    slli a3, a3, 12
-; RV64I-NEXT:    addi a3, a3, 1365
-; RV64I-NEXT:    slli a3, a3, 12
-; RV64I-NEXT:    addi a3, a3, 1365
-; RV64I-NEXT:    and a0, a0, a3
-; RV64I-NEXT:    or a0, a1, a0
-; RV64I-NEXT:    slli a1, a0, 2
-; RV64I-NEXT:    lui a4, 1035469
-; RV64I-NEXT:    addiw a4, a4, -819
-; RV64I-NEXT:    slli a4, a4, 12
-; RV64I-NEXT:    addi a4, a4, -819
-; RV64I-NEXT:    slli a4, a4, 12
-; RV64I-NEXT:    addi a4, a4, -819
-; RV64I-NEXT:    slli a4, a4, 12
-; RV64I-NEXT:    addi a4, a4, -820
-; RV64I-NEXT:    and a1, a1, a4
-; RV64I-NEXT:    srli a0, a0, 2
-; RV64I-NEXT:    lui a5, 13107
-; RV64I-NEXT:    addiw a5, a5, 819
-; RV64I-NEXT:    slli a5, a5, 12
-; RV64I-NEXT:    addi a5, a5, 819
-; RV64I-NEXT:    slli a5, a5, 12
-; RV64I-NEXT:    addi a5, a5, 819
-; RV64I-NEXT:    slli a5, a5, 12
-; RV64I-NEXT:    addi a5, a5, 819
-; RV64I-NEXT:    and a0, a0, a5
-; RV64I-NEXT:    or a0, a1, a0
-; RV64I-NEXT:    slli a1, a0, 1
-; RV64I-NEXT:    and a1, a1, a2
-; RV64I-NEXT:    srli a0, a0, 1
-; RV64I-NEXT:    and a0, a0, a3
-; RV64I-NEXT:    or a0, a1, a0
-; RV64I-NEXT:    slli a1, a0, 2
-; RV64I-NEXT:    and a1, a1, a4
-; RV64I-NEXT:    srli a0, a0, 2
-; RV64I-NEXT:    and a0, a0, a5
-; RV64I-NEXT:    or a0, a1, a0
-; RV64I-NEXT:    ret
-;
-; RV64B-LABEL: grev0_i64:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    ret
-;
-; RV64ZBP-LABEL: grev0_i64:
-; RV64ZBP:       # %bb.0:
-; RV64ZBP-NEXT:    ret
-  %and1 = shl i64 %a, 1
-  %shl1 = and i64 %and1, -6148914691236517206
-  %and1b = lshr i64 %a, 1
-  %shr1 = and i64 %and1b, 6148914691236517205
-  %or1 = or i64 %shl1, %shr1
-  %and2 = shl i64 %or1, 2
-  %shl2 = and i64 %and2, -3689348814741910324
-  %and2b = lshr i64 %or1, 2
-  %shr2 = and i64 %and2b, 3689348814741910323
-  %or2 = or i64 %shl2, %shr2
-  %and3 = shl i64 %or2, 1
-  %shl3 = and i64 %and3, -6148914691236517206
-  %and3b = lshr i64 %or2, 1
-  %shr3 = and i64 %and3b, 6148914691236517205
-  %or3 = or i64 %shl3, %shr3
-  %and4 = shl i64 %or3, 2
-  %shl4 = and i64 %and4, -3689348814741910324
-  %and4b = lshr i64 %or3, 2
-  %shr4 = and i64 %and4b, 3689348814741910323
-  %or4 = or i64 %shl4, %shr4
-  ret i64 %or4
-}
-
-declare i64 @llvm.fshl.i64(i64, i64, i64)
-declare i64 @llvm.fshr.i64(i64, i64, i64)
-
-define i64 @grev32_fshl(i64 %a) nounwind {
-; RV64I-LABEL: grev32_fshl:
-; RV64I:       # %bb.0:
-; RV64I-NEXT:    srli a1, a0, 32
-; RV64I-NEXT:    slli a0, a0, 32
-; RV64I-NEXT:    or a0, a0, a1
-; RV64I-NEXT:    ret
-;
-; RV64B-LABEL: grev32_fshl:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    rori a0, a0, 32
-; RV64B-NEXT:    ret
-;
-; RV64ZBP-LABEL: grev32_fshl:
-; RV64ZBP:       # %bb.0:
-; RV64ZBP-NEXT:    rori a0, a0, 32
-; RV64ZBP-NEXT:    ret
-  %or = tail call i64 @llvm.fshl.i64(i64 %a, i64 %a, i64 32)
-  ret i64 %or
-}
-
-define i64 @grev32_fshr(i64 %a) nounwind {
-; RV64I-LABEL: grev32_fshr:
-; RV64I:       # %bb.0:
-; RV64I-NEXT:    slli a1, a0, 32
-; RV64I-NEXT:    srli a0, a0, 32
-; RV64I-NEXT:    or a0, a0, a1
-; RV64I-NEXT:    ret
-;
-; RV64B-LABEL: grev32_fshr:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    rori a0, a0, 32
-; RV64B-NEXT:    ret
-;
-; RV64ZBP-LABEL: grev32_fshr:
-; RV64ZBP:       # %bb.0:
-; RV64ZBP-NEXT:    rori a0, a0, 32
-; RV64ZBP-NEXT:    ret
-  %or = tail call i64 @llvm.fshr.i64(i64 %a, i64 %a, i64 32)
-  ret i64 %or
-}
-
-declare i16 @llvm.bswap.i16(i16)
-
-define zeroext i16 @bswap_i16(i16 zeroext %a) nounwind {
-; RV64I-LABEL: bswap_i16:
-; RV64I:       # %bb.0:
-; RV64I-NEXT:    srli a1, a0, 8
-; RV64I-NEXT:    slli a0, a0, 8
-; RV64I-NEXT:    or a0, a0, a1
-; RV64I-NEXT:    lui a1, 16
-; RV64I-NEXT:    addiw a1, a1, -1
-; RV64I-NEXT:    and a0, a0, a1
-; RV64I-NEXT:    ret
-;
-; RV64B-LABEL: bswap_i16:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    greviw a0, a0, 8
-; RV64B-NEXT:    ret
-;
-; RV64ZBP-LABEL: bswap_i16:
-; RV64ZBP:       # %bb.0:
-; RV64ZBP-NEXT:    greviw a0, a0, 8
-; RV64ZBP-NEXT:    ret
-  %1 = tail call i16 @llvm.bswap.i16(i16 %a)
-  ret i16 %1
-}
-
-declare i32 @llvm.bswap.i32(i32)
-
-define signext i32 @bswap_i32(i32 signext %a) nounwind {
-; RV64I-LABEL: bswap_i32:
-; RV64I:       # %bb.0:
-; RV64I-NEXT:    srliw a1, a0, 8
-; RV64I-NEXT:    lui a2, 16
-; RV64I-NEXT:    addiw a2, a2, -256
-; RV64I-NEXT:    and a1, a1, a2
-; RV64I-NEXT:    srliw a2, a0, 24
-; RV64I-NEXT:    or a1, a1, a2
-; RV64I-NEXT:    slli a2, a0, 8
-; RV64I-NEXT:    lui a3, 4080
-; RV64I-NEXT:    and a2, a2, a3
-; RV64I-NEXT:    slliw a0, a0, 24
-; RV64I-NEXT:    or a0, a0, a2
-; RV64I-NEXT:    or a0, a0, a1
-; RV64I-NEXT:    ret
-;
-; RV64B-LABEL: bswap_i32:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    greviw a0, a0, 24
-; RV64B-NEXT:    ret
-;
-; RV64ZBP-LABEL: bswap_i32:
-; RV64ZBP:       # %bb.0:
-; RV64ZBP-NEXT:    greviw a0, a0, 24
-; RV64ZBP-NEXT:    ret
-  %1 = tail call i32 @llvm.bswap.i32(i32 %a)
-  ret i32 %1
-}
-
-; Similar to bswap_i32 but the result is not sign extended.
-define void @bswap_i32_nosext(i32 signext %a, i32* %x) nounwind {
-; RV64I-LABEL: bswap_i32_nosext:
-; RV64I:       # %bb.0:
-; RV64I-NEXT:    srliw a2, a0, 8
-; RV64I-NEXT:    lui a3, 16
-; RV64I-NEXT:    addiw a3, a3, -256
-; RV64I-NEXT:    and a2, a2, a3
-; RV64I-NEXT:    srliw a3, a0, 24
-; RV64I-NEXT:    or a2, a2, a3
-; RV64I-NEXT:    slli a3, a0, 8
-; RV64I-NEXT:    lui a4, 4080
-; RV64I-NEXT:    and a3, a3, a4
-; RV64I-NEXT:    slli a0, a0, 24
-; RV64I-NEXT:    or a0, a0, a3
-; RV64I-NEXT:    or a0, a0, a2
-; RV64I-NEXT:    sw a0, 0(a1)
-; RV64I-NEXT:    ret
-;
-; RV64B-LABEL: bswap_i32_nosext:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    greviw a0, a0, 24
-; RV64B-NEXT:    sw a0, 0(a1)
-; RV64B-NEXT:    ret
-;
-; RV64ZBP-LABEL: bswap_i32_nosext:
-; RV64ZBP:       # %bb.0:
-; RV64ZBP-NEXT:    greviw a0, a0, 24
-; RV64ZBP-NEXT:    sw a0, 0(a1)
-; RV64ZBP-NEXT:    ret
-  %1 = tail call i32 @llvm.bswap.i32(i32 %a)
-  store i32 %1, i32* %x
-  ret void
-}
-
-declare i64 @llvm.bswap.i64(i64)
-
-define i64 @bswap_i64(i64 %a) {
-; RV64I-LABEL: bswap_i64:
-; RV64I:       # %bb.0:
-; RV64I-NEXT:    srli a1, a0, 24
-; RV64I-NEXT:    lui a2, 4080
-; RV64I-NEXT:    and a1, a1, a2
-; RV64I-NEXT:    srli a2, a0, 8
-; RV64I-NEXT:    addi a3, zero, 255
-; RV64I-NEXT:    slli a4, a3, 24
-; RV64I-NEXT:    and a2, a2, a4
-; RV64I-NEXT:    or a1, a2, a1
-; RV64I-NEXT:    srli a2, a0, 40
-; RV64I-NEXT:    lui a4, 16
-; RV64I-NEXT:    addiw a4, a4, -256
-; RV64I-NEXT:    and a2, a2, a4
-; RV64I-NEXT:    srli a4, a0, 56
-; RV64I-NEXT:    or a2, a2, a4
-; RV64I-NEXT:    or a1, a1, a2
-; RV64I-NEXT:    slli a2, a0, 8
-; RV64I-NEXT:    slli a4, a3, 32
-; RV64I-NEXT:    and a2, a2, a4
-; RV64I-NEXT:    slli a4, a0, 24
-; RV64I-NEXT:    slli a5, a3, 40
-; RV64I-NEXT:    and a4, a4, a5
-; RV64I-NEXT:    or a2, a4, a2
-; RV64I-NEXT:    slli a4, a0, 40
-; RV64I-NEXT:    slli a3, a3, 48
-; RV64I-NEXT:    and a3, a4, a3
-; RV64I-NEXT:    slli a0, a0, 56
-; RV64I-NEXT:    or a0, a0, a3
-; RV64I-NEXT:    or a0, a0, a2
-; RV64I-NEXT:    or a0, a0, a1
-; RV64I-NEXT:    ret
-;
-; RV64B-LABEL: bswap_i64:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    rev8 a0, a0
-; RV64B-NEXT:    ret
-;
-; RV64ZBP-LABEL: bswap_i64:
-; RV64ZBP:       # %bb.0:
-; RV64ZBP-NEXT:    rev8 a0, a0
-; RV64ZBP-NEXT:    ret
-  %1 = call i64 @llvm.bswap.i64(i64 %a)
-  ret i64 %1
-}
-
-declare i8 @llvm.bitreverse.i8(i8)
-
-define zeroext i8 @bitreverse_i8(i8 zeroext %a) nounwind {
-; RV64I-LABEL: bitreverse_i8:
-; RV64I:       # %bb.0:
-; RV64I-NEXT:    srli a1, a0, 4
-; RV64I-NEXT:    andi a0, a0, 15
-; RV64I-NEXT:    slli a0, a0, 4
-; RV64I-NEXT:    or a0, a1, a0
-; RV64I-NEXT:    andi a1, a0, 51
-; RV64I-NEXT:    slli a1, a1, 2
-; RV64I-NEXT:    srli a0, a0, 2
-; RV64I-NEXT:    andi a0, a0, 51
-; RV64I-NEXT:    or a0, a0, a1
-; RV64I-NEXT:    andi a1, a0, 85
-; RV64I-NEXT:    slli a1, a1, 1
-; RV64I-NEXT:    srli a0, a0, 1
-; RV64I-NEXT:    andi a0, a0, 85
-; RV64I-NEXT:    or a0, a0, a1
-; RV64I-NEXT:    ret
-;
-; RV64B-LABEL: bitreverse_i8:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    greviw a0, a0, 7
-; RV64B-NEXT:    ret
-;
-; RV64ZBP-LABEL: bitreverse_i8:
-; RV64ZBP:       # %bb.0:
-; RV64ZBP-NEXT:    greviw a0, a0, 7
-; RV64ZBP-NEXT:    ret
-  %1 = tail call i8 @llvm.bitreverse.i8(i8 %a)
-  ret i8 %1
-}
-
-declare i16 @llvm.bitreverse.i16(i16)
-
-define zeroext i16 @bitreverse_i16(i16 zeroext %a) nounwind {
-; RV64I-LABEL: bitreverse_i16:
-; RV64I:       # %bb.0:
-; RV64I-NEXT:    srli a1, a0, 8
-; RV64I-NEXT:    slli a0, a0, 8
-; RV64I-NEXT:    or a0, a0, a1
-; RV64I-NEXT:    srli a1, a0, 4
-; RV64I-NEXT:    lui a2, 1
-; RV64I-NEXT:    addiw a2, a2, -241
-; RV64I-NEXT:    and a1, a1, a2
-; RV64I-NEXT:    and a0, a0, a2
-; RV64I-NEXT:    slli a0, a0, 4
-; RV64I-NEXT:    or a0, a1, a0
-; RV64I-NEXT:    srli a1, a0, 2
-; RV64I-NEXT:    lui a2, 3
-; RV64I-NEXT:    addiw a2, a2, 819
-; RV64I-NEXT:    and a1, a1, a2
-; RV64I-NEXT:    and a0, a0, a2
-; RV64I-NEXT:    slli a0, a0, 2
-; RV64I-NEXT:    or a0, a1, a0
-; RV64I-NEXT:    srli a1, a0, 1
-; RV64I-NEXT:    lui a2, 5
-; RV64I-NEXT:    addiw a2, a2, 1365
-; RV64I-NEXT:    and a1, a1, a2
-; RV64I-NEXT:    and a0, a0, a2
-; RV64I-NEXT:    slli a0, a0, 1
-; RV64I-NEXT:    or a0, a1, a0
-; RV64I-NEXT:    ret
-;
-; RV64B-LABEL: bitreverse_i16:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    greviw a0, a0, 15
-; RV64B-NEXT:    ret
-;
-; RV64ZBP-LABEL: bitreverse_i16:
-; RV64ZBP:       # %bb.0:
-; RV64ZBP-NEXT:    greviw a0, a0, 15
-; RV64ZBP-NEXT:    ret
-  %1 = tail call i16 @llvm.bitreverse.i16(i16 %a)
-  ret i16 %1
-}
-
-declare i32 @llvm.bitreverse.i32(i32)
-
-define signext i32 @bitreverse_i32(i32 signext %a) nounwind {
-; RV64I-LABEL: bitreverse_i32:
-; RV64I:       # %bb.0:
-; RV64I-NEXT:    srliw a1, a0, 8
-; RV64I-NEXT:    lui a2, 16
-; RV64I-NEXT:    addiw a2, a2, -256
-; RV64I-NEXT:    and a1, a1, a2
-; RV64I-NEXT:    srliw a2, a0, 24
-; RV64I-NEXT:    or a1, a1, a2
-; RV64I-NEXT:    slli a2, a0, 8
-; RV64I-NEXT:    lui a3, 4080
-; RV64I-NEXT:    and a2, a2, a3
-; RV64I-NEXT:    slliw a0, a0, 24
-; RV64I-NEXT:    or a0, a0, a2
-; RV64I-NEXT:    or a0, a0, a1
-; RV64I-NEXT:    srli a1, a0, 4
-; RV64I-NEXT:    lui a2, 61681
-; RV64I-NEXT:    addiw a2, a2, -241
-; RV64I-NEXT:    and a1, a1, a2
-; RV64I-NEXT:    and a0, a0, a2
-; RV64I-NEXT:    slliw a0, a0, 4
-; RV64I-NEXT:    or a0, a1, a0
-; RV64I-NEXT:    srli a1, a0, 2
-; RV64I-NEXT:    lui a2, 209715
-; RV64I-NEXT:    addiw a2, a2, 819
-; RV64I-NEXT:    and a1, a1, a2
-; RV64I-NEXT:    and a0, a0, a2
-; RV64I-NEXT:    slliw a0, a0, 2
-; RV64I-NEXT:    or a0, a1, a0
-; RV64I-NEXT:    srli a1, a0, 1
-; RV64I-NEXT:    lui a2, 349525
-; RV64I-NEXT:    addiw a2, a2, 1365
-; RV64I-NEXT:    and a1, a1, a2
-; RV64I-NEXT:    and a0, a0, a2
-; RV64I-NEXT:    slliw a0, a0, 1
-; RV64I-NEXT:    or a0, a1, a0
-; RV64I-NEXT:    ret
-;
-; RV64B-LABEL: bitreverse_i32:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    greviw a0, a0, 31
-; RV64B-NEXT:    ret
-;
-; RV64ZBP-LABEL: bitreverse_i32:
-; RV64ZBP:       # %bb.0:
-; RV64ZBP-NEXT:    greviw a0, a0, 31
-; RV64ZBP-NEXT:    ret
-  %1 = tail call i32 @llvm.bitreverse.i32(i32 %a)
-  ret i32 %1
-}
-
-; Similar to bitreverse_i32 but the result is not sign extended.
-define void @bitreverse_i32_nosext(i32 signext %a, i32* %x) nounwind {
-; RV64I-LABEL: bitreverse_i32_nosext:
-; RV64I:       # %bb.0:
-; RV64I-NEXT:    srliw a2, a0, 8
-; RV64I-NEXT:    lui a3, 16
-; RV64I-NEXT:    addiw a3, a3, -256
-; RV64I-NEXT:    and a2, a2, a3
-; RV64I-NEXT:    srliw a3, a0, 24
-; RV64I-NEXT:    or a2, a2, a3
-; RV64I-NEXT:    slli a3, a0, 8
-; RV64I-NEXT:    lui a4, 4080
-; RV64I-NEXT:    and a3, a3, a4
-; RV64I-NEXT:    slliw a0, a0, 24
-; RV64I-NEXT:    or a0, a0, a3
-; RV64I-NEXT:    or a0, a0, a2
-; RV64I-NEXT:    srli a2, a0, 4
-; RV64I-NEXT:    lui a3, 61681
-; RV64I-NEXT:    addiw a3, a3, -241
-; RV64I-NEXT:    and a2, a2, a3
-; RV64I-NEXT:    and a0, a0, a3
-; RV64I-NEXT:    slliw a0, a0, 4
-; RV64I-NEXT:    or a0, a2, a0
-; RV64I-NEXT:    srli a2, a0, 2
-; RV64I-NEXT:    lui a3, 209715
-; RV64I-NEXT:    addiw a3, a3, 819
-; RV64I-NEXT:    and a2, a2, a3
-; RV64I-NEXT:    and a0, a0, a3
-; RV64I-NEXT:    slliw a0, a0, 2
-; RV64I-NEXT:    or a0, a2, a0
-; RV64I-NEXT:    srli a2, a0, 1
-; RV64I-NEXT:    lui a3, 349525
-; RV64I-NEXT:    addiw a3, a3, 1365
-; RV64I-NEXT:    and a2, a2, a3
-; RV64I-NEXT:    and a0, a0, a3
-; RV64I-NEXT:    slli a0, a0, 1
-; RV64I-NEXT:    or a0, a2, a0
-; RV64I-NEXT:    sw a0, 0(a1)
-; RV64I-NEXT:    ret
-;
-; RV64B-LABEL: bitreverse_i32_nosext:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    greviw a0, a0, 31
-; RV64B-NEXT:    sw a0, 0(a1)
-; RV64B-NEXT:    ret
-;
-; RV64ZBP-LABEL: bitreverse_i32_nosext:
-; RV64ZBP:       # %bb.0:
-; RV64ZBP-NEXT:    greviw a0, a0, 31
-; RV64ZBP-NEXT:    sw a0, 0(a1)
-; RV64ZBP-NEXT:    ret
-  %1 = tail call i32 @llvm.bitreverse.i32(i32 %a)
-  store i32 %1, i32* %x
-  ret void
-}
-
-declare i64 @llvm.bitreverse.i64(i64)
-
-define i64 @bitreverse_i64(i64 %a) nounwind {
-; RV64I-LABEL: bitreverse_i64:
-; RV64I:       # %bb.0:
-; RV64I-NEXT:    srli a1, a0, 24
-; RV64I-NEXT:    lui a2, 4080
-; RV64I-NEXT:    and a1, a1, a2
-; RV64I-NEXT:    srli a2, a0, 8
-; RV64I-NEXT:    addi a3, zero, 255
-; RV64I-NEXT:    slli a4, a3, 24
-; RV64I-NEXT:    and a2, a2, a4
-; RV64I-NEXT:    or a1, a2, a1
-; RV64I-NEXT:    srli a2, a0, 40
-; RV64I-NEXT:    lui a4, 16
-; RV64I-NEXT:    addiw a4, a4, -256
-; RV64I-NEXT:    and a2, a2, a4
-; RV64I-NEXT:    srli a4, a0, 56
-; RV64I-NEXT:    or a2, a2, a4
-; RV64I-NEXT:    or a1, a1, a2
-; RV64I-NEXT:    slli a2, a0, 8
-; RV64I-NEXT:    slli a4, a3, 32
-; RV64I-NEXT:    and a2, a2, a4
-; RV64I-NEXT:    slli a4, a0, 24
-; RV64I-NEXT:    slli a5, a3, 40
-; RV64I-NEXT:    and a4, a4, a5
-; RV64I-NEXT:    or a2, a4, a2
-; RV64I-NEXT:    slli a4, a0, 40
-; RV64I-NEXT:    slli a3, a3, 48
-; RV64I-NEXT:    and a3, a4, a3
-; RV64I-NEXT:    slli a0, a0, 56
-; RV64I-NEXT:    or a0, a0, a3
-; RV64I-NEXT:    or a0, a0, a2
-; RV64I-NEXT:    or a0, a0, a1
-; RV64I-NEXT:    srli a1, a0, 4
-; RV64I-NEXT:    lui a2, 3855
-; RV64I-NEXT:    addiw a2, a2, 241
-; RV64I-NEXT:    slli a2, a2, 12
-; RV64I-NEXT:    addi a2, a2, -241
-; RV64I-NEXT:    slli a2, a2, 12
-; RV64I-NEXT:    addi a2, a2, 241
-; RV64I-NEXT:    slli a2, a2, 12
-; RV64I-NEXT:    addi a2, a2, -241
-; RV64I-NEXT:    and a1, a1, a2
-; RV64I-NEXT:    and a0, a0, a2
-; RV64I-NEXT:    slli a0, a0, 4
-; RV64I-NEXT:    or a0, a1, a0
-; RV64I-NEXT:    srli a1, a0, 2
-; RV64I-NEXT:    lui a2, 13107
-; RV64I-NEXT:    addiw a2, a2, 819
-; RV64I-NEXT:    slli a2, a2, 12
-; RV64I-NEXT:    addi a2, a2, 819
-; RV64I-NEXT:    slli a2, a2, 12
-; RV64I-NEXT:    addi a2, a2, 819
-; RV64I-NEXT:    slli a2, a2, 12
-; RV64I-NEXT:    addi a2, a2, 819
-; RV64I-NEXT:    and a1, a1, a2
-; RV64I-NEXT:    and a0, a0, a2
-; RV64I-NEXT:    slli a0, a0, 2
-; RV64I-NEXT:    or a0, a1, a0
-; RV64I-NEXT:    srli a1, a0, 1
-; RV64I-NEXT:    lui a2, 21845
-; RV64I-NEXT:    addiw a2, a2, 1365
-; RV64I-NEXT:    slli a2, a2, 12
-; RV64I-NEXT:    addi a2, a2, 1365
-; RV64I-NEXT:    slli a2, a2, 12
-; RV64I-NEXT:    addi a2, a2, 1365
-; RV64I-NEXT:    slli a2, a2, 12
-; RV64I-NEXT:    addi a2, a2, 1365
-; RV64I-NEXT:    and a1, a1, a2
-; RV64I-NEXT:    and a0, a0, a2
-; RV64I-NEXT:    slli a0, a0, 1
-; RV64I-NEXT:    or a0, a1, a0
-; RV64I-NEXT:    ret
-;
-; RV64B-LABEL: bitreverse_i64:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    rev a0, a0
-; RV64B-NEXT:    ret
-;
-; RV64ZBP-LABEL: bitreverse_i64:
-; RV64ZBP:       # %bb.0:
-; RV64ZBP-NEXT:    rev a0, a0
-; RV64ZBP-NEXT:    ret
-  %1 = call i64 @llvm.bitreverse.i64(i64 %a)
-  ret i64 %1
-}
-
-define i32 @bswap_rotr_i32(i32 %a) {
-; RV64I-LABEL: bswap_rotr_i32:
-; RV64I:       # %bb.0:
-; RV64I-NEXT:    slli a1, a0, 8
-; RV64I-NEXT:    lui a2, 4080
-; RV64I-NEXT:    and a1, a1, a2
-; RV64I-NEXT:    slli a2, a0, 24
-; RV64I-NEXT:    or a1, a2, a1
-; RV64I-NEXT:    srliw a2, a0, 24
-; RV64I-NEXT:    srliw a0, a0, 8
-; RV64I-NEXT:    andi a0, a0, -256
-; RV64I-NEXT:    or a0, a0, a2
-; RV64I-NEXT:    slliw a0, a0, 16
-; RV64I-NEXT:    srliw a1, a1, 16
-; RV64I-NEXT:    or a0, a1, a0
-; RV64I-NEXT:    ret
-;
-; RV64B-LABEL: bswap_rotr_i32:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    greviw a0, a0, 8
-; RV64B-NEXT:    ret
-;
-; RV64ZBP-LABEL: bswap_rotr_i32:
-; RV64ZBP:       # %bb.0:
-; RV64ZBP-NEXT:    greviw a0, a0, 8
-; RV64ZBP-NEXT:    ret
-  %1 = call i32 @llvm.bswap.i32(i32 %a)
-  %2 = call i32 @llvm.fshr.i32(i32 %1, i32 %1, i32 16)
-  ret i32 %2
-}
-
-define i32 @bswap_rotl_i32(i32 %a) {
-; RV64I-LABEL: bswap_rotl_i32:
-; RV64I:       # %bb.0:
-; RV64I-NEXT:    srliw a1, a0, 24
-; RV64I-NEXT:    srliw a2, a0, 8
-; RV64I-NEXT:    andi a2, a2, -256
-; RV64I-NEXT:    or a1, a2, a1
-; RV64I-NEXT:    slli a2, a0, 8
-; RV64I-NEXT:    lui a3, 4080
-; RV64I-NEXT:    and a2, a2, a3
-; RV64I-NEXT:    slli a0, a0, 24
-; RV64I-NEXT:    or a0, a0, a2
-; RV64I-NEXT:    srliw a0, a0, 16
-; RV64I-NEXT:    slliw a1, a1, 16
-; RV64I-NEXT:    or a0, a1, a0
-; RV64I-NEXT:    ret
-;
-; RV64B-LABEL: bswap_rotl_i32:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    greviw a0, a0, 8
-; RV64B-NEXT:    ret
-;
-; RV64ZBP-LABEL: bswap_rotl_i32:
-; RV64ZBP:       # %bb.0:
-; RV64ZBP-NEXT:    greviw a0, a0, 8
-; RV64ZBP-NEXT:    ret
-  %1 = call i32 @llvm.bswap.i32(i32 %a)
-  %2 = call i32 @llvm.fshl.i32(i32 %1, i32 %1, i32 16)
-  ret i32 %2
-}
-
-define i32 @bitreverse_bswap_i32(i32 %a) {
-; RV64I-LABEL: bitreverse_bswap_i32:
-; RV64I:       # %bb.0:
-; RV64I-NEXT:    srliw a1, a0, 8
-; RV64I-NEXT:    lui a2, 16
-; RV64I-NEXT:    addiw a2, a2, -256
-; RV64I-NEXT:    and a1, a1, a2
-; RV64I-NEXT:    srliw a3, a0, 24
-; RV64I-NEXT:    or a1, a1, a3
-; RV64I-NEXT:    slli a3, a0, 8
-; RV64I-NEXT:    lui a4, 4080
-; RV64I-NEXT:    and a3, a3, a4
-; RV64I-NEXT:    slliw a0, a0, 24
-; RV64I-NEXT:    or a0, a0, a3
-; RV64I-NEXT:    or a0, a0, a1
-; RV64I-NEXT:    srli a1, a0, 4
-; RV64I-NEXT:    lui a3, 61681
-; RV64I-NEXT:    addiw a3, a3, -241
-; RV64I-NEXT:    and a1, a1, a3
-; RV64I-NEXT:    and a0, a0, a3
-; RV64I-NEXT:    slliw a0, a0, 4
-; RV64I-NEXT:    or a0, a1, a0
-; RV64I-NEXT:    srli a1, a0, 2
-; RV64I-NEXT:    lui a3, 209715
-; RV64I-NEXT:    addiw a3, a3, 819
-; RV64I-NEXT:    and a1, a1, a3
-; RV64I-NEXT:    and a0, a0, a3
-; RV64I-NEXT:    slliw a0, a0, 2
-; RV64I-NEXT:    or a0, a1, a0
-; RV64I-NEXT:    srli a1, a0, 1
-; RV64I-NEXT:    lui a3, 349525
-; RV64I-NEXT:    addiw a3, a3, 1365
-; RV64I-NEXT:    and a1, a1, a3
-; RV64I-NEXT:    and a0, a0, a3
-; RV64I-NEXT:    slliw a0, a0, 1
-; RV64I-NEXT:    or a0, a1, a0
-; RV64I-NEXT:    srliw a1, a0, 8
-; RV64I-NEXT:    and a1, a1, a2
-; RV64I-NEXT:    srliw a2, a0, 24
-; RV64I-NEXT:    or a1, a1, a2
-; RV64I-NEXT:    slli a2, a0, 8
-; RV64I-NEXT:    and a2, a2, a4
-; RV64I-NEXT:    slliw a0, a0, 24
-; RV64I-NEXT:    or a0, a0, a2
-; RV64I-NEXT:    or a0, a0, a1
-; RV64I-NEXT:    ret
-;
-; RV64B-LABEL: bitreverse_bswap_i32:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    greviw a0, a0, 7
-; RV64B-NEXT:    ret
-;
-; RV64ZBP-LABEL: bitreverse_bswap_i32:
-; RV64ZBP:       # %bb.0:
-; RV64ZBP-NEXT:    greviw a0, a0, 7
-; RV64ZBP-NEXT:    ret
-  %1 = call i32 @llvm.bitreverse.i32(i32 %a)
-  %2 = call i32 @llvm.bswap.i32(i32 %1)
-  ret i32 %2
-}
-
-define i64 @bitreverse_bswap_i64(i64 %a) {
-; RV64I-LABEL: bitreverse_bswap_i64:
-; RV64I:       # %bb.0:
-; RV64I-NEXT:    srli a1, a0, 24
-; RV64I-NEXT:    lui a6, 4080
-; RV64I-NEXT:    and a1, a1, a6
-; RV64I-NEXT:    srli a3, a0, 8
-; RV64I-NEXT:    addi a5, zero, 255
-; RV64I-NEXT:    slli a7, a5, 24
-; RV64I-NEXT:    and a3, a3, a7
-; RV64I-NEXT:    or a3, a3, a1
-; RV64I-NEXT:    srli a4, a0, 40
-; RV64I-NEXT:    lui a1, 16
-; RV64I-NEXT:    addiw a1, a1, -256
-; RV64I-NEXT:    and a4, a4, a1
-; RV64I-NEXT:    srli a2, a0, 56
-; RV64I-NEXT:    or a2, a4, a2
-; RV64I-NEXT:    or a2, a3, a2
-; RV64I-NEXT:    slli a4, a0, 8
-; RV64I-NEXT:    slli t0, a5, 32
-; RV64I-NEXT:    and a3, a4, t0
-; RV64I-NEXT:    slli a4, a0, 24
-; RV64I-NEXT:    slli t1, a5, 40
-; RV64I-NEXT:    and a4, a4, t1
-; RV64I-NEXT:    or a3, a4, a3
-; RV64I-NEXT:    slli a4, a0, 40
-; RV64I-NEXT:    slli a5, a5, 48
-; RV64I-NEXT:    and a4, a4, a5
-; RV64I-NEXT:    slli a0, a0, 56
-; RV64I-NEXT:    or a0, a0, a4
-; RV64I-NEXT:    or a0, a0, a3
-; RV64I-NEXT:    or a0, a0, a2
-; RV64I-NEXT:    srli a2, a0, 4
-; RV64I-NEXT:    lui a3, 3855
-; RV64I-NEXT:    addiw a3, a3, 241
-; RV64I-NEXT:    slli a3, a3, 12
-; RV64I-NEXT:    addi a3, a3, -241
-; RV64I-NEXT:    slli a3, a3, 12
-; RV64I-NEXT:    addi a3, a3, 241
-; RV64I-NEXT:    slli a3, a3, 12
-; RV64I-NEXT:    addi a3, a3, -241
-; RV64I-NEXT:    and a2, a2, a3
-; RV64I-NEXT:    and a0, a0, a3
-; RV64I-NEXT:    slli a0, a0, 4
-; RV64I-NEXT:    or a0, a2, a0
-; RV64I-NEXT:    srli a2, a0, 2
-; RV64I-NEXT:    lui a3, 13107
-; RV64I-NEXT:    addiw a3, a3, 819
-; RV64I-NEXT:    slli a3, a3, 12
-; RV64I-NEXT:    addi a3, a3, 819
-; RV64I-NEXT:    slli a3, a3, 12
-; RV64I-NEXT:    addi a3, a3, 819
-; RV64I-NEXT:    slli a3, a3, 12
-; RV64I-NEXT:    addi a3, a3, 819
-; RV64I-NEXT:    and a2, a2, a3
-; RV64I-NEXT:    and a0, a0, a3
-; RV64I-NEXT:    slli a0, a0, 2
-; RV64I-NEXT:    or a0, a2, a0
-; RV64I-NEXT:    srli a2, a0, 1
-; RV64I-NEXT:    lui a3, 21845
-; RV64I-NEXT:    addiw a3, a3, 1365
-; RV64I-NEXT:    slli a3, a3, 12
-; RV64I-NEXT:    addi a3, a3, 1365
-; RV64I-NEXT:    slli a3, a3, 12
-; RV64I-NEXT:    addi a3, a3, 1365
-; RV64I-NEXT:    slli a3, a3, 12
-; RV64I-NEXT:    addi a3, a3, 1365
-; RV64I-NEXT:    and a2, a2, a3
-; RV64I-NEXT:    and a0, a0, a3
-; RV64I-NEXT:    slli a0, a0, 1
-; RV64I-NEXT:    or a0, a2, a0
-; RV64I-NEXT:    srli a2, a0, 40
-; RV64I-NEXT:    and a1, a2, a1
-; RV64I-NEXT:    srli a2, a0, 56
-; RV64I-NEXT:    or a1, a1, a2
-; RV64I-NEXT:    srli a2, a0, 24
-; RV64I-NEXT:    and a2, a2, a6
-; RV64I-NEXT:    srli a3, a0, 8
-; RV64I-NEXT:    and a3, a3, a7
-; RV64I-NEXT:    or a2, a3, a2
-; RV64I-NEXT:    or a1, a2, a1
-; RV64I-NEXT:    slli a2, a0, 8
-; RV64I-NEXT:    and a2, a2, t0
-; RV64I-NEXT:    slli a3, a0, 24
-; RV64I-NEXT:    and a3, a3, t1
-; RV64I-NEXT:    or a2, a3, a2
-; RV64I-NEXT:    slli a3, a0, 40
-; RV64I-NEXT:    and a3, a3, a5
-; RV64I-NEXT:    slli a0, a0, 56
-; RV64I-NEXT:    or a0, a0, a3
-; RV64I-NEXT:    or a0, a0, a2
-; RV64I-NEXT:    or a0, a0, a1
-; RV64I-NEXT:    ret
-;
-; RV64B-LABEL: bitreverse_bswap_i64:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    rev.b a0, a0
-; RV64B-NEXT:    ret
-;
-; RV64ZBP-LABEL: bitreverse_bswap_i64:
-; RV64ZBP:       # %bb.0:
-; RV64ZBP-NEXT:    rev.b a0, a0
-; RV64ZBP-NEXT:    ret
-  %1 = call i64 @llvm.bitreverse.i64(i64 %a)
-  %2 = call i64 @llvm.bswap.i64(i64 %1)
-  ret i64 %2
-}
-
-define signext i32 @shfl1_i32(i32 signext %a, i32 signext %b) nounwind {
-; RV64I-LABEL: shfl1_i32:
-; RV64I:       # %bb.0:
-; RV64I-NEXT:    lui a1, 629146
-; RV64I-NEXT:    addiw a1, a1, -1639
-; RV64I-NEXT:    and a1, a0, a1
-; RV64I-NEXT:    slli a2, a0, 1
-; RV64I-NEXT:    lui a3, 279620
-; RV64I-NEXT:    addiw a3, a3, 1092
-; RV64I-NEXT:    and a2, a2, a3
-; RV64I-NEXT:    or a1, a2, a1
-; RV64I-NEXT:    srli a0, a0, 1
-; RV64I-NEXT:    lui a2, 139810
-; RV64I-NEXT:    addiw a2, a2, 546
-; RV64I-NEXT:    and a0, a0, a2
-; RV64I-NEXT:    or a0, a1, a0
-; RV64I-NEXT:    ret
-;
-; RV64B-LABEL: shfl1_i32:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    zip.n a0, a0
-; RV64B-NEXT:    ret
-;
-; RV64ZBP-LABEL: shfl1_i32:
-; RV64ZBP:       # %bb.0:
-; RV64ZBP-NEXT:    zip.n a0, a0
-; RV64ZBP-NEXT:    ret
-  %and = and i32 %a, -1717986919
-  %shl = shl i32 %a, 1
-  %and1 = and i32 %shl, 1145324612
-  %or = or i32 %and1, %and
-  %shr = lshr i32 %a, 1
-  %and2 = and i32 %shr, 572662306
-  %or3 = or i32 %or, %and2
-  ret i32 %or3
-}
-
-define i64 @shfl1_i64(i64 %a, i64 %b) nounwind {
-; RV64I-LABEL: shfl1_i64:
-; RV64I:       # %bb.0:
-; RV64I-NEXT:    lui a1, 1035469
-; RV64I-NEXT:    addiw a1, a1, -819
-; RV64I-NEXT:    slli a1, a1, 12
-; RV64I-NEXT:    addi a1, a1, -819
-; RV64I-NEXT:    slli a1, a1, 12
-; RV64I-NEXT:    addi a1, a1, -819
-; RV64I-NEXT:    slli a1, a1, 13
-; RV64I-NEXT:    addi a1, a1, -1639
-; RV64I-NEXT:    and a1, a0, a1
-; RV64I-NEXT:    slli a2, a0, 1
-; RV64I-NEXT:    lui a3, 4369
-; RV64I-NEXT:    addiw a3, a3, 273
-; RV64I-NEXT:    slli a3, a3, 12
-; RV64I-NEXT:    addi a3, a3, 273
-; RV64I-NEXT:    slli a3, a3, 12
-; RV64I-NEXT:    addi a3, a3, 273
-; RV64I-NEXT:    slli a4, a3, 14
-; RV64I-NEXT:    addi a4, a4, 1092
-; RV64I-NEXT:    and a2, a2, a4
-; RV64I-NEXT:    or a1, a1, a2
-; RV64I-NEXT:    srli a0, a0, 1
-; RV64I-NEXT:    slli a2, a3, 13
-; RV64I-NEXT:    addi a2, a2, 546
-; RV64I-NEXT:    and a0, a0, a2
-; RV64I-NEXT:    or a0, a1, a0
-; RV64I-NEXT:    ret
-;
-; RV64B-LABEL: shfl1_i64:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    zip.n a0, a0
-; RV64B-NEXT:    ret
-;
-; RV64ZBP-LABEL: shfl1_i64:
-; RV64ZBP:       # %bb.0:
-; RV64ZBP-NEXT:    zip.n a0, a0
-; RV64ZBP-NEXT:    ret
-  %and = and i64 %a, -7378697629483820647
-  %shl = shl i64 %a, 1
-  %and1 = and i64 %shl, 4919131752989213764
-  %or = or i64 %and, %and1
-  %shr = lshr i64 %a, 1
-  %and2 = and i64 %shr, 2459565876494606882
-  %or3 = or i64 %or, %and2
-  ret i64 %or3
-}
-
-define signext i32 @shfl2_i32(i32 signext %a, i32 signext %b) nounwind {
-; RV64I-LABEL: shfl2_i32:
-; RV64I:       # %bb.0:
-; RV64I-NEXT:    lui a1, 801852
-; RV64I-NEXT:    addiw a1, a1, 963
-; RV64I-NEXT:    and a1, a0, a1
-; RV64I-NEXT:    slli a2, a0, 2
-; RV64I-NEXT:    lui a3, 197379
-; RV64I-NEXT:    addiw a3, a3, 48
-; RV64I-NEXT:    and a2, a2, a3
-; RV64I-NEXT:    or a1, a2, a1
-; RV64I-NEXT:    srli a0, a0, 2
-; RV64I-NEXT:    lui a2, 49345
-; RV64I-NEXT:    addiw a2, a2, -1012
-; RV64I-NEXT:    and a0, a0, a2
-; RV64I-NEXT:    or a0, a0, a1
-; RV64I-NEXT:    ret
-;
-; RV64B-LABEL: shfl2_i32:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    zip2.b a0, a0
-; RV64B-NEXT:    ret
-;
-; RV64ZBP-LABEL: shfl2_i32:
-; RV64ZBP:       # %bb.0:
-; RV64ZBP-NEXT:    zip2.b a0, a0
-; RV64ZBP-NEXT:    ret
-  %and = and i32 %a, -1010580541
-  %shl = shl i32 %a, 2
-  %and1 = and i32 %shl, 808464432
-  %or = or i32 %and1, %and
-  %shr = lshr i32 %a, 2
-  %and2 = and i32 %shr, 202116108
-  %or3 = or i32 %and2, %or
-  ret i32 %or3
-}
-
-define i64 @shfl2_i64(i64 %a, i64 %b) nounwind {
-; RV64I-LABEL: shfl2_i64:
-; RV64I:       # %bb.0:
-; RV64I-NEXT:    lui a1, 1044721
-; RV64I-NEXT:    addiw a1, a1, -241
-; RV64I-NEXT:    slli a1, a1, 12
-; RV64I-NEXT:    addi a1, a1, 241
-; RV64I-NEXT:    slli a1, a1, 12
-; RV64I-NEXT:    addi a1, a1, -241
-; RV64I-NEXT:    slli a1, a1, 14
-; RV64I-NEXT:    addi a1, a1, 963
-; RV64I-NEXT:    and a1, a0, a1
-; RV64I-NEXT:    slli a2, a0, 2
-; RV64I-NEXT:    lui a3, 197379
-; RV64I-NEXT:    slli a3, a3, 4
-; RV64I-NEXT:    addi a3, a3, 771
-; RV64I-NEXT:    slli a4, a3, 16
-; RV64I-NEXT:    addi a4, a4, 771
-; RV64I-NEXT:    slli a4, a4, 12
-; RV64I-NEXT:    addi a4, a4, 48
-; RV64I-NEXT:    and a2, a2, a4
-; RV64I-NEXT:    or a1, a1, a2
-; RV64I-NEXT:    srli a0, a0, 2
-; RV64I-NEXT:    slli a2, a3, 14
-; RV64I-NEXT:    addi a2, a2, 193
-; RV64I-NEXT:    slli a2, a2, 12
-; RV64I-NEXT:    addi a2, a2, -1012
-; RV64I-NEXT:    and a0, a0, a2
-; RV64I-NEXT:    or a0, a0, a1
-; RV64I-NEXT:    ret
-;
-; RV64B-LABEL: shfl2_i64:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    zip2.b a0, a0
-; RV64B-NEXT:    ret
-;
-; RV64ZBP-LABEL: shfl2_i64:
-; RV64ZBP:       # %bb.0:
-; RV64ZBP-NEXT:    zip2.b a0, a0
-; RV64ZBP-NEXT:    ret
-  %and = and i64 %a, -4340410370284600381
-  %shl = shl i64 %a, 2
-  %and1 = and i64 %shl, 3472328296227680304
-  %or = or i64 %and, %and1
-  %shr = lshr i64 %a, 2
-  %and2 = and i64 %shr, 868082074056920076
-  %or3 = or i64 %and2, %or
-  ret i64 %or3
-}
-
-define signext i32 @shfl4_i32(i32 signext %a, i32 signext %b) nounwind {
-; RV64I-LABEL: shfl4_i32:
-; RV64I:       # %bb.0:
-; RV64I-NEXT:    lui a1, 983295
-; RV64I-NEXT:    addiw a1, a1, 15
-; RV64I-NEXT:    and a1, a0, a1
-; RV64I-NEXT:    slli a2, a0, 4
-; RV64I-NEXT:    lui a3, 61441
-; RV64I-NEXT:    addiw a3, a3, -256
-; RV64I-NEXT:    and a2, a2, a3
-; RV64I-NEXT:    srli a0, a0, 4
-; RV64I-NEXT:    lui a3, 3840
-; RV64I-NEXT:    addiw a3, a3, 240
-; RV64I-NEXT:    and a0, a0, a3
-; RV64I-NEXT:    or a0, a0, a1
-; RV64I-NEXT:    or a0, a0, a2
-; RV64I-NEXT:    ret
-;
-; RV64B-LABEL: shfl4_i32:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    zip4.h a0, a0
-; RV64B-NEXT:    ret
-;
-; RV64ZBP-LABEL: shfl4_i32:
-; RV64ZBP:       # %bb.0:
-; RV64ZBP-NEXT:    zip4.h a0, a0
-; RV64ZBP-NEXT:    ret
-  %and = and i32 %a, -267390961
-  %shl = shl i32 %a, 4
-  %and1 = and i32 %shl, 251662080
-  %shr = lshr i32 %a, 4
-  %and2 = and i32 %shr, 15728880
-  %or = or i32 %and2, %and
-  %or3 = or i32 %or, %and1
-  ret i32 %or3
-}
-
-define i64 @shfl4_i64(i64 %a, i64 %b) nounwind {
-; RV64I-LABEL: shfl4_i64:
-; RV64I:       # %bb.0:
-; RV64I-NEXT:    lui a1, 983295
-; RV64I-NEXT:    slli a1, a1, 4
-; RV64I-NEXT:    addi a1, a1, 255
-; RV64I-NEXT:    slli a1, a1, 16
-; RV64I-NEXT:    addi a1, a1, 255
-; RV64I-NEXT:    slli a1, a1, 12
-; RV64I-NEXT:    addi a1, a1, 15
-; RV64I-NEXT:    and a1, a0, a1
-; RV64I-NEXT:    slli a2, a0, 4
-; RV64I-NEXT:    lui a3, 983055
-; RV64I-NEXT:    slli a3, a3, 4
-; RV64I-NEXT:    addi a3, a3, 15
-; RV64I-NEXT:    slli a3, a3, 16
-; RV64I-NEXT:    addi a3, a3, 15
-; RV64I-NEXT:    slli a3, a3, 12
-; RV64I-NEXT:    srli a3, a3, 4
-; RV64I-NEXT:    and a2, a2, a3
-; RV64I-NEXT:    srli a0, a0, 4
-; RV64I-NEXT:    lui a3, 240
-; RV64I-NEXT:    addiw a3, a3, 15
-; RV64I-NEXT:    slli a3, a3, 16
-; RV64I-NEXT:    addi a3, a3, 15
-; RV64I-NEXT:    slli a3, a3, 20
-; RV64I-NEXT:    addi a3, a3, 240
-; RV64I-NEXT:    and a0, a0, a3
-; RV64I-NEXT:    or a0, a2, a0
-; RV64I-NEXT:    or a0, a0, a1
-; RV64I-NEXT:    ret
-;
-; RV64B-LABEL: shfl4_i64:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    zip4.h a0, a0
-; RV64B-NEXT:    ret
-;
-; RV64ZBP-LABEL: shfl4_i64:
-; RV64ZBP:       # %bb.0:
-; RV64ZBP-NEXT:    zip4.h a0, a0
-; RV64ZBP-NEXT:    ret
-  %and = and i64 %a, -1148435428713435121
-  %shl = shl i64 %a, 4
-  %and1 = and i64 %shl, 1080880403494997760
-  %shr = lshr i64 %a, 4
-  %and2 = and i64 %shr, 67555025218437360
-  %or = or i64 %and1, %and2
-  %or3 = or i64 %or, %and
-  ret i64 %or3
-}
-
-define signext i32 @shfl8_i32(i32 signext %a, i32 signext %b) nounwind {
-; RV64I-LABEL: shfl8_i32:
-; RV64I:       # %bb.0:
-; RV64I-NEXT:    lui a1, 1044480
-; RV64I-NEXT:    addiw a1, a1, 255
-; RV64I-NEXT:    and a1, a0, a1
-; RV64I-NEXT:    slli a2, a0, 8
-; RV64I-NEXT:    lui a3, 4080
-; RV64I-NEXT:    and a2, a2, a3
-; RV64I-NEXT:    srli a0, a0, 8
-; RV64I-NEXT:    lui a3, 16
-; RV64I-NEXT:    addiw a3, a3, -256
-; RV64I-NEXT:    and a0, a0, a3
-; RV64I-NEXT:    or a0, a1, a0
-; RV64I-NEXT:    or a0, a0, a2
-; RV64I-NEXT:    ret
-;
-; RV64B-LABEL: shfl8_i32:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    zip8.w a0, a0
-; RV64B-NEXT:    ret
-;
-; RV64ZBP-LABEL: shfl8_i32:
-; RV64ZBP:       # %bb.0:
-; RV64ZBP-NEXT:    zip8.w a0, a0
-; RV64ZBP-NEXT:    ret
-  %and = and i32 %a, -16776961
-  %shl = shl i32 %a, 8
-  %and1 = and i32 %shl, 16711680
-  %shr = lshr i32 %a, 8
-  %and2 = and i32 %shr, 65280
-  %or = or i32 %and, %and2
-  %or3 = or i32 %or, %and1
-  ret i32 %or3
-}
-
-define i64 @shfl8_i64(i64 %a, i64 %b) nounwind {
-; RV64I-LABEL: shfl8_i64:
-; RV64I:       # %bb.0:
-; RV64I-NEXT:    lui a1, 983041
-; RV64I-NEXT:    slli a1, a1, 4
-; RV64I-NEXT:    addi a1, a1, -1
-; RV64I-NEXT:    slli a1, a1, 24
-; RV64I-NEXT:    addi a1, a1, 255
-; RV64I-NEXT:    and a1, a0, a1
-; RV64I-NEXT:    slli a2, a0, 8
-; RV64I-NEXT:    addi a3, zero, 255
-; RV64I-NEXT:    slli a4, a3, 32
-; RV64I-NEXT:    addi a4, a4, 255
-; RV64I-NEXT:    slli a4, a4, 16
-; RV64I-NEXT:    and a2, a2, a4
-; RV64I-NEXT:    srli a0, a0, 8
-; RV64I-NEXT:    slli a3, a3, 24
-; RV64I-NEXT:    addi a3, a3, 1
-; RV64I-NEXT:    slli a3, a3, 16
-; RV64I-NEXT:    addi a3, a3, -256
-; RV64I-NEXT:    and a0, a0, a3
-; RV64I-NEXT:    or a0, a0, a1
-; RV64I-NEXT:    or a0, a2, a0
-; RV64I-NEXT:    ret
-;
-; RV64B-LABEL: shfl8_i64:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    zip8.w a0, a0
-; RV64B-NEXT:    ret
-;
-; RV64ZBP-LABEL: shfl8_i64:
-; RV64ZBP:       # %bb.0:
-; RV64ZBP-NEXT:    zip8.w a0, a0
-; RV64ZBP-NEXT:    ret
-  %and = and i64 %a, -72056494543077121
-  %shl = shl i64 %a, 8
-  %and1 = and i64 %shl, 71776119077928960
-  %shr = lshr i64 %a, 8
-  %and2 = and i64 %shr, 280375465148160
-  %or = or i64 %and2, %and
-  %or3 = or i64 %and1, %or
-  ret i64 %or3
-}
-
-define i64 @shfl16(i64 %a, i64 %b) nounwind {
-; RV64I-LABEL: shfl16:
-; RV64I:       # %bb.0:
-; RV64I-NEXT:    addi a1, zero, -1
-; RV64I-NEXT:    slli a1, a1, 32
-; RV64I-NEXT:    addi a1, a1, 1
-; RV64I-NEXT:    slli a1, a1, 16
-; RV64I-NEXT:    addi a1, a1, -1
-; RV64I-NEXT:    and a1, a0, a1
-; RV64I-NEXT:    slli a2, a0, 16
-; RV64I-NEXT:    lui a3, 65535
-; RV64I-NEXT:    slli a4, a3, 20
-; RV64I-NEXT:    and a2, a2, a4
-; RV64I-NEXT:    or a1, a2, a1
-; RV64I-NEXT:    srli a0, a0, 16
-; RV64I-NEXT:    slli a2, a3, 4
-; RV64I-NEXT:    and a0, a0, a2
-; RV64I-NEXT:    or a0, a1, a0
-; RV64I-NEXT:    ret
-;
-; RV64B-LABEL: shfl16:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    zip16 a0, a0
-; RV64B-NEXT:    ret
-;
-; RV64ZBP-LABEL: shfl16:
-; RV64ZBP:       # %bb.0:
-; RV64ZBP-NEXT:    zip16 a0, a0
-; RV64ZBP-NEXT:    ret
-  %and = and i64 %a, -281474976645121
-  %shl = shl i64 %a, 16
-  %and1 = and i64 %shl, 281470681743360
-  %or = or i64 %and1, %and
-  %shr = lshr i64 %a, 16
-  %and2 = and i64 %shr, 4294901760
-  %or3 = or i64 %or, %and2
-  ret i64 %or3
-}
-
-define signext i32 @pack_i32(i32 signext %a, i32 signext %b) nounwind {
-; RV64I-LABEL: pack_i32:
-; RV64I:       # %bb.0:
-; RV64I-NEXT:    lui a2, 16
-; RV64I-NEXT:    addiw a2, a2, -1
-; RV64I-NEXT:    and a0, a0, a2
-; RV64I-NEXT:    slliw a1, a1, 16
-; RV64I-NEXT:    or a0, a1, a0
-; RV64I-NEXT:    ret
-;
-; RV64B-LABEL: pack_i32:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    packw a0, a0, a1
-; RV64B-NEXT:    ret
-;
-; RV64ZBP-LABEL: pack_i32:
-; RV64ZBP:       # %bb.0:
-; RV64ZBP-NEXT:    packw a0, a0, a1
-; RV64ZBP-NEXT:    ret
-  %shl = and i32 %a, 65535
-  %shl1 = shl i32 %b, 16
-  %or = or i32 %shl1, %shl
-  ret i32 %or
-}
-
-define i64 @pack_i64(i64 %a, i64 %b) nounwind {
-; RV64I-LABEL: pack_i64:
-; RV64I:       # %bb.0:
-; RV64I-NEXT:    slli a0, a0, 32
-; RV64I-NEXT:    srli a0, a0, 32
-; RV64I-NEXT:    slli a1, a1, 32
-; RV64I-NEXT:    or a0, a1, a0
-; RV64I-NEXT:    ret
-;
-; RV64B-LABEL: pack_i64:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    pack a0, a0, a1
-; RV64B-NEXT:    ret
-;
-; RV64ZBP-LABEL: pack_i64:
-; RV64ZBP:       # %bb.0:
-; RV64ZBP-NEXT:    pack a0, a0, a1
-; RV64ZBP-NEXT:    ret
-  %shl = and i64 %a, 4294967295
-  %shl1 = shl i64 %b, 32
-  %or = or i64 %shl1, %shl
-  ret i64 %or
-}
-
-define signext i32 @packu_i32(i32 signext %a, i32 signext %b) nounwind {
-; RV64I-LABEL: packu_i32:
-; RV64I:       # %bb.0:
-; RV64I-NEXT:    srliw a0, a0, 16
-; RV64I-NEXT:    lui a2, 1048560
-; RV64I-NEXT:    and a1, a1, a2
-; RV64I-NEXT:    or a0, a1, a0
-; RV64I-NEXT:    ret
-;
-; RV64B-LABEL: packu_i32:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    packuw a0, a0, a1
-; RV64B-NEXT:    ret
-;
-; RV64ZBP-LABEL: packu_i32:
-; RV64ZBP:       # %bb.0:
-; RV64ZBP-NEXT:    packuw a0, a0, a1
-; RV64ZBP-NEXT:    ret
-  %shr = lshr i32 %a, 16
-  %shr1 = and i32 %b, -65536
-  %or = or i32 %shr1, %shr
-  ret i32 %or
-}
-
-define i64 @packu_i64(i64 %a, i64 %b) nounwind {
-; RV64I-LABEL: packu_i64:
-; RV64I:       # %bb.0:
-; RV64I-NEXT:    srli a0, a0, 32
-; RV64I-NEXT:    addi a2, zero, -1
-; RV64I-NEXT:    slli a2, a2, 32
-; RV64I-NEXT:    and a1, a1, a2
-; RV64I-NEXT:    or a0, a1, a0
-; RV64I-NEXT:    ret
-;
-; RV64B-LABEL: packu_i64:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    packu a0, a0, a1
-; RV64B-NEXT:    ret
-;
-; RV64ZBP-LABEL: packu_i64:
-; RV64ZBP:       # %bb.0:
-; RV64ZBP-NEXT:    packu a0, a0, a1
-; RV64ZBP-NEXT:    ret
-  %shr = lshr i64 %a, 32
-  %shr1 = and i64 %b, -4294967296
-  %or = or i64 %shr1, %shr
-  ret i64 %or
-}
-
-define signext i32 @packh_i32(i32 signext %a, i32 signext %b) nounwind {
-; RV64I-LABEL: packh_i32:
-; RV64I:       # %bb.0:
-; RV64I-NEXT:    andi a0, a0, 255
-; RV64I-NEXT:    slli a1, a1, 56
-; RV64I-NEXT:    srli a1, a1, 48
-; RV64I-NEXT:    or a0, a1, a0
-; RV64I-NEXT:    ret
-;
-; RV64B-LABEL: packh_i32:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    packh a0, a0, a1
-; RV64B-NEXT:    ret
-;
-; RV64ZBP-LABEL: packh_i32:
-; RV64ZBP:       # %bb.0:
-; RV64ZBP-NEXT:    packh a0, a0, a1
-; RV64ZBP-NEXT:    ret
-  %and = and i32 %a, 255
-  %and1 = shl i32 %b, 8
-  %shl = and i32 %and1, 65280
-  %or = or i32 %shl, %and
-  ret i32 %or
-}
-
-define i64 @packh_i64(i64 %a, i64 %b) nounwind {
-; RV64I-LABEL: packh_i64:
-; RV64I:       # %bb.0:
-; RV64I-NEXT:    andi a0, a0, 255
-; RV64I-NEXT:    slli a1, a1, 56
-; RV64I-NEXT:    srli a1, a1, 48
-; RV64I-NEXT:    or a0, a1, a0
-; RV64I-NEXT:    ret
-;
-; RV64B-LABEL: packh_i64:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    packh a0, a0, a1
-; RV64B-NEXT:    ret
-;
-; RV64ZBP-LABEL: packh_i64:
-; RV64ZBP:       # %bb.0:
-; RV64ZBP-NEXT:    packh a0, a0, a1
-; RV64ZBP-NEXT:    ret
-  %and = and i64 %a, 255
-  %and1 = shl i64 %b, 8
-  %shl = and i64 %and1, 65280
-  %or = or i64 %shl, %and
-  ret i64 %or
-}
-
-define i32 @zexth_i32(i32 %a) nounwind {
-; RV64I-LABEL: zexth_i32:
-; RV64I:       # %bb.0:
-; RV64I-NEXT:    lui a1, 16
-; RV64I-NEXT:    addiw a1, a1, -1
-; RV64I-NEXT:    and a0, a0, a1
-; RV64I-NEXT:    ret
-;
-; RV64B-LABEL: zexth_i32:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    zext.h a0, a0
-; RV64B-NEXT:    ret
-;
-; RV64ZBP-LABEL: zexth_i32:
-; RV64ZBP:       # %bb.0:
-; RV64ZBP-NEXT:    zext.h a0, a0
-; RV64ZBP-NEXT:    ret
-  %and = and i32 %a, 65535
-  ret i32 %and
-}
-
-define i64 @zexth_i64(i64 %a) nounwind {
-; RV64I-LABEL: zexth_i64:
-; RV64I:       # %bb.0:
-; RV64I-NEXT:    lui a1, 16
-; RV64I-NEXT:    addiw a1, a1, -1
-; RV64I-NEXT:    and a0, a0, a1
-; RV64I-NEXT:    ret
-;
-; RV64B-LABEL: zexth_i64:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    zext.h a0, a0
-; RV64B-NEXT:    ret
-;
-; RV64ZBP-LABEL: zexth_i64:
-; RV64ZBP:       # %bb.0:
-; RV64ZBP-NEXT:    zext.h a0, a0
-; RV64ZBP-NEXT:    ret
-  %and = and i64 %a, 65535
-  ret i64 %and
-}
diff --git a/llvm/test/CodeGen/RISCV/rv64zbr.ll b/llvm/test/CodeGen/RISCV/rv64zbr.ll
deleted file mode 100644
index fadb2cb1e798..000000000000
--- a/llvm/test/CodeGen/RISCV/rv64zbr.ll
+++ /dev/null
@@ -1,91 +0,0 @@
-; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
-; RUN: llc -mtriple=riscv64 -mattr=experimental-zbr -verify-machineinstrs < %s \
-; RUN:   | FileCheck %s -check-prefix=RV64ZBR
-
-declare i64 @llvm.riscv.crc32.b.i64(i64)
-
-define i64 @crc32b(i64 %a) nounwind {
-; RV64ZBR-LABEL: crc32b:
-; RV64ZBR:       # %bb.0:
-; RV64ZBR-NEXT:    crc32.b a0, a0
-; RV64ZBR-NEXT:    ret
-  %tmp = call i64 @llvm.riscv.crc32.b.i64(i64 %a)
- ret i64 %tmp
-}
-
-declare i64 @llvm.riscv.crc32.h.i64(i64)
-
-define i64 @crc32h(i64 %a) nounwind {
-; RV64ZBR-LABEL: crc32h:
-; RV64ZBR:       # %bb.0:
-; RV64ZBR-NEXT:    crc32.h a0, a0
-; RV64ZBR-NEXT:    ret
-  %tmp = call i64 @llvm.riscv.crc32.h.i64(i64 %a)
- ret i64 %tmp
-}
-
-declare i64 @llvm.riscv.crc32.w.i64(i64)
-
-define i64 @crc32w(i64 %a) nounwind {
-; RV64ZBR-LABEL: crc32w:
-; RV64ZBR:       # %bb.0:
-; RV64ZBR-NEXT:    crc32.w a0, a0
-; RV64ZBR-NEXT:    ret
-  %tmp = call i64 @llvm.riscv.crc32.w.i64(i64 %a)
- ret i64 %tmp
-}
-
-declare i64 @llvm.riscv.crc32c.b.i64(i64)
-
-define i64 @crc32cb(i64 %a) nounwind {
-; RV64ZBR-LABEL: crc32cb:
-; RV64ZBR:       # %bb.0:
-; RV64ZBR-NEXT:    crc32c.b a0, a0
-; RV64ZBR-NEXT:    ret
-  %tmp = call i64 @llvm.riscv.crc32c.b.i64(i64 %a)
- ret i64 %tmp
-}
-
-declare i64 @llvm.riscv.crc32c.h.i64(i64)
-
-define i64 @crc32ch(i64 %a) nounwind {
-; RV64ZBR-LABEL: crc32ch:
-; RV64ZBR:       # %bb.0:
-; RV64ZBR-NEXT:    crc32c.h a0, a0
-; RV64ZBR-NEXT:    ret
-  %tmp = call i64 @llvm.riscv.crc32c.h.i64(i64 %a)
- ret i64 %tmp
-}
-
-declare i64 @llvm.riscv.crc32c.w.i64(i64)
-
-define i64 @crc32cw(i64 %a) nounwind {
-; RV64ZBR-LABEL: crc32cw:
-; RV64ZBR:       # %bb.0:
-; RV64ZBR-NEXT:    crc32c.w a0, a0
-; RV64ZBR-NEXT:    ret
-  %tmp = call i64 @llvm.riscv.crc32c.w.i64(i64 %a)
- ret i64 %tmp
-}
-
-declare i64 @llvm.riscv.crc32.d.i64(i64)
-
-define i64 @crc32d(i64 %a) nounwind {
-; RV64ZBR-LABEL: crc32d:
-; RV64ZBR:       # %bb.0:
-; RV64ZBR-NEXT:    crc32.d a0, a0
-; RV64ZBR-NEXT:    ret
-  %tmp = call i64 @llvm.riscv.crc32.d.i64(i64 %a)
- ret i64 %tmp
-}
-
-declare i64 @llvm.riscv.crc32c.d.i64(i64)
-
-define i64 @crc32cd(i64 %a) nounwind {
-; RV64ZBR-LABEL: crc32cd:
-; RV64ZBR:       # %bb.0:
-; RV64ZBR-NEXT:    crc32c.d a0, a0
-; RV64ZBR-NEXT:    ret
-  %tmp = call i64 @llvm.riscv.crc32c.d.i64(i64 %a)
- ret i64 %tmp
-}
diff --git a/llvm/test/CodeGen/RISCV/rv64zbt.ll b/llvm/test/CodeGen/RISCV/rv64zbt.ll
deleted file mode 100644
index 33d0421adfb8..000000000000
--- a/llvm/test/CodeGen/RISCV/rv64zbt.ll
+++ /dev/null
@@ -1,618 +0,0 @@
-; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
-; RUN: llc -mtriple=riscv64 -verify-machineinstrs < %s \
-; RUN:   | FileCheck %s -check-prefix=RV64I
-; RUN: llc -mtriple=riscv64 -mattr=+experimental-b -verify-machineinstrs < %s \
-; RUN:   | FileCheck %s -check-prefix=RV64B
-; RUN: llc -mtriple=riscv64 -mattr=+experimental-zbt -verify-machineinstrs < %s \
-; RUN:   | FileCheck %s -check-prefix=RV64ZBT
-
-define signext i32 @cmix_i32(i32 signext %a, i32 signext %b, i32 signext %c) nounwind {
-; RV64I-LABEL: cmix_i32:
-; RV64I:       # %bb.0:
-; RV64I-NEXT:    and a0, a1, a0
-; RV64I-NEXT:    not a1, a1
-; RV64I-NEXT:    and a1, a1, a2
-; RV64I-NEXT:    or a0, a1, a0
-; RV64I-NEXT:    ret
-;
-; RV64B-LABEL: cmix_i32:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    cmix a0, a1, a0, a2
-; RV64B-NEXT:    ret
-;
-; RV64ZBT-LABEL: cmix_i32:
-; RV64ZBT:       # %bb.0:
-; RV64ZBT-NEXT:    cmix a0, a1, a0, a2
-; RV64ZBT-NEXT:    ret
-  %and = and i32 %b, %a
-  %neg = xor i32 %b, -1
-  %and1 = and i32 %neg, %c
-  %or = or i32 %and1, %and
-  ret i32 %or
-}
-
-define i64 @cmix_i64(i64 %a, i64 %b, i64 %c) nounwind {
-; RV64I-LABEL: cmix_i64:
-; RV64I:       # %bb.0:
-; RV64I-NEXT:    and a0, a1, a0
-; RV64I-NEXT:    not a1, a1
-; RV64I-NEXT:    and a1, a1, a2
-; RV64I-NEXT:    or a0, a1, a0
-; RV64I-NEXT:    ret
-;
-; RV64B-LABEL: cmix_i64:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    cmix a0, a1, a0, a2
-; RV64B-NEXT:    ret
-;
-; RV64ZBT-LABEL: cmix_i64:
-; RV64ZBT:       # %bb.0:
-; RV64ZBT-NEXT:    cmix a0, a1, a0, a2
-; RV64ZBT-NEXT:    ret
-  %and = and i64 %b, %a
-  %neg = xor i64 %b, -1
-  %and1 = and i64 %neg, %c
-  %or = or i64 %and1, %and
-  ret i64 %or
-}
-
-define signext i32 @cmov_i32(i32 signext %a, i32 signext %b, i32 signext %c) nounwind {
-; RV64I-LABEL: cmov_i32:
-; RV64I:       # %bb.0:
-; RV64I-NEXT:    beqz a1, .LBB2_2
-; RV64I-NEXT:  # %bb.1:
-; RV64I-NEXT:    mv a2, a0
-; RV64I-NEXT:  .LBB2_2:
-; RV64I-NEXT:    mv a0, a2
-; RV64I-NEXT:    ret
-;
-; RV64B-LABEL: cmov_i32:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    cmov a0, a1, a0, a2
-; RV64B-NEXT:    ret
-;
-; RV64ZBT-LABEL: cmov_i32:
-; RV64ZBT:       # %bb.0:
-; RV64ZBT-NEXT:    cmov a0, a1, a0, a2
-; RV64ZBT-NEXT:    ret
-  %tobool.not = icmp eq i32 %b, 0
-  %cond = select i1 %tobool.not, i32 %c, i32 %a
-  ret i32 %cond
-}
-
-define signext i32 @cmov_sle_i32(i32 signext %a, i32 signext %b, i32 signext %c, i32 signext %d) nounwind {
-; RV64I-LABEL: cmov_sle_i32:
-; RV64I:       # %bb.0:
-; RV64I-NEXT:    bge a2, a1, .LBB3_2
-; RV64I-NEXT:  # %bb.1:
-; RV64I-NEXT:    mv a0, a3
-; RV64I-NEXT:  .LBB3_2:
-; RV64I-NEXT:    ret
-;
-; RV64B-LABEL: cmov_sle_i32:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    slt a1, a2, a1
-; RV64B-NEXT:    cmov a0, a1, a3, a0
-; RV64B-NEXT:    ret
-;
-; RV64ZBT-LABEL: cmov_sle_i32:
-; RV64ZBT:       # %bb.0:
-; RV64ZBT-NEXT:    slt a1, a2, a1
-; RV64ZBT-NEXT:    cmov a0, a1, a3, a0
-; RV64ZBT-NEXT:    ret
-  %tobool = icmp sle i32 %b, %c
-  %cond = select i1 %tobool, i32 %a, i32 %d
-  ret i32 %cond
-}
-
-define signext i32 @cmov_sge_i32(i32 signext %a, i32 signext %b, i32 signext %c, i32 signext %d) nounwind {
-; RV64I-LABEL: cmov_sge_i32:
-; RV64I:       # %bb.0:
-; RV64I-NEXT:    bge a1, a2, .LBB4_2
-; RV64I-NEXT:  # %bb.1:
-; RV64I-NEXT:    mv a0, a3
-; RV64I-NEXT:  .LBB4_2:
-; RV64I-NEXT:    ret
-;
-; RV64B-LABEL: cmov_sge_i32:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    slt a1, a1, a2
-; RV64B-NEXT:    cmov a0, a1, a3, a0
-; RV64B-NEXT:    ret
-;
-; RV64ZBT-LABEL: cmov_sge_i32:
-; RV64ZBT:       # %bb.0:
-; RV64ZBT-NEXT:    slt a1, a1, a2
-; RV64ZBT-NEXT:    cmov a0, a1, a3, a0
-; RV64ZBT-NEXT:    ret
-  %tobool = icmp sge i32 %b, %c
-  %cond = select i1 %tobool, i32 %a, i32 %d
-  ret i32 %cond
-}
-
-define signext i32 @cmov_ule_i32(i32 signext %a, i32 signext %b, i32 signext %c, i32 signext %d) nounwind {
-; RV64I-LABEL: cmov_ule_i32:
-; RV64I:       # %bb.0:
-; RV64I-NEXT:    bgeu a2, a1, .LBB5_2
-; RV64I-NEXT:  # %bb.1:
-; RV64I-NEXT:    mv a0, a3
-; RV64I-NEXT:  .LBB5_2:
-; RV64I-NEXT:    ret
-;
-; RV64B-LABEL: cmov_ule_i32:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    sltu a1, a2, a1
-; RV64B-NEXT:    cmov a0, a1, a3, a0
-; RV64B-NEXT:    ret
-;
-; RV64ZBT-LABEL: cmov_ule_i32:
-; RV64ZBT:       # %bb.0:
-; RV64ZBT-NEXT:    sltu a1, a2, a1
-; RV64ZBT-NEXT:    cmov a0, a1, a3, a0
-; RV64ZBT-NEXT:    ret
-  %tobool = icmp ule i32 %b, %c
-  %cond = select i1 %tobool, i32 %a, i32 %d
-  ret i32 %cond
-}
-
-define signext i32 @cmov_uge_i32(i32 signext %a, i32 signext %b, i32 signext %c, i32 signext %d) nounwind {
-; RV64I-LABEL: cmov_uge_i32:
-; RV64I:       # %bb.0:
-; RV64I-NEXT:    bgeu a1, a2, .LBB6_2
-; RV64I-NEXT:  # %bb.1:
-; RV64I-NEXT:    mv a0, a3
-; RV64I-NEXT:  .LBB6_2:
-; RV64I-NEXT:    ret
-;
-; RV64B-LABEL: cmov_uge_i32:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    sltu a1, a1, a2
-; RV64B-NEXT:    cmov a0, a1, a3, a0
-; RV64B-NEXT:    ret
-;
-; RV64ZBT-LABEL: cmov_uge_i32:
-; RV64ZBT:       # %bb.0:
-; RV64ZBT-NEXT:    sltu a1, a1, a2
-; RV64ZBT-NEXT:    cmov a0, a1, a3, a0
-; RV64ZBT-NEXT:    ret
-  %tobool = icmp uge i32 %b, %c
-  %cond = select i1 %tobool, i32 %a, i32 %d
-  ret i32 %cond
-}
-
-define i64 @cmov_i64(i64 %a, i64 %b, i64 %c) nounwind {
-; RV64I-LABEL: cmov_i64:
-; RV64I:       # %bb.0:
-; RV64I-NEXT:    beqz a1, .LBB7_2
-; RV64I-NEXT:  # %bb.1:
-; RV64I-NEXT:    mv a2, a0
-; RV64I-NEXT:  .LBB7_2:
-; RV64I-NEXT:    mv a0, a2
-; RV64I-NEXT:    ret
-;
-; RV64B-LABEL: cmov_i64:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    cmov a0, a1, a0, a2
-; RV64B-NEXT:    ret
-;
-; RV64ZBT-LABEL: cmov_i64:
-; RV64ZBT:       # %bb.0:
-; RV64ZBT-NEXT:    cmov a0, a1, a0, a2
-; RV64ZBT-NEXT:    ret
-  %tobool.not = icmp eq i64 %b, 0
-  %cond = select i1 %tobool.not, i64 %c, i64 %a
-  ret i64 %cond
-}
-
-define i64 @cmov_sle_i64(i64 %a, i64 %b, i64 %c, i64 %d) nounwind {
-; RV64I-LABEL: cmov_sle_i64:
-; RV64I:       # %bb.0:
-; RV64I-NEXT:    bge a2, a1, .LBB8_2
-; RV64I-NEXT:  # %bb.1:
-; RV64I-NEXT:    mv a0, a3
-; RV64I-NEXT:  .LBB8_2:
-; RV64I-NEXT:    ret
-;
-; RV64B-LABEL: cmov_sle_i64:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    slt a1, a2, a1
-; RV64B-NEXT:    cmov a0, a1, a3, a0
-; RV64B-NEXT:    ret
-;
-; RV64ZBT-LABEL: cmov_sle_i64:
-; RV64ZBT:       # %bb.0:
-; RV64ZBT-NEXT:    slt a1, a2, a1
-; RV64ZBT-NEXT:    cmov a0, a1, a3, a0
-; RV64ZBT-NEXT:    ret
-  %tobool = icmp sle i64 %b, %c
-  %cond = select i1 %tobool, i64 %a, i64 %d
-  ret i64 %cond
-}
-
-define i64 @cmov_sge_i64(i64 %a, i64 %b, i64 %c, i64 %d) nounwind {
-; RV64I-LABEL: cmov_sge_i64:
-; RV64I:       # %bb.0:
-; RV64I-NEXT:    bge a1, a2, .LBB9_2
-; RV64I-NEXT:  # %bb.1:
-; RV64I-NEXT:    mv a0, a3
-; RV64I-NEXT:  .LBB9_2:
-; RV64I-NEXT:    ret
-;
-; RV64B-LABEL: cmov_sge_i64:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    slt a1, a1, a2
-; RV64B-NEXT:    cmov a0, a1, a3, a0
-; RV64B-NEXT:    ret
-;
-; RV64ZBT-LABEL: cmov_sge_i64:
-; RV64ZBT:       # %bb.0:
-; RV64ZBT-NEXT:    slt a1, a1, a2
-; RV64ZBT-NEXT:    cmov a0, a1, a3, a0
-; RV64ZBT-NEXT:    ret
-  %tobool = icmp sge i64 %b, %c
-  %cond = select i1 %tobool, i64 %a, i64 %d
-  ret i64 %cond
-}
-
-define i64 @cmov_ule_i64(i64 %a, i64 %b, i64 %c, i64 %d) nounwind {
-; RV64I-LABEL: cmov_ule_i64:
-; RV64I:       # %bb.0:
-; RV64I-NEXT:    bgeu a2, a1, .LBB10_2
-; RV64I-NEXT:  # %bb.1:
-; RV64I-NEXT:    mv a0, a3
-; RV64I-NEXT:  .LBB10_2:
-; RV64I-NEXT:    ret
-;
-; RV64B-LABEL: cmov_ule_i64:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    sltu a1, a2, a1
-; RV64B-NEXT:    cmov a0, a1, a3, a0
-; RV64B-NEXT:    ret
-;
-; RV64ZBT-LABEL: cmov_ule_i64:
-; RV64ZBT:       # %bb.0:
-; RV64ZBT-NEXT:    sltu a1, a2, a1
-; RV64ZBT-NEXT:    cmov a0, a1, a3, a0
-; RV64ZBT-NEXT:    ret
-  %tobool = icmp ule i64 %b, %c
-  %cond = select i1 %tobool, i64 %a, i64 %d
-  ret i64 %cond
-}
-
-define i64 @cmov_uge_i64(i64 %a, i64 %b, i64 %c, i64 %d) nounwind {
-; RV64I-LABEL: cmov_uge_i64:
-; RV64I:       # %bb.0:
-; RV64I-NEXT:    bgeu a1, a2, .LBB11_2
-; RV64I-NEXT:  # %bb.1:
-; RV64I-NEXT:    mv a0, a3
-; RV64I-NEXT:  .LBB11_2:
-; RV64I-NEXT:    ret
-;
-; RV64B-LABEL: cmov_uge_i64:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    sltu a1, a1, a2
-; RV64B-NEXT:    cmov a0, a1, a3, a0
-; RV64B-NEXT:    ret
-;
-; RV64ZBT-LABEL: cmov_uge_i64:
-; RV64ZBT:       # %bb.0:
-; RV64ZBT-NEXT:    sltu a1, a1, a2
-; RV64ZBT-NEXT:    cmov a0, a1, a3, a0
-; RV64ZBT-NEXT:    ret
-  %tobool = icmp uge i64 %b, %c
-  %cond = select i1 %tobool, i64 %a, i64 %d
-  ret i64 %cond
-}
-
-declare i32 @llvm.fshl.i32(i32, i32, i32)
-
-define signext i32 @fshl_i32(i32 signext %a, i32 signext %b, i32 signext %c) nounwind {
-; RV64I-LABEL: fshl_i32:
-; RV64I:       # %bb.0:
-; RV64I-NEXT:    slli a0, a0, 32
-; RV64I-NEXT:    slli a1, a1, 32
-; RV64I-NEXT:    srli a1, a1, 32
-; RV64I-NEXT:    or a0, a0, a1
-; RV64I-NEXT:    andi a1, a2, 31
-; RV64I-NEXT:    sll a0, a0, a1
-; RV64I-NEXT:    srai a0, a0, 32
-; RV64I-NEXT:    ret
-;
-; RV64B-LABEL: fshl_i32:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    andi a2, a2, 31
-; RV64B-NEXT:    fslw a0, a0, a1, a2
-; RV64B-NEXT:    ret
-;
-; RV64ZBT-LABEL: fshl_i32:
-; RV64ZBT:       # %bb.0:
-; RV64ZBT-NEXT:    andi a2, a2, 31
-; RV64ZBT-NEXT:    fslw a0, a0, a1, a2
-; RV64ZBT-NEXT:    ret
-  %1 = tail call i32 @llvm.fshl.i32(i32 %a, i32 %b, i32 %c)
-  ret i32 %1
-}
-
-; Similar to fshl_i32 but result is not sign extended.
-define void @fshl_i32_nosext(i32 signext %a, i32 signext %b, i32 signext %c, i32* %x) nounwind {
-; RV64I-LABEL: fshl_i32_nosext:
-; RV64I:       # %bb.0:
-; RV64I-NEXT:    slli a0, a0, 32
-; RV64I-NEXT:    slli a1, a1, 32
-; RV64I-NEXT:    srli a1, a1, 32
-; RV64I-NEXT:    or a0, a0, a1
-; RV64I-NEXT:    andi a1, a2, 31
-; RV64I-NEXT:    sll a0, a0, a1
-; RV64I-NEXT:    srli a0, a0, 32
-; RV64I-NEXT:    sw a0, 0(a3)
-; RV64I-NEXT:    ret
-;
-; RV64B-LABEL: fshl_i32_nosext:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    andi a2, a2, 31
-; RV64B-NEXT:    fslw a0, a0, a1, a2
-; RV64B-NEXT:    sw a0, 0(a3)
-; RV64B-NEXT:    ret
-;
-; RV64ZBT-LABEL: fshl_i32_nosext:
-; RV64ZBT:       # %bb.0:
-; RV64ZBT-NEXT:    andi a2, a2, 31
-; RV64ZBT-NEXT:    fslw a0, a0, a1, a2
-; RV64ZBT-NEXT:    sw a0, 0(a3)
-; RV64ZBT-NEXT:    ret
-  %1 = tail call i32 @llvm.fshl.i32(i32 %a, i32 %b, i32 %c)
-  store i32 %1, i32* %x
-  ret void
-}
-
-declare i64 @llvm.fshl.i64(i64, i64, i64)
-
-define i64 @fshl_i64(i64 %a, i64 %b, i64 %c) nounwind {
-; RV64I-LABEL: fshl_i64:
-; RV64I:       # %bb.0:
-; RV64I-NEXT:    sll a0, a0, a2
-; RV64I-NEXT:    not a2, a2
-; RV64I-NEXT:    srli a1, a1, 1
-; RV64I-NEXT:    srl a1, a1, a2
-; RV64I-NEXT:    or a0, a0, a1
-; RV64I-NEXT:    ret
-;
-; RV64B-LABEL: fshl_i64:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    andi a2, a2, 63
-; RV64B-NEXT:    fsl a0, a0, a1, a2
-; RV64B-NEXT:    ret
-;
-; RV64ZBT-LABEL: fshl_i64:
-; RV64ZBT:       # %bb.0:
-; RV64ZBT-NEXT:    andi a2, a2, 63
-; RV64ZBT-NEXT:    fsl a0, a0, a1, a2
-; RV64ZBT-NEXT:    ret
-  %1 = tail call i64 @llvm.fshl.i64(i64 %a, i64 %b, i64 %c)
-  ret i64 %1
-}
-
-declare i32 @llvm.fshr.i32(i32, i32, i32)
-
-define signext i32 @fshr_i32(i32 signext %a, i32 signext %b, i32 signext %c) nounwind {
-; RV64I-LABEL: fshr_i32:
-; RV64I:       # %bb.0:
-; RV64I-NEXT:    slli a0, a0, 32
-; RV64I-NEXT:    slli a1, a1, 32
-; RV64I-NEXT:    srli a1, a1, 32
-; RV64I-NEXT:    or a0, a0, a1
-; RV64I-NEXT:    andi a1, a2, 31
-; RV64I-NEXT:    srl a0, a0, a1
-; RV64I-NEXT:    sext.w a0, a0
-; RV64I-NEXT:    ret
-;
-; RV64B-LABEL: fshr_i32:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    andi a2, a2, 31
-; RV64B-NEXT:    fsrw a0, a1, a0, a2
-; RV64B-NEXT:    ret
-;
-; RV64ZBT-LABEL: fshr_i32:
-; RV64ZBT:       # %bb.0:
-; RV64ZBT-NEXT:    andi a2, a2, 31
-; RV64ZBT-NEXT:    fsrw a0, a1, a0, a2
-; RV64ZBT-NEXT:    ret
-  %1 = tail call i32 @llvm.fshr.i32(i32 %a, i32 %b, i32 %c)
-  ret i32 %1
-}
-
-; Similar to fshr_i32 but result is not sign extended.
-define void @fshr_i32_nosext(i32 signext %a, i32 signext %b, i32 signext %c, i32* %x) nounwind {
-; RV64I-LABEL: fshr_i32_nosext:
-; RV64I:       # %bb.0:
-; RV64I-NEXT:    slli a0, a0, 32
-; RV64I-NEXT:    slli a1, a1, 32
-; RV64I-NEXT:    srli a1, a1, 32
-; RV64I-NEXT:    or a0, a0, a1
-; RV64I-NEXT:    andi a1, a2, 31
-; RV64I-NEXT:    srl a0, a0, a1
-; RV64I-NEXT:    sw a0, 0(a3)
-; RV64I-NEXT:    ret
-;
-; RV64B-LABEL: fshr_i32_nosext:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    andi a2, a2, 31
-; RV64B-NEXT:    fsrw a0, a1, a0, a2
-; RV64B-NEXT:    sw a0, 0(a3)
-; RV64B-NEXT:    ret
-;
-; RV64ZBT-LABEL: fshr_i32_nosext:
-; RV64ZBT:       # %bb.0:
-; RV64ZBT-NEXT:    andi a2, a2, 31
-; RV64ZBT-NEXT:    fsrw a0, a1, a0, a2
-; RV64ZBT-NEXT:    sw a0, 0(a3)
-; RV64ZBT-NEXT:    ret
-  %1 = tail call i32 @llvm.fshr.i32(i32 %a, i32 %b, i32 %c)
-  store i32 %1, i32* %x
-  ret void
-}
-
-declare i64 @llvm.fshr.i64(i64, i64, i64)
-
-define i64 @fshr_i64(i64 %a, i64 %b, i64 %c) nounwind {
-; RV64I-LABEL: fshr_i64:
-; RV64I:       # %bb.0:
-; RV64I-NEXT:    srl a1, a1, a2
-; RV64I-NEXT:    not a2, a2
-; RV64I-NEXT:    slli a0, a0, 1
-; RV64I-NEXT:    sll a0, a0, a2
-; RV64I-NEXT:    or a0, a0, a1
-; RV64I-NEXT:    ret
-;
-; RV64B-LABEL: fshr_i64:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    andi a2, a2, 63
-; RV64B-NEXT:    fsr a0, a1, a0, a2
-; RV64B-NEXT:    ret
-;
-; RV64ZBT-LABEL: fshr_i64:
-; RV64ZBT:       # %bb.0:
-; RV64ZBT-NEXT:    andi a2, a2, 63
-; RV64ZBT-NEXT:    fsr a0, a1, a0, a2
-; RV64ZBT-NEXT:    ret
-  %1 = tail call i64 @llvm.fshr.i64(i64 %a, i64 %b, i64 %c)
-  ret i64 %1
-}
-
-define signext i32 @fshri_i32(i32 signext %a, i32 signext %b) nounwind {
-; RV64I-LABEL: fshri_i32:
-; RV64I:       # %bb.0:
-; RV64I-NEXT:    srliw a1, a1, 5
-; RV64I-NEXT:    slli a0, a0, 27
-; RV64I-NEXT:    or a0, a0, a1
-; RV64I-NEXT:    sext.w a0, a0
-; RV64I-NEXT:    ret
-;
-; RV64B-LABEL: fshri_i32:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    fsriw a0, a1, a0, 5
-; RV64B-NEXT:    ret
-;
-; RV64ZBT-LABEL: fshri_i32:
-; RV64ZBT:       # %bb.0:
-; RV64ZBT-NEXT:    fsriw a0, a1, a0, 5
-; RV64ZBT-NEXT:    ret
-  %1 = tail call i32 @llvm.fshr.i32(i32 %a, i32 %b, i32 5)
-  ret i32 %1
-}
-
-; Similar to fshr_i32 but result is not sign extended.
-define void @fshri_i32_nosext(i32 signext %a, i32 signext %b, i32* %x) nounwind {
-; RV64I-LABEL: fshri_i32_nosext:
-; RV64I:       # %bb.0:
-; RV64I-NEXT:    srliw a1, a1, 5
-; RV64I-NEXT:    slli a0, a0, 27
-; RV64I-NEXT:    or a0, a0, a1
-; RV64I-NEXT:    sw a0, 0(a2)
-; RV64I-NEXT:    ret
-;
-; RV64B-LABEL: fshri_i32_nosext:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    fsriw a0, a1, a0, 5
-; RV64B-NEXT:    sw a0, 0(a2)
-; RV64B-NEXT:    ret
-;
-; RV64ZBT-LABEL: fshri_i32_nosext:
-; RV64ZBT:       # %bb.0:
-; RV64ZBT-NEXT:    fsriw a0, a1, a0, 5
-; RV64ZBT-NEXT:    sw a0, 0(a2)
-; RV64ZBT-NEXT:    ret
-  %1 = tail call i32 @llvm.fshr.i32(i32 %a, i32 %b, i32 5)
-  store i32 %1, i32* %x
-  ret void
-}
-
-define i64 @fshri_i64(i64 %a, i64 %b) nounwind {
-; RV64I-LABEL: fshri_i64:
-; RV64I:       # %bb.0:
-; RV64I-NEXT:    srli a1, a1, 5
-; RV64I-NEXT:    slli a0, a0, 59
-; RV64I-NEXT:    or a0, a0, a1
-; RV64I-NEXT:    ret
-;
-; RV64B-LABEL: fshri_i64:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    fsri a0, a1, a0, 5
-; RV64B-NEXT:    ret
-;
-; RV64ZBT-LABEL: fshri_i64:
-; RV64ZBT:       # %bb.0:
-; RV64ZBT-NEXT:    fsri a0, a1, a0, 5
-; RV64ZBT-NEXT:    ret
-  %1 = tail call i64 @llvm.fshr.i64(i64 %a, i64 %b, i64 5)
-  ret i64 %1
-}
-
-define signext i32 @fshli_i32(i32 signext %a, i32 signext %b) nounwind {
-; RV64I-LABEL: fshli_i32:
-; RV64I:       # %bb.0:
-; RV64I-NEXT:    srliw a1, a1, 27
-; RV64I-NEXT:    slli a0, a0, 5
-; RV64I-NEXT:    or a0, a0, a1
-; RV64I-NEXT:    sext.w a0, a0
-; RV64I-NEXT:    ret
-;
-; RV64B-LABEL: fshli_i32:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    fsriw a0, a1, a0, 27
-; RV64B-NEXT:    ret
-;
-; RV64ZBT-LABEL: fshli_i32:
-; RV64ZBT:       # %bb.0:
-; RV64ZBT-NEXT:    fsriw a0, a1, a0, 27
-; RV64ZBT-NEXT:    ret
-  %1 = tail call i32 @llvm.fshl.i32(i32 %a, i32 %b, i32 5)
-  ret i32 %1
-}
-
-; Similar to fshl_i32 but result is not sign extended.
-define void @fshli_i32_nosext(i32 signext %a, i32 signext %b, i32* %x) nounwind {
-; RV64I-LABEL: fshli_i32_nosext:
-; RV64I:       # %bb.0:
-; RV64I-NEXT:    srliw a1, a1, 27
-; RV64I-NEXT:    slli a0, a0, 5
-; RV64I-NEXT:    or a0, a0, a1
-; RV64I-NEXT:    sw a0, 0(a2)
-; RV64I-NEXT:    ret
-;
-; RV64B-LABEL: fshli_i32_nosext:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    fsriw a0, a1, a0, 27
-; RV64B-NEXT:    sw a0, 0(a2)
-; RV64B-NEXT:    ret
-;
-; RV64ZBT-LABEL: fshli_i32_nosext:
-; RV64ZBT:       # %bb.0:
-; RV64ZBT-NEXT:    fsriw a0, a1, a0, 27
-; RV64ZBT-NEXT:    sw a0, 0(a2)
-; RV64ZBT-NEXT:    ret
-  %1 = tail call i32 @llvm.fshl.i32(i32 %a, i32 %b, i32 5)
-  store i32 %1, i32* %x
-  ret void
-}
-
-define i64 @fshli_i64(i64 %a, i64 %b) nounwind {
-; RV64I-LABEL: fshli_i64:
-; RV64I:       # %bb.0:
-; RV64I-NEXT:    srli a1, a1, 59
-; RV64I-NEXT:    slli a0, a0, 5
-; RV64I-NEXT:    or a0, a0, a1
-; RV64I-NEXT:    ret
-;
-; RV64B-LABEL: fshli_i64:
-; RV64B:       # %bb.0:
-; RV64B-NEXT:    fsri a0, a1, a0, 59
-; RV64B-NEXT:    ret
-;
-; RV64ZBT-LABEL: fshli_i64:
-; RV64ZBT:       # %bb.0:
-; RV64ZBT-NEXT:    fsri a0, a1, a0, 59
-; RV64ZBT-NEXT:    ret
-  %1 = tail call i64 @llvm.fshl.i64(i64 %a, i64 %b, i64 5)
-  ret i64 %1
-}
-- 
2.33.1

